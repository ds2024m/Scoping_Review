@inproceedings{10.1145/3512731.3534208,
author = {Gan, Wenbin and Dao, Minh Son and Zettsu, Koji and Sun, Yuan},
title = {IoT-based Multimodal Analysis for Smart Education: Current Status, Challenges and Opportunities},
year = {2022},
isbn = {9781450392419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512731.3534208},
doi = {10.1145/3512731.3534208},
abstract = {IoT-based multimodal learning analytics promises to obtain an in-depth understanding of the learning process. It provides the insights for not only the explicit learning indicators but also the implicit attributes of learners, based on which further potential learning support can be timely provided in both physical and cyber world accordingly. In this paper, we present a systematic review of the existing studies for examining the empirical evidences on the usage of IoT data in education and the capabilities of multimodal analysis to provide useful insights for smarter education. In particular, we classify the multimodal data into four categories based on the data sources (data from digital, physical, physiological and environmental spaces). Moreover, we propose a concept framework for better understanding the current state of the filed and summarize the insights into six main themes (learner behavior understanding, learner affection computing, smart learning environment, learning performance prediction, group collaboration modeling and intelligent feedback) based on the objectives for intelligent learning. The associations between different combinations of data modalities and various learning indicators are comprehensively discussed. Finally, the challenges and future directions are also presented from three aspects.},
booktitle = {Proceedings of the 3rd ACM Workshop on Intelligent Cross-Data Analysis and Retrieval},
pages = {32–40},
numpages = {9},
keywords = {IoT in education, internet of things, learning analytics, multimodal analysis, smart education},
location = {Newark, NJ, USA},
series = {ICDAR '22}
}

@inproceedings{10.1145/3664647.3680978,
author = {Shi, Yuanchen and Kong, Fang},
title = {Integrating Stickers into Multimodal Dialogue Summarization: A Novel Dataset and Approach for Enhancing Social Media Interaction},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680978},
doi = {10.1145/3664647.3680978},
abstract = {With the popularity of social media, growing number of online chats and comments are presented in the form of multimodal dialogues containing stickers. Automatically summarizing these dialogues can effectively reduce content overload and save reading time. However, existing datasets and works are either text dialogue summarization, or articles with real photos that respectively perform text summaries and key image extraction, and have not simultaneously considered the multimodal dialogue automatic summarization tasks with sticker images and online chat scenarios. To compensate for the lack of datasets and researches in this field, we propose a brand-new Multimodal Chat Dialogue Summarization Containing Stickers (MCDSCS) task and dataset. It consists of 5,527 Chinese multimodal chat dialogues and 14,356 different sticker images, with each dialogue interspersed with stickers in the text to reflect the real social media chat scenario. MCDSCS can also contribute to filling the gap in Chinese multimodal dialogue data. We use the most advanced GPT4 model and carefully design Chain-of-Thoughts (COT) supplemented with manual review to generate dialogues and extract summaries. We also propose a novel method that integrates the visual information of stickers with the text descriptions of emotions and intentions (TEI). Experiments show that our method can effectively improve the performance of various mainstream summary generation models, even better than some other multimodal models, ChatGPT, and Vision Large Language Models (VLMs). Our data and code are publicly available at https://github.com/FakerBoom/MCDSCS.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {9525–9534},
numpages = {10},
keywords = {multimodal dialogue summarization, social media chat, sticker images, visual information},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3664647.3681102,
author = {Zheng, Changmeng and Liang, Dayong and Zhang, Wengyu and Wei, Xiao-Yong and Chua, Tat-Seng and Li, Qing},
title = {A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681102},
doi = {10.1145/3664647.3681102},
abstract = {This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate that BDoG is able to achieve state-of-the-art results in ScienceQA and MMBench with significant improvements over previous methods. The source code can be accessed at https://github.com/thecharm/BDoG.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {419–428},
numpages = {10},
keywords = {large language models, multi-agent debate, multi-modal reasoning},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3573942.3573947,
author = {Wang, Zhe and Liu, Ying and Fang, Jie and Li, Daxiang},
title = {Deep Learning-Based Sentiment Analysis for Social Media},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3573947},
doi = {10.1145/3573942.3573947},
abstract = {Due to the continuous popularization of the Internet and mobile phones, people have gradually entered a participatory network era, and the rapid growth of social networks has caused an explosion of digital information content. It has turned online opinions, blogs, tweets and posts into highly valuable assets, allowing governments and businesses to gain insights from the data and make their strategies. Business organizations need to process and analyze these sentiments to investigate the data and gain business insights. In recent years, deep learning techniques have been very successful in performing sentiment analysis, which offers automatic feature extraction, rich representation capabilities and better performance compared with traditional feature-based techniques. The core idea is to extract complex features automatically from large amounts of data by building deep neural networks to generate up-to-date predictions. This paper reviews social media sentiment analysis methods based on deep learning. Firstly, it introduces the process of single-modal text sentiment analysis on social media. Then it summarizes the multimodal sentiment analysis algorithms for social media, and divides the algorithm into feature layer fusion, decision layer fusion and linear regression model according to different fusion strategies. finally, the difficulties of social media sentiment analysis based on deep learning and future research directions are discussed.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {30–37},
numpages = {8},
keywords = {Deep learning, multimodality, sentiment analysis, social media},
location = {Xiamen, China},
series = {AIPR '22}
}

@inproceedings{10.1145/3677052.3698694,
author = {Papasotiriou, Kassiani and Sood, Srijan and Reynolds, Shayleen and Balch, Tucker},
title = {AI in Investment Analysis: LLMs for Equity Stock Ratings},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698694},
doi = {10.1145/3677052.3698694},
abstract = {Investment Analysis is a cornerstone of the Financial Services industry. The rapid integration of advanced machine learning techniques, particularly Large Language Models (LLMs), offers opportunities to enhance the equity stock rating process. This paper explores the application of LLMs to predict stock performance and generate stock ratings by ingesting diverse datasets. Traditional stock rating methods rely heavily on the expertise of financial analysts, and face several challenges such as data overload, inconsistencies in filings, and delayed reactions to market events. Our study addresses these issues by leveraging LLMs to improve the accuracy and consistency of stock ratings. Additionally, we assess the efficacy of using different data modalities with LLMs for the financial domain. We utilize varied datasets comprising fundamental financial, market, and news data from January 2022 to June 2024, along with GPT-4-32k (v0613) (with a training cutoff in Sep. 2021 to prevent information leakage). Our results show that our benchmark method outperforms traditional stock rating methods when assessed by forward returns. Specifically, incorporating financial fundamentals enhances ratings accuracy. While integrating news data improves short-term performance, substituting detailed news summaries with sentiment scores reduces token use without loss of performance. In many cases, omitting news data entirely enhances performance by reducing bias. Our research shows that LLMs can be leveraged to effectively utilize large amounts of multimodal financial data, as showcased by their effectiveness at the stock rating prediction task. Our work provides a reproducible framework for generating consistent and accurate stock ratings, offering a cost-effective and efficient alternative to traditional methods. Future work will extend the analysis to longer time horizons, incorporating more diverse data, and utilizing newer models to enhance detailed investment analysis and reports.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {419–427},
numpages = {9},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3477495.3531715,
author = {Rao, Jun and Wang, Fei and Ding, Liang and Qi, Shuhan and Zhan, Yibing and Liu, Weifeng and Tao, Dacheng},
title = {Where Does the Performance Improvement Come From? - A Reproducibility Concern about Image-Text Retrieval},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531715},
doi = {10.1145/3477495.3531715},
abstract = {This article aims to provide the information retrieval community with some reflections on recent advances in retrieval learning by analyzing the reproducibility of image-text retrieval models. Due to the increase of multimodal data over the last decade, image-text retrieval has steadily become a major research direction in the field of information retrieval. Numerous researchers train and evaluate image-text retrieval algorithms using benchmark datasets such as MS-COCO and Flickr30k. Research in the past has mostly focused on performance, with multiple state-of-the-art methodologies being suggested in a variety of ways. According to their assertions, these techniques provide improved modality interactions and hence more precise multimodal representations. In contrast to previous works, we focus on the reproducibility of the approaches and the examination of the elements that lead to improved performance by pretrained and nonpretrained models in retrieving images and text. To be more specific, we first examine the related reproducibility concerns and explain why our focus is on image-text retrieval tasks. Second, we systematically summarize the current paradigm of image-text retrieval models and the stated contributions of those approaches. Third, we analyze various aspects of the reproduction of pretrained and nonpretrained retrieval models. To complete this, we conducted ablation experiments and obtained some influencing factors that affect retrieval recall more than the improvement claimed in the original paper. Finally, we present some reflections and challenges that the retrieval community should consider in the future. Our source code is publicly available at https://github.com/WangFei-2019/Image-text-Retrieval.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2727–2737},
numpages = {11},
keywords = {image-text retrieval, network reliability, reproducibility},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3664647.3681601,
author = {Wang, Fanfan and Ma, Heqing and Shen, Xiangqing and Yu, Jianfei and Xia, Rui},
title = {Observe before Generate: Emotion-Cause aware Video Caption for Multimodal Emotion Cause Generation in Conversations},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681601},
doi = {10.1145/3664647.3681601},
abstract = {Emotion cause analysis has attracted increasing attention in recent years. However, the integration of multimodal information with emotion causes remains underexplored. Existing studies merely extract utterances from conversations as cause evidence, which is too coarse-grained to locate the exact causes from other modalities, especially those that may be reflected only in a specific video frame of an utterance. To address these limitations, we introduce a new task named Multimodal Emotion Cause Generation in Conversations (MECGC), which aims to generate an abstractive summary clearly and intuitively describing the causes that trigger the given emotion based on the multimodal context of conversations. We accordingly construct a dataset named ECGF that contains 1,374 conversations and 7,690 emotion instances from TV series. We further develop a generative framework that first generates emotion-cause aware video captions (Observe) and then facilitates the generation of emotion causes (Generate). The captioning model is trained with examples synthesized by a Multimodal Large Language Model (MLLM). Experimental results demonstrate the effectiveness of our framework and the significance of multimodal information for emotion cause analysis. Our dataset and source codes are available at https://github.com/NUSTM/MECGC.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {5820–5828},
numpages = {9},
keywords = {emotion cause analysis in conversations, multimodal emotion cause generation, video captioning},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3506860.3506862,
author = {Yan, Lixiang and Zhao, Linxuan and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {Scalability, Sustainability, and Ethicality of Multimodal Learning Analytics},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506862},
doi = {10.1145/3506860.3506862},
abstract = {Multimodal Learning Analytics (MMLA) innovations are commonly aimed at supporting learners in physical learning spaces through state-of-the-art sensing technologies and analysis techniques. Although a growing body of MMLA research has demonstrated the potential benefits of sensor-based technologies in education, whether their use can be scalable, sustainable, and ethical remains questionable. Such uncertainty can limit future research and the potential adoption of MMLA by educational stakeholders in authentic learning situations. To address this, we systematically reviewed the methodological, operational, and ethical challenges faced by current MMLA works that can affect the scalability and sustainability of future MMLA innovations. A total of 96 peer-reviewed articles published after 2010 were included. The findings were summarised into three recommendations, including i) improving reporting standards by including sufficient details about sensors, analysis techniques, and the full disclosure of evaluation metrics, ii) fostering interdisciplinary collaborations among experts in learning analytics, software, and hardware engineering to develop affordable sensors and upgrade MMLA innovations that used discontinued technologies, and iii) developing ethical guidelines to address the potential risks of bias, privacy, and equality concerns with using MMLA innovations. Through these future research directions, MMLA can remain relevant and eventually have actual impacts on educational practices.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {13–23},
numpages = {11},
keywords = {ethics, learning analytics, multimodal learning analytics, scalability, sensors, sustainability},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3630106.3658968,
author = {Birhane, Abeba and Dehdashtian, Sepehr and Prabhu, Vinay and Boddeti, Vishnu},
title = {The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658968},
doi = {10.1145/3630106.3658968},
abstract = {‘Scale the model, scale the data, scale the GPU farms’ is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. All the meta-datasets curated in this endeavor and the code used are shared at: https://github.com/SepehrDehdashtian/the-dark-side-of-dataset-scaling. Content warning: This article contains racially dehumanising and offensive descriptions.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1229–1244},
numpages = {16},
keywords = {Audits, Bias, CLIP, Evaluations, Multimodal Datasets, Racism, Scale, Visio-Linguistic Models},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@article{10.1145/3363560,
author = {Zhao, Sicheng and Wang, Shangfei and Soleymani, Mohammad and Joshi, Dhiraj and Ji, Qiang},
title = {Affective Computing for Large-scale Heterogeneous Multimedia Data: A Survey},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3363560},
doi = {10.1145/3363560},
abstract = {The wide popularity of digital photography and social networks has generated a rapidly growing volume of multimedia data (i.e., images, music, and videos), resulting in a great demand for managing, retrieving, and understanding these data. Affective computing (AC) of these data can help to understand human behaviors and enable wide applications. In this article, we survey the state-of-the-art AC technologies comprehensively for large-scale heterogeneous multimedia data. We begin this survey by introducing the typical emotion representation models from psychology that are widely employed in AC. We briefly describe the available datasets for evaluating AC algorithms. We then summarize and compare the representative methods on AC of different multimedia types, i.e., images, music, videos, and multimodal data, with the focus on both handcrafted features-based methods and deep learning methods. Finally, we discuss some challenges and future directions for multimedia affective computing.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {93},
numpages = {32},
keywords = {Affective computing, emotion recognition, large-scale multimedia, sentiment analysis}
}

@inproceedings{10.1145/3678957.3685716,
author = {W\"{o}rtwein, Torsten and Allen, Nicholas B. and Cohn, Jeffrey F. and Morency, Louis-Philippe},
title = {SMURF: Statistical Modality Uniqueness and Redundancy Factorization},
year = {2024},
isbn = {9798400704628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678957.3685716},
doi = {10.1145/3678957.3685716},
abstract = {Multimodal late fusion is a well-performing fusion method that sums the outputs of separately processed modalities, so-called modality contributions, to create a prediction; for example, summing contributions from vision, acoustic, and language to predict affective states. In this paper, our primary goal is to improve the interpretability of what modalities contribute to the prediction in late fusion models. More specifically, we want to factorize modality contributions into what is consistently shared by at least two modalities (pairwise redundant contributions) and what the remaining modality-specific contributions are (unique contributions). Our secondary goal is to improve robustness to missing modalities by encouraging the model to learn redundant contributions. To achieve our two goals, we propose SMURF (Statistical Modality Uniqueness and Redundancy Factorization), a late fusion method that factorizes its outputs into a) unique contributions that are uncorrelated with all other modalities and b) pairwise redundant contributions that are maximally correlated between two modalities. For our primary goal, we 1) verify SMURF’s factorization on a synthetic dataset, 2) ensure that its factorization does not degrade predictive performance on eight affective datasets, and 3) observe significant relationships between its factorization and human judgments on three datasets. For our secondary goal, we demonstrate that SMURF leads to more robustness to missing modalities at test time compared to three late fusion baselines.},
booktitle = {Proceedings of the 26th International Conference on Multimodal Interaction},
pages = {339–349},
numpages = {11},
keywords = {Machine Learning, Multimodal, Redundant, Unique},
location = {San Jose, Costa Rica},
series = {ICMI '24}
}

@inproceedings{10.5555/3237383.3237974,
author = {Spaulding, Samuel},
title = {Personalized Robot Tutors that Learn from Multimodal Data},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As the cost of sensors decreases and ability to model and learn from multi-modal data increases, researchers are exploring how to use the unique qualities of physically embodied robots to help engage students and promote learning. These robots are designed to emulate the emotive, perceptual, and empathic abilities of human teachers, and are capable of replicating some of the benefits of one-on-one tutoring from human teachers. My thesis research focuses on developing methods for robots to analyze and integrate multimodal data including speech, facial expressions, and task performance to build rich models of the user's knowledge and preferences. These student models are then used to provide personalized educational experiences, such as optimal curricular sequencing, or leaning preferences for educational style. In this abstract, we summarize past projects in this area and discuss applications such as learning from affective signals and model transfer across tasks.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1781–1783},
numpages = {3},
keywords = {human-robot interaction, multimodal interaction, social robotics},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3347318.3355524,
author = {Sanabria, Melissa and Sherly and Precioso, Fr\'{e}d\'{e}ric and Menguy, Thomas},
title = {A Deep Architecture for Multimodal Summarization of Soccer Games},
year = {2019},
isbn = {9781450369114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347318.3355524},
doi = {10.1145/3347318.3355524},
abstract = {The massive growth of sports videos, specially in soccer, has resulted in a need for the automatic generation of summaries, where the objective is not only to show the most important actions of the match but also to elicit as much emotion as the ones bring upon by human editors. State-of-the-art methods on video summarization mostly rely on video processing, however this is not an optimal approach for long videos such as soccer matches. In this paper we propose a multimodal approach to automatically generate summaries of soccer match videos that consider both event and audio features. The event features get a shorter and better representation of the match, and the audio helps detect the excitement generated by the game. Our method consists of three consecutive stages: Proposals, Summarization and Content Refinement. The first one generates summary proposals, using Multiple Instance Learning to deal with the similarity between the events inside the summary and the rest of the match. The Summarization stage uses event and audio features as input of a hierarchical Recurrent Neural Network to decide which proposals should indeed be in the summary. And the last stage, takes advantage of the visual content to create the final summary. The results show that our approach outperforms by a large margin not only the video processing methods but also methods that use event and audio features.},
booktitle = {Proceedings Proceedings of the 2nd International Workshop on Multimedia Content Analysis in Sports},
pages = {16–24},
numpages = {9},
keywords = {long short-term memory, multimodal analysis, multiple instance learning, sports summarization, video summarization},
location = {Nice, France},
series = {MMSports '19}
}

@inproceedings{10.1145/2964284.2971475,
author = {You, Quanzeng},
title = {Sentiment and Emotion Analysis for Social Multimedia: Methodologies and Applications},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2971475},
doi = {10.1145/2964284.2971475},
abstract = {Online social networks have attracted the attention from both the academia and real world. In particular, the rich multimedia information accumulated in recent years provides an easy and convenient way for more active communication between people. This offers an opportunity to research people's behaviors and activities based on those multimedia content. One emerging area is driven by the fact that these massive multimedia data contain people's daily sentiments and opinions. However, existing sentiment analysis typically focuses on textual information regardless of the visual content, which may be as informative in expressing people's sentiments and opinions. In this research, we attempt to analyze the online sentiment changes of social media users using both the textual and visual content. Nowadays, social media networks such as Twitter have become major platforms of information exchange and communication between users, with tweets as the common information carrier. As an old saying has it, an image is worth a thousand words. The image tweet is a great example of multimodal sentiment. In this research, we focus on sentiment analysis based on visual and multimedia information analysis. We will review the state-of-the-art in this topic. Several of our projects related to this research area will also be discussed. Experimental results are included to demonstrate and summarize our contributions.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1445–1449},
numpages = {5},
keywords = {joint sentiment analysis, multimodal, visual sentiment analysis},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.1145/2818346.2829993,
author = {Jung, Merel M. and Cang, Xi Laura and Poel, Mannes and MacLean, Karon E.},
title = {Touch Challenge '15: Recognizing Social Touch Gestures},
year = {2015},
isbn = {9781450339124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818346.2829993},
doi = {10.1145/2818346.2829993},
abstract = {Advances in the field of touch recognition could open up applications for touch-based interaction in areas such as Human-Robot Interaction (HRI). We extended this challenge to the research community working on multimodal interaction with the goal of sparking interest in the touch modality and to promote exploration of the use of data processing techniques from other more mature modalities for touch recognition. Two data sets were made available containing labeled pressure sensor data of social touch gestures that were performed by touching a touch-sensitive surface with the hand. Each set was collected from similar sensor grids, but under conditions reflecting different application orientations: CoST: Corpus of Social Touch and HAART: The Human-Animal Affective Robot Touch gesture set. In this paper we describe the challenge protocol and summarize the results from the touch challenge hosted in conjunction with the 2015 ACM International Conference on Multimodal Interaction (ICMI). The most important outcomes of the challenges were: (1) transferring techniques from other modalities, such as image processing, speech, and human action recognition provided valuable feature sets; (2) gesture classification confusions were similar despite the various data processing methods used.},
booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
pages = {387–390},
numpages = {4},
keywords = {social touch, touch data set, touch gesture recognition},
location = {Seattle, Washington, USA},
series = {ICMI '15}
}

@inproceedings{10.1145/3640771.3640773,
author = {Jiang, Keqin and Jiang, Mingyan},
title = {A Hybrid Improved Lion Swarm Optimization Algorithm},
year = {2024},
isbn = {9798400708954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640771.3640773},
doi = {10.1145/3640771.3640773},
abstract = {Abstract: When solving multimodal optimization problems, the lion swarm optimization (LSO) algorithm will face many problems, such as low individual diversity, slow search speed, premature convergence or even falling into local extremum. The traditional approach is to introduce chaotic search, gaussian mutation or other mutation strategies to enhance the local search ability of the LSO algorithm. These improvements have been verified and the performance of the algorithm can be improved to a certain extent. However, these improved strategies lack effective use of population information, which will still affect the performance of the algorithm in search speed and search accuracy. To solve this problem, on the basis of the above-mentioned improved algorithm, this paper introduces the distribution estimation algorithm and proposes a hybrid improved LSO algorithm. The hybrid improved algorithm analyzes and learns the structure of the problem by constructing a probability model for the dominant group, and guides the efficient optimization of individuals in the population according to this information. It is verified in five standard test functions and ten test functions of IEEE CEC 2021. Compared with the traditional improved LSO algorithm, the results show that the hybrid improved LSO algorithm is superior.},
booktitle = {Proceedings of the 2023 2nd International Symposium on Computing and Artificial Intelligence},
pages = {1–4},
numpages = {4},
keywords = {Chaotic search, Distribution estimation algorithm, Lion swarm optimization algorithm, Mutation strategy},
location = {Shanghai, China},
series = {ISCAI '23}
}

@inproceedings{10.1145/3206025.3206076,
author = {Sivaprasad, Sarath and Joshi, Tanmayee and Agrawal, Rishabh and Pedanekar, Niranjan},
title = {Multimodal Continuous Prediction of Emotions in Movies using Long Short-Term Memory Networks},
year = {2018},
isbn = {9781450350464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206025.3206076},
doi = {10.1145/3206025.3206076},
abstract = {Predicting emotions that movies are designed to evoke, can be useful in entertainment applications such as content personalization, video summarization and ad placement. Multimodal input, primarily audio and video, helps in building the emotional content of a movie. Since the emotion is built over time by audio and video, the temporal context of these modalities is an important aspect in modeling it. In this paper, we use Long Short-Term Memory networks (LSTMs) to model the temporal context in audio-video features of movies. We present continuous emotion prediction results using a multimodal fusion scheme on an annotated dataset of Academy Award winning movies. We report a significant improvement over the state-of-the-art results, wherein the correlation between predicted and annotated values is improved from 0.62 vs 0.84 for arousal, and from 0.29 to 0.50 for valence.},
booktitle = {Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval},
pages = {413–419},
numpages = {7},
keywords = {deep learning, emotion prediction, lstm, movies, multimodal},
location = {Yokohama, Japan},
series = {ICMR '18}
}

@inproceedings{10.5555/3400397.3400704,
author = {Alexopoulos, Christos and Goldsman, David and Mokashi, Anup C. and Wilson, James R.},
title = {Sequential estimation of steady-state quantiles: some new developments in methods and software},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Recent developments are summarized concerning Sequest and Sequem, sequential procedures for estimating nonextreme and extreme steady-state quantiles of a simulation output process. The procedures deliver point and confidence-interval (CI) estimators of a given quantile, where each CI approximately satisfies given requirements on its coverage probability and its absolute or relative precision. The public-domain Sequest software now includes both procedures. The software is applied to a user-supplied dataset exhibiting warm-up effects, autocorrelation, and a multimodal marginal distribution. For the simulation analysis method of standardized time series (STS), we also sketch an elementary proof of a functional central limit theorem (FCLT) that is needed to develop STS-based quantile-estimation procedures when the output process satisfies a conventional density-regularity condition and either (i) a geometric-moment contraction condition and an FCLT for a related binary process, or (ii) conventional strong-mixing conditions.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3774–3785},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3664647.3680584,
author = {Ding, Yang and Dai, Yi and Wang, Xin and Feng, Ling and Cao, Lei and Zhang, Huijun},
title = {Integrating Content-Semantics-World Knowledge to Detect Stress from Videos},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680584},
doi = {10.1145/3664647.3680584},
abstract = {Stress has rapidly emerged as a significant public health concern in the contemporary society, necessitating prompt identification and effective intervention strategies. Video-based stress detection offers a non-invasive, low-cost, and mass-reaching approach for identifying stress. In this paper, we propose a three-level content-semantic-world knowledge framework, addressing three particular issues for video-based stress detection. (1) How to abstract and encode video semantics with frame contents into visual representation? (2) How to leverage general-purpose LMMs to augment task-specific visual representation? (3) To what extent could general-purpose LMMs contribute to video-based stress detection? We design a Slow-Emotion-Fast-Action scheme to encode fast temporal changes of body actions revealed from video frames, as well as subtle details of emotions per video segment, into visual representation. We augment task-specific visual representation with linguistic facial expression descriptions by prompting general-purpose Large Multimodal Models (LMMs). A knowledge retriever is designed to evaluate and select the most proper deliverable of LMMs. Experimental results on two video-based stress detection datasets show that 1) our proposed three-level framework can achieve 90.89% F1-score in UVSD dataset and 80.79% F1-score, outperforming state-of-the-art; 2) leveraging LMMs helps to improve the F1-score by 2.25% in UVSD and 3.55% in RSL, compared to using the traditional Facial Action Coding System; 3) purely relying on general-purpose LMMs is insufficient with 88.73% F1-score in UVSD dataset and 77.48% F1-score in RSL dataset, demonstrating the necessity to combine task-specific dedicated solutions with world knowledge given by LMMs.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {10373–10381},
numpages = {9},
keywords = {large multimodal models, stress detection, video},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3382507.3417969,
author = {Petrova, Anastasia and Vaufreydaz, Dominique and Dessus, Philippe},
title = {Group-Level Emotion Recognition Using a Unimodal Privacy-Safe Non-Individual Approach},
year = {2020},
isbn = {9781450375818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382507.3417969},
doi = {10.1145/3382507.3417969},
abstract = {This article presents our unimodal privacy-safe and non-individual proposal for the audio-video group emotion recognition subtask at the Emotion Recognition in the Wild (EmotiW) Challenge 2020. This sub challenge aims to classify in the wild videos into three categories: Positive, Neutral and Negative. Recent deep learning models have shown tremendous advances in analyzing interactions between people, predicting human behavior and affective evaluation. Nonetheless, their performance comes from individual-based analysis, which means summing up and averaging scores from individual detections, which inevitably leads to some privacy issues. In this research, we investigated a frugal approach towards a model able to capture the global moods from the whole image without using face or pose detection, or any individual-based feature as input. The proposed methodology mixes state-of-the-art and dedicated synthetic corpora as training sources. With an in-depth exploration of neural network architectures for group-level emotion recognition, we built a VGG-based model achieving 59.13% accuracy on the VGAF test set (eleventh place of the challenge). Given that the analysis is unimodal based only on global features and that the performance is evaluated on a real-world dataset, these results are promising and let us envision extending this model to multimodality for classroom ambiance evaluation, our final target application.},
booktitle = {Proceedings of the 2020 International Conference on Multimodal Interaction},
pages = {813–820},
numpages = {8},
keywords = {affective computing, audio-video group emotion recognition, deep learning, emotiw 2020, privacy},
location = {Virtual Event, Netherlands},
series = {ICMI '20}
}

@inproceedings{10.1145/3347320.3357689,
author = {Li, Yan and Yang, Tao and Yang, Le and Xia, Xiaohan and Jiang, Dongmei and Sahli, Hichem},
title = {A Multimodal Framework for State of Mind Assessment with Sentiment Pre-classification},
year = {2019},
isbn = {9781450369138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347320.3357689},
doi = {10.1145/3347320.3357689},
abstract = {In this paper, we aim at the AVEC2019 State of Mind Sub-Challenge (SoMS), and propose a multimodal state of mind assessment framework, for valence and arousal, respectively. For valence, sentiment analysis is firstly performed on the English text obtained via German speech recognition and translation to classify the audio visual session into positive/negative narrative. Then each overlapping 60s segment of the session is input into an audio visual SoM assessment model trained for positive/negative narratives. The mean prediction of all the segments is adopted as the final prediction of the audio visual session. For arousal, the first step of positive/negative classification is not performed. For the audio-visual SoM assessment models, we propose to extract the functional features (Function) and VGGish based deep learning features (VGGish) from speech, and the abstract visual features based on convolutional neural network (CNN) from the baseline visual features. For each feature stream, a long short term memory (LSTM) model is trained to predict the valence/arousal values of a segment, and a support vector regression (SVR) model is adopted for the final decision fusion. Experiments on the USoM dataset show that the model with Function, baseline ResNet features and baseline VGG features obtains promising prediction results for valence, with concordance correlation coefficient (CCC) up to 0.531 on the test set, which is much higher than the baseline result 0.219.},
booktitle = {Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop},
pages = {13–18},
numpages = {6},
keywords = {multimodal SoM assessment model, sentiment analysis, state of mind (SoM), valence},
location = {Nice, France},
series = {AVEC '19}
}

@article{10.1145/1596517.1596518,
author = {Murray, Gabriel and Kleinbauer, Thomas and Poller, Peter and Becker, Tilman and Renals, Steve and Kilgour, Jonathan},
title = {Extrinsic summarization evaluation: A decision audit task},
year = {2009},
issue_date = {October 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1550-4875},
url = {https://doi.org/10.1145/1596517.1596518},
doi = {10.1145/1596517.1596518},
abstract = {In this work we describe a large-scale extrinsic evaluation of automatic speech summarization technologies for meeting speech. The particular task is a decision audit, wherein a user must satisfy a complex information need, navigating several meetings in order to gain an understanding of how and why a given decision was made. We compare the usefulness of extractive and abstractive technologies in satisfying this information need, and assess the impact of automatic speech recognition (ASR) errors on user performance. We employ several evaluation methods for participant performance, including post-questionnaire data, human subjective and objective judgments, and a detailed analysis of participant browsing behavior. We find that while ASR errors affect user satisfaction on an information retrieval task, users can adapt their browsing behavior to complete the task satisfactorily. Results also indicate that users consider extractive summaries to be intuitive and useful tools for browsing multimodal meeting data. We discuss areas in which automatic summarization techniques can be improved in comparison with gold-standard meeting abstracts.},
journal = {ACM Trans. Speech Lang. Process.},
month = oct,
articleno = {2},
numpages = {29},
keywords = {Summarization, abstraction, browsing, evaluation, extraction, interfaces}
}

@article{10.1109/TASLP.2021.3096037,
author = {Zhou, Hengshun and Du, Jun and Zhang, Yuanyuan and Wang, Qing and Liu, Qing-Feng and Lee, Chin-Hui},
title = {Information Fusion in Attention Networks Using Adaptive and Multi-Level Factorized Bilinear Pooling for Audio-Visual Emotion Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3096037},
doi = {10.1109/TASLP.2021.3096037},
abstract = {Multimodal emotion recognition is a challenging task in emotion computing as it is quite difficult to extract discriminative features to identify the subtle differences in human emotions with abstract concept and multiple expressions. Moreover, how to fully utilize both audio and visual information is still an open problem. In this paper, we propose a novel multimodal fusion attention network for audio-visual emotion recognition based on adaptive and multi-level factorized bilinear pooling (FBP). First, for the audio stream, a fully convolutional network (FCN) equipped with 1-D attention mechanism and local response normalization is designed for speech emotion recognition. Next, a global FBP (G-FBP) approach is presented to perform audio-visual information fusion by integrating self-attention based video stream with the proposed audio stream. To improve G-FBP, an adaptive strategy (AG-FBP) to dynamically calculate the fusion weight of two modalities is devised based on the emotion-related representation vectors from the attention mechanism of respective modalities. Finally, to fully utilize the local emotion information, adaptive and multi-level FBP (AM-FBP) is introduced by combining both global-trunk and intra-trunk data in one recording on top of AG-FBP. Tested on the IEMOCAP corpus for speech emotion recognition with only audio stream, the new FCN method outperforms the state-of-the-art results with an accuracy of 71.40%. Moreover, validated on the AFEW database of EmotiW2019 sub-challenge and the IEMOCAP corpus for audio-visual emotion recognition, the proposed AM-FBP approach achieves the best accuracy of 63.09% and 75.49% respectively on the test set.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2617–2629},
numpages = {13}
}

@inproceedings{10.1145/3338286.3344395,
author = {Rzayev, Rufat and Karaman, G\"{u}rkan and Henze, Niels and Schwind, Valentin},
title = {Fostering Virtual Guide in Exhibitions},
year = {2019},
isbn = {9781450368254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338286.3344395},
doi = {10.1145/3338286.3344395},
abstract = {Museums are essential to make culture accessible to the mass audience. Human museum guides are important to explain the presented artifacts to the visitors. Recently, museums started to experiment with enhancing exhibitions through mixed reality. It enables cultural exhibitors to provide each visitor with an individualized virtual guide that adapts to the visitor's interests. The effect of the presence and appearance of a virtual museum guide is, however, unclear. In this paper, we compare a real-world guide with a realistic, an abstract, and an audio-only representation of the virtual guide. Participants followed four multimodal presentations while we investigated the effect on comprehension and perceived co-presence. We found that a realistic representation of a virtual guide increases the perceived co-presence and does not adversely affect the comprehension of learning content in mixed reality exhibitions. Insights from our study inform the design of virtual guides for real-world exhibitions.},
booktitle = {Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {48},
numpages = {6},
keywords = {Mixed reality, co-presence, exhibition, virtual avatar},
location = {Taipei, Taiwan},
series = {MobileHCI '19}
}

@inproceedings{10.1145/3125739.3125744,
author = {Tsiourti, Christiana and Weiss, Astrid and Wac, Katarzyna and Vincze, Markus},
title = {Designing Emotionally Expressive Robots: A Comparative Study on the Perception of Communication Modalities},
year = {2017},
isbn = {9781450351133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3125739.3125744},
doi = {10.1145/3125739.3125744},
abstract = {Socially assistive agents, be it virtual avatars or robots, need to engage in social interactions with humans and express their internal emotional states, goals, and desires. In this work, we conducted a comparative study to investigate how humans perceive emotional cues expressed by humanoid robots through five communication modalities (face, head, body, voice, locomotion) and examined whether the degree of a robot's human-like embodiment affects this perception. In an online survey, we asked people to identify emotions communicated by Pepper - a highly human-like robot and Hobbit - a robot with abstract humanlike features. A qualitative and quantitative data analysis confirmed the expressive power of the face, but also demonstrated that body expressions or even simple head and locomotion movements could convey emotional information. These findings suggest that emotion recognition accuracy varies as a function of the modality, and a higher degree of anthropomorphism does not necessarily lead to a higher level of recognition accuracy. Our results further the understanding of how people respond to single communication modalities and have implications for designing recognizable multimodal expressions for robots.},
booktitle = {Proceedings of the 5th International Conference on Human Agent Interaction},
pages = {213–222},
numpages = {10},
keywords = {HAI, HRI, body motion, emotional expression, facial expression, multi-modal interaction, social robots},
location = {Bielefeld, Germany},
series = {HAI '17}
}

@inproceedings{10.1145/2470654.2466139,
author = {Warnock, David and McGee-Lennon, Marilyn and Brewster, Stephen},
title = {Multiple notification modalities and older users},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2466139},
doi = {10.1145/2470654.2466139},
abstract = {Multimodal interaction can make home care reminder systems more accessible to their users, most of whom are older and/or have sensory impairments. Existing research into the properties of different notification modalities have used younger participants rather than members of the older population at which they are aimed. This paper presents the results of a user study with older adults that examined how different notification modalities affected (a) performance in a card matching game and (b) how effective the different modalities were at delivering information. Participants were all aged over 50 and notifications were delivered using textual, pictographic, abstract visual, speech, Earcon, Auditory Icon, tactile and olfactory modalities while playing the game. The results showed that older users were influenced by the same factors as younger users, yet there were subjective differences. The implications for the design of multimodal reminder systems for home care are discussed.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1091–1094},
numpages = {4},
keywords = {multimodal, notifications, older users, reminders},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/1877937.1877952,
author = {S\o{}berg, Jarle and Goebel, Vera and Plagemann, Thomas},
title = {Detection of spatial events in CommonSens},
year = {2010},
isbn = {9781450301763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1877937.1877952},
doi = {10.1145/1877937.1877952},
abstract = {The aim of our complex event processing system, CommonSens, is to simplify the application programmers' tasks by abstracting away from any particular sensor installation.Queries only need to refer to capabilities and locations of interest. In order to detect spatial events, CommonSens identifies the sensors that cover the specific locations where the spatial events should occur. This requires calculating how the coverage areas of sensors are affected by the environment. CommonSens promotes the use of heterogeneous multimodal sensors to reduce the probability of errors. It calculates the probability of false positives related to locations of interest and assists the application programmer with placing sensors in the environment effectively. We demonstrate through a series of experiments the functional properties of CommonSens related to these spatial issues. Furthermore, our experiments show that our non-optimised prototype is able to process all sensor readings and detect complex events in real-time.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Events in Multimedia},
pages = {53–58},
numpages = {6},
keywords = {automated homecare, complex event processing, multimodality},
location = {Firenze, Italy},
series = {EiMM '10}
}

