@inproceedings{b-etal-2023-findings,
 abstract = {This paper summarizes the shared task on multimodal abusive language detection and sentiment analysis in Dravidian languages as part of the third Workshop on Speech and Language Technologies for Dravidian Languages at RANLP 2023. This shared task provides a platform for researchers worldwide to submit their models on two crucial social media data analysis problems in Dravidian languages - abusive language detection and sentiment analysis. Abusive language detection identifies social media content with abusive information, whereas sentiment analysis refers to the problem of determining the sentiments expressed in a text. This task aims to build models for detecting abusive content and analyzing fine-grained sentiment from multimodal data in Tamil and Malayalam. The multimodal data consists of three modalities - video, audio and text. The datasets for both tasks were prepared by collecting videos from YouTube. Sixty teams participated in both tasks. However, only two teams submitted their results. The submissions were evaluated using macro F1-score.},
 address = {Varna, Bulgaria},
 author = {B, Premjith  and
G, Jyothish Lal  and
V, Sowmya  and
Chakravarthi, Bharathi Raja  and
Natarajan, Rajeswari  and
K, Nandhini  and
Murugappan, Abirami  and
B, Bharathi  and
M, Kaushik  and
Sn, Prasanth  and
R, Aswin Raj  and
S, Vijai Simmon},
 booktitle = {Proceedings of the Third Workshop on Speech and Language Technologies for Dravidian Languages},
 editor = {Chakravarthi, Bharathi R.  and
Priyadharshini, Ruba  and
M, Anand Kumar  and
Thavareesan, Sajeetha  and
Sherly, Elizabeth},
 month = sep,
 pages = {72--79},
 publisher = {INCOMA Ltd., Shoumen, Bulgaria},
 title = {Findings of the Shared Task on Multimodal Abusive Language Detection and Sentiment Analysis in {T}amil and {M}alayalam},
 url = {https://aclanthology.org/2023.dravidianlangtech-1.10/},
 year = {2023}
}

@inproceedings{wu-etal-2023-denoising,
 abstract = {Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on multiple benchmarks covering multimodal sentiment analysis and multimodal summarization tasks. It proves that our model can effectively capture salient features from noisy and redundant video, audio, and text inputs. The code for this paper will be publicly available at \url{https://github.com/WSXRHFG/DBF}},
 address = {Toronto, Canada},
 author = {Wu, Shaoxiang  and
Dai, Damai  and
Qin, Ziwei  and
Liu, Tianyu  and
Lin, Binghuai  and
Cao, Yunbo  and
Sui, Zhifang},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.124},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 month = jul,
 pages = {2231--2243},
 publisher = {Association for Computational Linguistics},
 title = {Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion},
 url = {https://aclanthology.org/2023.acl-long.124/},
 year = {2023}
}
