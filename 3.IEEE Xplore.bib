@INPROCEEDINGS{8288527,
  author={Wang, Yiming and Rao, Yuan and Wu, Lianwei},
  booktitle={2017 13th International Conference on Computational Intelligence and Security (CIS)}, 
  title={A Review of Sentiment Semantic Analysis Technology and Progress}, 
  year={2017},
  volume={},
  number={},
  pages={452-455},
  abstract={Sentiment computing brings some new application opportunities and technique challenges in artificial intelligence of the next generation, and it has become a fascinating research field. In this paper, the conception of sentiment computing with some core elements and feature vectors is defined, and some vital issues are proposed. Based on the theories mentioned above, the subjective content or objective content is classified by some special algorithms in the scenarios of single modal, such as text, image, audio and video data. Furthermore, how to merge these different kinds of data and to form the multimodal analysis methods for emotion detection is an important problem, and the fusion strategy is summarized in the paper. Finally, some trends about the sentiment cognition and sentiment generation are analyzed, which provides new ways for further research work.},
  keywords={Feature extraction;Speech;Sentiment analysis;Semantics;Fuses;Classification algorithms;Algorithm design and analysis;sentiment analysis;multimodal;social sentiment;opinion mining;sentiment generation},
  doi={10.1109/CIS.2017.00105},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{7890858,
  author={Ullah, Mohammad Aman and Islam, Md. Monirul and Binti Azman, Norhidayah and Zaki, Zulkifly Mohd},
  booktitle={2017 IEEE International Conference on Imaging, Vision & Pattern Recognition (icIVPR)}, 
  title={An overview of Multimodal Sentiment Analysis research: Opportunities and Difficulties}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={The scatter form of multimedia data such as text, image, audio, and video posted regularly in the social media may contain useful information for the organizations. But, this information should be derived with the use of some form of analysis known as Multimodal Sentiment Analysis (MSA). But, there is a lack of proper analytic tools for such analysis. This paper presents a thorough overview of more than fifty most recent MSA research articles to find the gaps in terms of tasks, approaches theories and applications used till date. There seems to be no single approach, theory, and tool which can support MSA. The study showed that each and every mode presents different difficulties which have not bee n fully solved yet, such as feature points of a face, voice clarity in audio, video summarization and so on, and are great research opportunities for the future researchers. Also, this research recommends a list of existing and upcoming difficulties and opportunities of MSA research.},
  keywords={Sentiment analysis;Visualization;Feature extraction;Face;Support vector machines;Semantics;Social network services;Multimodal;Sentiments;Opportunities;Difficulties;Review},
  doi={10.1109/ICIVPR.2017.7890858},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{7100481,
  author={Yadav, Sumit K and Bhushan, Mayank and Gupta, Swati},
  booktitle={2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)}, 
  title={Multimodal sentiment analysis: Sentiment analysis using audiovisual format}, 
  year={2015},
  volume={},
  number={},
  pages={1415-1419},
  abstract={Multimodal sentiment analysis is the analysis of emotions, attitude, and opinion from audiovisual format. A company can improve the quality of its product and services by analyzing the reviews about the product [5]. Sentiment analysis is widely used in managing customer relations. There are many textual reviews from which we cannot extract emotions by traditional sentiment analysis techniques. Some sentences in the textual reviews may derive deep emotions but do not contain any keyword to detect those emotions, so we used audiovisual reviews in order to detect emotions from the facial expressions of the customer. In this paper we take audiovisual input and extract emotions from video and audio in parallel from audiovisual input, finally classify the overall review as positive, negative or neutral based on the emotions detected.},
  keywords={Sentiment analysis;Feature extraction;Accuracy;Electronic mail;Eyebrows;Computer science;Logic gates;Analyzing reviews;facial expression;audiovisual;audio features},
  doi={},
  ISSN={},
  month={March},}@INPROCEEDINGS{10533605,
  author={Raj, Ankit and Raj, Mehul and Umasankari, N. and Geethanjali, D.},
  booktitle={2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)}, 
  title={Document-Based Text Summarization using T5 small and gTTS}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The research project on “Deep Learning-Based Text Summarization System using T5 small and gTTS” introduces a method to automatically extract and understand information from PDFs. The first step is to extract text from PDF files accurately. Following this, advanced natural language processing techniques are applied, utilizing a BERT-based model for sentiment analysis to identify emotional nuances in the text. Additionally, the integration of the T5 model streamlines the text summarization process, condensing extensive information into a clear and concise summary. The sophistication of the project is enhanced through the inclusion of Google Text-to-Speech (GTTS) capabilities, converting the written summary into an audio file. This feature accommodates various user preferences, improving overall accessibility. The research establishes a multimodal strategy for information dissemination, delivering the summary in both written and audio formats for a concise yet inclusive presentation. Beyond its technical contributions, the document summarization system has applications in education, content curation, and information retrieval. This system can assist educators in creating concise educational materials, support content curators in efficiently summarizing articles, and aid users in quickly extracting relevant information from large datasets, showcasing its versatility and potential impact across different fields. The integration of advanced natural language processing techniques underscores their adaptability and efficiency in handling textual data, ultimately enhancing the overall user experience and accommodating a broader spectrum of use.},
  keywords={Sentiment analysis;Scalability;Refining;Information retrieval;User experience;Real-time systems;Internet;Text-summarization;Sentiment analysis;gTTS;T5 small;audio summary;Text-extraction},
  doi={10.1109/ADICS58448.2024.10533605},
  ISSN={},
  month={April},}@ARTICLE{10190090,
  author={Lu, Qiang and Sun, Xia and Long, Yunfei and Gao, Zhizezhang and Feng, Jun and Sun, Tao},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Sentiment Analysis: Comprehensive Reviews, Recent Advances, and Open Challenges}, 
  year={2024},
  volume={35},
  number={11},
  pages={15092-15112},
  abstract={Sentiment analysis (SA) aims to understand the attitudes and views of opinion holders with computers. Previous studies have achieved significant breakthroughs and extensive applications in the past decade, such as public opinion analysis and intelligent voice service. With the rapid development of deep learning, SA based on various modalities has become a research hotspot. However, only individual modality has been analyzed separately, lacking a systematic carding of comprehensive SA methods. Meanwhile, few surveys covering the topic of multimodal SA (MSA) have been explored yet. In this article, we first take the modality as the thread to design a novel framework of SA tasks to provide researchers with a comprehensive understanding of relevant advances in SA. Then, we introduce the general workflows and recent advances of single-modal in detail, discuss the similarities and differences of single-modal SA in data processing and modeling to guide MSA, and summarize the commonly used datasets to provide guidance on data and methods for researchers according to different task types. Next, a new taxonomy is proposed to fill the research gaps in MSA, which is divided into multimodal representation learning and multimodal data fusion. The similarities and differences between these two methods and the latest advances are described in detail, such as dynamic interaction between multimodalities, and the multimodal fusion technologies are further expanded. Moreover, we explore the advanced studies on multimodal alignment, chatbots, and Chat Generative Pre-trained Transformer (ChatGPT) in SA. Finally, we discuss the open research challenges of MSA and provide four potential aspects to improve future works, such as cross-modal contrastive learning and multimodal pretraining models.},
  keywords={Task analysis;Data models;Sentiment analysis;Visualization;Representation learning;Data integration;Taxonomy;Multimodal data fusion;multimodal representation learning;multimodal;sentiment analysis (SA);single-modal},
  doi={10.1109/TNNLS.2023.3294810},
  ISSN={2162-2388},
  month={Nov},}@INPROCEEDINGS{10388135,
  author={Girard, Jeffrey M. and Tie, Yanmei and Liebenthal, Einat},
  booktitle={2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)}, 
  title={DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={In this paper, we describe the design, collection, and validation of a new video database that includes holistic and dynamic emotion ratings from 83 participants watching 22 affective movie clips. In contrast to previous work in Affective Computing, which pursued a single “ground truth” label for the affective content of each moment of each video (e.g., by averaging the ratings of 2 to 7 trained participants), we embrace the subjectivity inherent to emotional experiences and provide the full distribution of all participants’ ratings (with an average of 76.7 raters per video). We argue that this choice represents a paradigm shift with the potential to unlock new research directions, generate new hypotheses, and inspire novel methods in the Affective Computing community. We also describe several interdisciplinary use cases for the database: to provide dynamic norms for emotion elicitation studies (e.g., in psychology, medicine, and neuroscience), to train and test affective content analysis algorithms (e.g., for dynamic emotion recognition, video summarization, and movie recommendation), and to study subjectivity in emotional reactions (e.g., to identify moments of emotional ambiguity or ambivalence within movies, identify predictors of subjectivity, and develop personalized affective content analysis algorithms). The database is made freely available to researchers for noncommercial use at https://dynamos.mgb.org.},
  keywords={Affective computing;Emotion recognition;Neuroscience;Databases;Heuristic algorithms;Psychology;Motion pictures;database;emotion elicitation;content analysis;affective computing;subjectivity;multimodal;movie clips},
  doi={10.1109/ACII59096.2023.10388135},
  ISSN={2156-8111},
  month={Sep.},}@ARTICLE{10475492,
  author={Sharma, Shivam and S, Ramaneswaran and Akhtar, Md. Shad and Chakraborty, Tanmoy},
  journal={IEEE Transactions on Affective Computing}, 
  title={Emotion-Aware Multimodal Fusion for Meme Emotion Detection}, 
  year={2024},
  volume={15},
  number={3},
  pages={1800-1811},
  abstract={The ever-evolving social media discourse has witnessed an overwhelming use of memes to express opinions or dissent. Besides being misused for spreading malcontent, they are mined by corporations and political parties to glean the public's opinion. Therefore, memes predominantly offer affect-enriched insights towards ascertaining the societal psyche. However, the current approaches are yet to model the affective dimensions expressed in memes effectively. They rely extensively on large multimodal datasets for pre-training and do not generalize well due to constrained visual-linguistic grounding. In this paper, we introduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions. We then present ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a novel multimodal neural framework that (i) explicitly models emotion-enriched visual cues, and (ii) employs an efficient cross-modal fusion via a gating mechanism. Our investigation establishes ALFRED's superiority over existing baselines by 4.94% F1. Additionally, ALFRED competes strongly with previous best approaches on the challenging Memotion task. We then discuss ALFRED's domain-agnostic generalizability by demonstrating its dominance on two recently-released datasets – HarMeme and Dank Memes, over other baselines. Further, we analyze ALFRED's interpretability using attention maps. Finally, we highlight the inherent challenges posed by the complex interplay of disparate modality-specific cues toward meme analysis.},
  keywords={Task analysis;Emotion recognition;Social networking (online);Visualization;Mood;Affective computing;Internet;Emotion analysis;information fusion;memes;multimodality;social media},
  doi={10.1109/TAFFC.2024.3378698},
  ISSN={1949-3045},
  month={July},}@INPROCEEDINGS{8996846,
  author={Shen, Yiqing and Li, Yingbo and Peng, Qinke},
  booktitle={2019 Chinese Automation Congress (CAC)}, 
  title={The User-Pleasant Video Skimming by Multi-Sources-Indices Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={2938-2943},
  abstract={Video skimming is the process to extract the most significant content of the video and represent it in a concise form. Previous research of video skimming generation could not make full use of the multi-modal information and thoroughly consider the user preferences. In this paper, we propose a novel approach of video skimming through exploiting the fusion of video temporal information and semantic information, supported by audio and text classification and keyframe extraction. By means of analysis on multi-modal video information including audio, text and visual indices, one video skim is generated for one original video. In addition, we introduce the brand-safe filtering and sentiment analysis in order to only reserve the user-pleasant content. Experiment conducted on Youtube-8M dataset has proved that our method highly outperforms the approaches which only consider partial information in video. Moreover, with brand-safe filtering, the unpleasant content has been successfully removed.},
  keywords={video skimming;audio and text classification;keyframe extraction;multi-modal information fusion},
  doi={10.1109/CAC48633.2019.8996846},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{9753981,
  author={Dutta, Poulami and Bhattacharyya, Chandan Kumar},
  booktitle={2022 6th International Conference on Computing Methodologies and Communication (ICCMC)}, 
  title={Multi-Modal Sarcasm Detection in Social Networks: A Comparative Review}, 
  year={2022},
  volume={},
  number={},
  pages={207-214},
  abstract={Sentiment Analysis (SA) has become an extremely sought after area of research especially post COVID-19 when people used to spent a lot of time on the social media to interact with each other. This interaction was done through posts having both textual and visual cues and also by participating in online discussions forums. Some of the inherent challenges encountered in the process of SA include discernment of sarcasm, irony, humor, negation, multi-polarity or Aspect-Level Sentiment Analysis (ASA) etc. Researchers are now gradually shifting their focus to the identification and detection of sarcasm and how it can empower SA. Sarcasm expresses a person’s downside feelings by using positive words in an implicit way. It also has an overall impact on increasing the efficiency of the SA models. Eliciting sarcastic statements is tough for humans as well as for machines without the knowledge of the context or background in which it is expressed, body language and/or facial expression of the speaker and his voice modulation. This review paper studies some of the approaches used for sarcasm detection and also guides researchers in exploring the different modalities of data for developing applications like a virtual chat-bot or assistant, depression analysis, stress management system at workplace etc.},
  keywords={Deep learning;Sentiment analysis;Visualization;Discussion forums;Social networking (online);Employment;Sentiment Analysis;Sarcasm Detection;Negation Detection;Multi-Modal;Social Networks},
  doi={10.1109/ICCMC53470.2022.9753981},
  ISSN={},
  month={March},}@ARTICLE{9896953,
  author={Lew, Wai-Cheong Lincoln and Wang, Di and Ang, Kai Keng and Lim, Joo-Hwee and Quek, Chai and Tan, Ah-Hwee},
  journal={IEEE Transactions on Affective Computing}, 
  title={EEG-Video Emotion-Based Summarization: Learning With EEG Auxiliary Signals}, 
  year={2022},
  volume={13},
  number={4},
  pages={1827-1839},
  abstract={Video summarization is the process of selecting a subset of informative keyframes to expedite storytelling with limited loss of information. In this article, we propose an EEG-Video Emotion-based Summarization (EVES) model based on a multimodal deep reinforcement learning (DRL) architecture that leverages neural signals to learn visual interestingness to produce quantitatively and qualitatively better video summaries. As such, EVES does not learn from the expensive human annotations but the multimodal signals. Furthermore, to ensure the temporal alignment and minimize the modality gap between the visual and EEG modalities, we introduce a Time Synchronization Module (TSM) that uses an attention mechanism to transform the EEG representations onto the visual representation space. We evaluate the performance of EVES on the TVSum and SumMe datasets. Based on the rank order statistics benchmarks, the experimental results show that EVES outperforms the unsupervised models and narrows the performance gap with supervised models. Furthermore, the human evaluation scores show that EVES receives a higher rating than the state-of-the-art DRL model DR-DSN by 11.4% on the coherency of the content and 7.4% on the emotion-evoking content. Thus, our work demonstrates the potential of EVES in selecting interesting content that is both coherent and emotion-evoking.},
  keywords={Electroencephalography;Brain modeling;Visualization;Semantics;Reinforcement learning;Annotations;Supervised learning;Video summarization;EEG-video representation;emotion-evoking;multimodality},
  doi={10.1109/TAFFC.2022.3208259},
  ISSN={1949-3045},
  month={Oct},}@ARTICLE{8012375,
  author={Harakawa, Ryosuke and Ogawa, Takahiro and Haseyama, Miki},
  journal={IEEE Access}, 
  title={Extracting Hierarchical Structure of Web Video Groups Based on Sentiment-Aware Signed Network Analysis}, 
  year={2017},
  volume={5},
  number={},
  pages={16963-16973},
  abstract={Sentiment in multimedia contents has an influence on their topics, since multimedia contents are tools for social media users to convey their sentiment. Performance of applications such as retrieval and recommendation will be improved if sentiment in multimedia contents can be estimated; however, there have been few works in which such applications were realized by utilizing sentiment analysis. In this paper, a novel method for extracting the hierarchical structure of Web video groups based on sentiment-aware signed network analysis is presented to realize Web video retrieval. First, the proposed method estimates latent links between Web videos by using multimodal features of contents and sentiment features obtained from texts attached to Web videos. Thus, our method enables construction of a signed network that reflects not only similarities but also positive and negative relations between topics of Web videos. Moreover, an algorithm to optimize a modularity-based measure, which can adaptively adjust the balance between positive and negative edges, was newly developed. This algorithm detects Web video groups with similar topics at multiple abstraction levels; thus, successful extraction of the hierarchical structure becomes feasible. By providing the hierarchical structure, users can obtain an overview of many Web videos and it becomes feasible to successfully retrieve the desired Web videos. Results of experiments using a new benchmark dataset, YouTube-8M, validate the contributions of this paper, i.e., 1) the first attempt to utilize sentiment analysis for Web video grouping and 2) a novel algorithm for analyzing a weighted signed network derived from sentiment and multimodal features.},
  keywords={Multimedia communication;Streaming media;Feature extraction;Algorithm design and analysis;Sentiment analysis;Social network services;Tools;Video retrieval;video clustering;network analysis;signed network;sentiment analysis},
  doi={10.1109/ACCESS.2017.2741098},
  ISSN={2169-3536},
  month={},}@ARTICLE{7579221,
  author={Stratou, Giota and Morency, Louis-Philippe},
  journal={IEEE Transactions on Affective Computing}, 
  title={MultiSense—Context-Aware Nonverbal Behavior Analysis Framework: A Psychological Distress Use Case}, 
  year={2017},
  volume={8},
  number={2},
  pages={190-203},
  abstract={During face-to-face interactions, people naturally integrate nonverbal behaviors such as facial expressions and body postures as part of the conversation to infer the communicative intent or emotional state of their interlocutor. The interpretation of these nonverbal behaviors will often be contextualized by interactional cues such as the previous spoken question, the general discussion topic or the physical environment. A critical step in creating computers able to understand or participate in this type of social face-to-face interactions is to develop a computational platform to synchronously recognize nonverbal behaviors as part of the interactional context. In this platform, information for the acoustic and visual modalities should be carefully synchronized and rapidly processed. At the same time, contextual and interactional cues should be remembered and integrated to better interpret nonverbal (and verbal) behaviors. In this article, we introduce a real-time computational framework, MultiSense, which offers flexible and efficient synchronization approaches for context-based nonverbal behavior analysis. MultiSense is designed to utilize interactional cues from both interlocutors (e.g., from the computer and the human participant) and integrate this contextual information when interpreting nonverbal behaviors. MultiSense can also assimilate behaviors over a full interaction and summarize the observed affective states of the user. We demonstrate the capabilities of the new framework with a concrete use case from the mental health domain where MultiSense is used as part of a decision support tool to assess indicators of psychological distress such as depression and post-traumatic stress disorder (PTSD). In this scenario, MultiSense not only infers psychological distress indicators from nonverbal behaviors but also broadcasts the user state in real-time to a virtual agent (i.e., a digital interviewer) designed to conduct semi-structured interviews with human participants. Our experiments show the added value of our multimodal synchronization approaches and also demonstrate the importance of MultiSense contextual interpretation when inferring distress indicators.},
  keywords={Real-time systems;Synchronization;Computer architecture;Pipelines;Psychology;Affective computing;Context;MultiSense;system for affective computing;behavior quantification;automatic distress assessment;framework for multimodal behavioral understanding},
  doi={10.1109/TAFFC.2016.2614300},
  ISSN={1949-3045},
  month={April},}@INPROCEEDINGS{8171587,
  author={Sonone, Prajakta and Deorankar, A. V.},
  booktitle={2017 International Conference on Recent Trends in Electrical, Electronics and Computing Technologies (ICRTEECT)}, 
  title={Multi-modal Topic Modelling and Summarization with Dense Block Detection: A Review}, 
  year={2017},
  volume={},
  number={},
  pages={177-182},
  abstract={There has been incredible growth of events over the internet in recent years. Google has become the giant source of knowledge for any event which has happened or happening over the internet. Some networking sites such as face book, micro blogging sites such as twitter are evolved with time and became the highly used sites over the internet. Various E-commerce websites such as Amazon, Ebay, Flipkart etc are the widely used sites for online shopping. These sites generates large amount of text data. In association with text data some images are also uploaded over the internet on these sites. To model this huge amount of multi-modal data having both textual and visual contents multi-modal topic model for summarization, analysis is suggested in this paper. While dealing with multimodality, study of semantic relationship between the images and text data is crucial part. This model also helps to study semantic relationship between them effectively. Topics which are trending, popular over the world can be seen on Social sites as well as micro blogging sites. In online shopping sites fake reviews, advertises, spam spreading information is posted. For summarizing and analyzing the data we have taken the dataset containing reviews and product information from Amazon, one of the leading E-commerce sites. This information is used for modeling topic on sites with summarization and analysis. In this paper detailed study of other previous methods is also shown.},
  keywords={Semantics;Visualization;Analytical models;Data models;Correlation;Data mining;Java;Event tracking;event analysis;multi-modality;suspicious behavior;topic model},
  doi={10.1109/ICRTEECT.2017.45},
  ISSN={},
  month={July},}@INPROCEEDINGS{10473534,
  author={Fantini, Alessia and Pilato, Giovanni and Vitale, Gianpaolo},
  booktitle={2023 Seventh IEEE International Conference on Robotic Computing (IRC)}, 
  title={Consistency, Uncertainty or Inconsistency Detection in Multimodal Emotion Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={377-380},
  abstract={Humans exploit several sensory channels to recognize emotions and combine the information coming from the different channels into a single perception. Emotion Perception (EP) is also closely related to the Theory of Mind (ToM), which includes processes that capture socially and emotionally related inputs; furthermore, it interprets their meaning and direct responses accordingly. In this paper, we present a first step towards recognizing incoherence in emotions that exploits a three-level cognitive architecture. Starting with multimodal emotion recognition, a decision-maker determines whether a situation of consistency, uncertainty, or inconsistency exists and ultimately attempts to identify which case occurs. The detection is based on a suitable vector representation, in the conceptual level of the architecture, of moods on Russel diagram. A system designed in this way can impact HRI in terms of effectiveness by allowing a robot to get an idea about the actual emotional state of the person it interacts with.},
  keywords={Emotion recognition;Uncertainty;Mood;Human-robot interaction;Coherence;Robot sensing systems;Vectors;Emotion Detection;Mood;Human-Robot Interaction;Inconsistency detection},
  doi={10.1109/IRC59093.2023.00067},
  ISSN={},
  month={Dec},}@ARTICLE{9376096,
  author={Tseng, Shao-Yen and Narayanan, Shrikanth and Georgiou, Panayiotis},
  journal={IEEE Signal Processing Letters}, 
  title={Multimodal Embeddings From Language Models for Emotion Recognition in the Wild}, 
  year={2021},
  volume={28},
  number={},
  pages={608-612},
  abstract={Word embeddings such as ELMo and BERT have been shown to model word usage in language with greater efficacy through contextualized learning on large-scale language corpora, resulting in significant performance improvement across many natural language processing tasks. In this work we integrate acoustic information into contextualized lexical embeddings through the addition of a parallel stream to the bidirectional language model. This multimodal language model is trained on spoken language data that includes both text and audio modalities. We show that embeddings extracted from this model integrate paralinguistic cues into word meanings and can provide vital affective information by applying these multimodal embeddings to the task of speaker emotion recognition.},
  keywords={Acoustics;Task analysis;Feature extraction;Convolution;Emotion recognition;Context modeling;Bit error rate;Machine learning;unsupervised learning;natural language processing;speech processing;emotion recognition},
  doi={10.1109/LSP.2021.3065598},
  ISSN={1558-2361},
  month={},}@INPROCEEDINGS{9533473,
  author={Cheng, Huijie and Tie, Yun and Qi, Lin and Jin, Cong},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Context-Aware Based Visual-Audio Feature Fusion for Emotion Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Video emotion recognition is a significant branch in the field of emotion computing. However, traditional recognition works mainly focus on human features, ignoring the contextual clues of video scenes and objects. In our work, we propose a context-aware framework for bi-modal video emotion recognition. Unlike existing methods that directly extract features of the entire video frame, we extract key frames and key regions of videos to obtain emotional cues contained in video scenes and objects. Specifically, for visual stream, the hierarchical Bidirectional Long-Short Term Memory (Bi-LSTM) is applied to summarize video scenes and find key frames that mostly contribute to video emotion; Meantime, we introduce the Region Proposal Network (RPN) to extract corresponding features of object regions in video frames and construct the emotional similarity graph. After using the Feedforward Neural Network (FNN) to assign different weight coefficients to different regions, the Graph Convolutional Network (GCN) is used to reason about the connections between key regions. Moreover, the context information of the frame-level Log-Mel spectrum fragments supplement the visual information. Finally, we fuse the visual and acoustics features by adaptive gated multimodal fusion module for video emotion classification. We conduct experiments on Video Emotion-8 and Ekman-6 datasets. The experimental results demonstrate that our model achieves better classification accuracy than several baseline models.},
  keywords={Emotion recognition;Visualization;Adaptation models;Fuses;Streaming media;Logic gates;Feature extraction;multimodal emotion recognition;video scenes summarization;relationship reasoning;graph convolutional network},
  doi={10.1109/IJCNN52387.2021.9533473},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10665103,
  author={Wang, Zhaoyang and Li, Li and He, Ketai},
  booktitle={2024 IEEE 19th Conference on Industrial Electronics and Applications (ICIEA)}, 
  title={Deep User Profile Construction and Behavior Prediction Based on Multimodal Heterogeneous Feature Fusion}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={As the platform for users to express their personal opinions and thoughts, Online Social Networks (OSNs) serve as a primary channel for most individuals to receive social information. During social emergencies, the OSNs experience a surge in discussions, some of which may be malicious and negatively affect public opinion. In this paper, we manually construct a dataset of malicious users based on Twitter, named Malicious_Users_2023 (MU2023). Beyond the tweet features, we summarize three sets of supplementary features for malicious user monitoring: the User Feature, the Network Feature, and the High-dimensional Feature. Leveraging all sets of features, a scheme for malicious individuals monitoring utilizing multimodal feature fusion analysis is proposed. Experimental results show that the proposed scheme achieves 84% accuracy on the MU2023 dataset and 96% on the Apontador dataset. Furthermore, we validate the performance enhancement of monitoring with three additional sets of features and two feature fusion approaches, respectively. This is crucial for comprehending and modeling individuals' malicious behavior effectively.},
  keywords={Industrial electronics;Accuracy;Social networking (online);Stacking;Noise;Blogs;Feature extraction;Online Social Networks;Malicious User Monitoring;Multimodal Features;Feature Fusion},
  doi={10.1109/ICIEA61579.2024.10665103},
  ISSN={2158-2297},
  month={Aug},}@INPROCEEDINGS{9391501,
  author={Wang, Yiding and Wang, Zhenyi and Li, Chenghao and Zhang, Yilin and Wang, Haizhou},
  booktitle={2020 IEEE 39th International Performance Computing and Communications Conference (IPCCC)}, 
  title={A Multimodal Feature Fusion-Based Method for Individual Depression Detection on Sina Weibo}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Existing studies have shown that various types of information on the online social network (OSN) can help predict the early stage of depression. However, studies using machine learning methods to accomplish depression detection tasks still do not have high classification performance, suggesting that there is much potential for improvement in their feature engineering. In this paper, we first construct a dataset on Sina Weibo (a leading OSN with the largest number of active users in the Chinese community), namely the Weibo User Depression Detection Dataset (WU3D). It includes more than 10,000 depressed users and 20,000 normal users, both of which are manually labeled and rechecked by specialists. Then, we extract text-based word features using the popular pretrained model XLNet and summarize nine statistical features related to user text, social behavior, and pictures. Moreover, we construct a deep neural network classification model, i.e. Multimodal Feature Fusion Network (MFFN), to fuse the above-extracted features from different information sources and further accomplish the classification task. The experimental results show that our approach achieves an F1-Score of 0.9685 on the test dataset, which has a good performance improvement compared to the existing works. In addition, we verify that our multimodal detecting approach is more robust than multimodel ensemble ones. Our work could also provide new research methods for depression detection on other OSN platforms.},
  keywords={Training;Social networking (online);Blogs;Depression;Feature extraction;Robustness;Task analysis;Depression detection;online social network;feature engineering;deep learning;multimodal fusion},
  doi={10.1109/IPCCC50635.2020.9391501},
  ISSN={2374-9628},
  month={Nov},}@ARTICLE{9226102,
  author={Niu, Mingyue and Tao, Jianhua and Liu, Bin and Huang, Jian and Lian, Zheng},
  journal={IEEE Transactions on Affective Computing}, 
  title={Multimodal Spatiotemporal Representation for Automatic Depression Level Detection}, 
  year={2023},
  volume={14},
  number={1},
  pages={294-307},
  abstract={Physiological studies have shown that there are some differences in speech and facial activities between depressive and healthy individuals. Based on this fact, we propose a novel spatio-temporal attention (STA) network and a multimodal attention feature fusion (MAFF) strategy to obtain the multimodal representation of depression cues for predicting the individual depression level. Specifically, we first divide the speech amplitude spectrum/video into fixed-length segments and input these segments into the STA network, which not only integrates the spatial and temporal information through attention mechanism, but also emphasizes the audio/video frames related to depression detection. The audio/video segment-level feature is obtained from the output of the last full connection layer of the STA network. Second, this article employs the eigen evolution pooling method to summarize the changes of each dimension of the audio/video segment-level features to aggregate them into the audio/video level feature. Third, the multimodal representation with modal complementary information is generated using the MAFF and inputs into the support vector regression predictor for estimating depression severity. Experimental results on the AVEC2013 and AVEC2014 depression databases illustrate the effectiveness of our method.},
  keywords={Feature extraction;Depression;Two dimensional displays;Spatiotemporal phenomena;Databases;Three-dimensional displays;Image segmentation;Multimodal depression detection;spatio-temporal attention;audio/video segment-level feature;eigen evolution pooling;audio/video level feature;multimodal attention feature fusion},
  doi={10.1109/TAFFC.2020.3031345},
  ISSN={1949-3045},
  month={Jan},}@INPROCEEDINGS{10205645,
  author={Singh, Gurpreet and Chaudhary, Mansi and Singh, Jaspreet and Madaan, Nikita},
  booktitle={2023 3rd International Conference on Intelligent Technologies (CONIT)}, 
  title={Parameter Design Approaches based on AI Techniques for Transformer Neural Network Optimization}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The Transformer neural network is a magnificent monster of the deep learning world, demanding attention with its capacity to digest data sequences with unmatched efficacy. It has a towering self-attention mechanism and powerful feedforward layers. People are in awe of its skills as a result of its skill in multimodal activities, language interpretation, and picture processing. Certainly, artificial intelligence (AI) continues to advance more quickly. Therefore, the preparation, procedure, preservation, and commercialization of the energy infrastructure are hotspots for research approaches based on data-driven technologies AI. In several areas of picture analysis and evaluating, including self-driving automobiles, it performs remarkably well. A brief overview of the transformer neural network and the specified input define are presented in this research. Next, a transformer neural network design accompanying model for tasks involving natural language processing is outlined. Following that, papers relating to distinct transformer neural network types together with distinct techniques and approaches have been addressed here through multiple points of view of different scenarios. This study additionally outlines many transformer neural network applications. Finally, various artificial intelligence-based strategies for transformer design optimising and prospective directions are highlighted.},
  keywords={Image processing;Neural networks;Transforms;Transformers;Natural language processing;Artificial intelligence;Task analysis;Transformer Neural Network;Transformer Neural Network Design;Application of Transform NN;Approaches for TNNs;Artificial Intelligence Techniques for Transformer Design},
  doi={10.1109/CONIT59222.2023.10205645},
  ISSN={},
  month={June},}@INPROCEEDINGS{9874651,
  author={Goyal, Pratham and Raj, Anjali and Kumar, Puneet and Nampalle, Kishore Babu},
  booktitle={2022 IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR)}, 
  title={Automatic Evaluation of Machine Generated Feedback For Text and Image Data}, 
  year={2022},
  volume={},
  number={},
  pages={413-418},
  abstract={In this paper, a novel system, ‘AutoEvaINet,’ has been developed for evaluating machine-generated feedback in response to multimodal input containing text and images. A new metric, ‘Automatically Evaluated Relevance Score’ (AER Score), has also been defined to automatically compute the similarity between human-generated comments and machine-generatedfeedback. The AutoEvalNet's architecture comprises a pre-trained feedback synthesis model and the proposed feedback evaluation model. It uses an ensemble of Bidirectional Encoder Representations from Transformers (BERT) and Global Vectors for Word Representation (GloVe) models to generate the embeddings of the ground-truth comment and machine-synthesized feedback using which the similarity score is calculated. The experiments have been performed on the MMFeed dataset. The generated feedback has been evaluated automatically using the AER score and manually by having the human users evaluate the feedbackfor relevance to the input and ground-truth comments. The values of the AER score and human evaluation scores are in line, affirming the AER score's applicability as an automatic evaluation measure for machine-generated text instead of human evaluation.},
  keywords={Measurement;Computational modeling;Semantics;Bit error rate;Multimedia computing;Manuals;Information processing;Multimodal Feedback Analysis;Automatic Evaluation;Similarity Score;Affective Computing},
  doi={10.1109/MIPR54900.2022.00081},
  ISSN={2770-4319},
  month={Aug},}@ARTICLE{10343095,
  author={Hassija, Vikas and Chakrabarti, Arjab and Singh, Anushka and Chamola, Vinay and Sikdar, Biplab},
  journal={IEEE Access}, 
  title={Unleashing the Potential of Conversational AI: Amplifying Chat-GPT’s Capabilities and Tackling Technical Hurdles}, 
  year={2023},
  volume={11},
  number={},
  pages={143657-143682},
  abstract={Conversational AI has seen a growing interest among government, researchers, and industrialists. This comprehensive survey paper provides an in-depth analysis of large language models, specifically focusing on ChatGPT. This paper discusses the architecture, training process, and challenges associated with large language models, including bias, interpretability, and ethics. It explores various applications of ChatGPT and examines future research trends, such as improving model generalization, addressing data scarcity, and integrating multimodal capabilities. This survey also serves as a roadmap for researchers, practitioners, and policymakers, offering valuable insights into the current state and future potential of large language models and ChatGPT.},
  keywords={Chatbots;Transformers;Artificial intelligence;Context modeling;Usability;Surveys;Analytical models;Natural language processing;Deep learning;Neural networks;Sentiment analysis;Large language models;ChatGPT;natural language processing;deep learning;neural networks;transformer models;pre-training and fine-tuning;language generation;text completion;model interpretability;bias in language models;ethics in AI;data scarcity;multimodal models;generalization;conversational AI;language understanding;text classification;sentiment analysis;dialogue systems},
  doi={10.1109/ACCESS.2023.3339553},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10157066,
  author={Barveen, A. and Geetha, S. and Faizal, M.K.Mohamed},
  booktitle={2023 Second International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)}, 
  title={Meme Expressive Classification in Multimodal State with Feature Extraction in Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Memes are a socially interactive way to communicate online. Memes are used by users to communicate with one another on social networking sites and other forums. Memes essentially focus on speech recognition and image macros. While a meme is being created, it focuses on the semiotic type of resources that the internet community interprets with other resources, which facilitates the interaction among the internet and meme creators. Memes recreate based on various approaches, which fall under various acts such as existing speech acts. Based on the expressive face with captioned short texts, even the short text is exaggerated. Every year, meme mimicking applications are created that allow users to use the imitated meme expressions. Memes represent the shared texts of the younger generations on various social platforms. The classifications of sentiment based on the various memetic expressions are the most efficient way to analyse those feelings and emotions. HOG feature extraction allows the images to be segmented into blocks of smaller size by using a single feature vector for dimension, which characterizes the local object appearances to characterize the meme classification. The existence of specific characteristics, including such edges, angles, or patterns, is then analyzed by combining HOG features using multi-feature analysis on patches. Based upon the classification methodology, it classifies the sentiments, which tend to improve the learning process in an efficient manner. By combining a deep learning approach with a recurrent neural network, the extended LSTM-RNN can identify subtle nuances in memes, allowing for more accurate and detailed meme classification. This proposed method effectively evaluates several classification techniques, including CNN and Extended LSTM-RNN for meme image characterization. Through training and validation, Extended LSTM-RNN achieved 0.98% accuracy with better performance than CNN.},
  keywords={Deep learning;Training;Memetics;Sentiment analysis;Recurrent neural networks;Social networking (online);Speech recognition;Sentiment Expressions;Deep Learning;Classification;Feature Extraction;Meme Classification;CNN},
  doi={10.1109/ICEEICT56924.2023.10157066},
  ISSN={},
  month={April},}@INPROCEEDINGS{10503756,
  author={Wang, Teng and Zhu, Qingsong},
  booktitle={2024 IEEE 7th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
  title={ChatGPT – Technical Research Model, Capability Analysis, and Application Prospects}, 
  year={2024},
  volume={7},
  number={},
  pages={787-796},
  abstract={ChatGPT is one of the most advanced research achievements in the field of natural language processing. In order to showcase the research technology and application value of the large language model ChatGPT, this article provides a comprehensive summary of its development history, research status, and key technologies. This paper focuses on key technologies such as large-scale corpus training, support for high computing power, the fundamental model architecture of the Transformer, and reinforcement learning from human feedback. Three types of features are analyzed: the ChatGPT model GPT-4 Turbo, custom ChatGPT, and the multimodal API. In light of the limitations of technology based on current deep neural networks, which rely on big data and extensive computing power, and the presence of illusory performance, several suggestions are being proposed. The revolutionary applications of ChatGPT in the fields of education, search engines, metaverse, and high-tech enterprises are summarized. Finally, the full text is summarized and prospected.},
  keywords={Training;Ethics;Analytical models;Technological innovation;Computational modeling;Computer architecture;Reinforcement learning;large language models;natural language processing;chatgpt;reinforcement learning from human feedback;transformer},
  doi={10.1109/IAEAC59436.2024.10503756},
  ISSN={2689-6621},
  month={March},}@ARTICLE{9376944,
  author={Veltmeijer, Emmeke A. and Gerritsen, Charlotte and Hindriks, Koen V.},
  journal={IEEE Transactions on Affective Computing}, 
  title={Automatic Emotion Recognition for Groups: A Review}, 
  year={2023},
  volume={14},
  number={1},
  pages={89-107},
  abstract={This article aims to summarize and describe research on the topic of automatic group emotion recognition. In recent years, the topic of emotion analysis of groups or crowds has gained interest, with studies performing emotion detection in different contexts, using different datasets and modalities (such as images, video, audio, social media messages), and taking different approaches. Articles are included after an innovative search method, including Dense Query Extraction and automatic cross-referencing. Discussed are the types of groups and emotion models considered in automatic emotion recognition research, common datasets for all modalities, general approaches taken, and reported performances. These performances are discussed, followed by an analysis of the application possibilities of the discussed methods. To ensure clear, replicable, and comparable studies, we suggest research should test on multiple, common datasets and report on multiple metrics, when possible. Implementation details and code should be made available where possible. An area of interest for future work is to build systems with more real-world application possibilities, coping with changing group sizes, different emotional subgroups, and changing emotions over time, while having a higher robustness and working with datasets with reduced biases.},
  keywords={Emotion recognition;Licenses;Visualization;Systematics;Sentiment analysis;Mood;Media;Group emotion review;crowd mood survey;automatic affect recognition;sentiment analysis},
  doi={10.1109/TAFFC.2021.3065726},
  ISSN={1949-3045},
  month={Jan},}@ARTICLE{1369348,
  author={Xiangyang Li and Qiang Ji},
  journal={IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans}, 
  title={Active affective State detection and user assistance with dynamic bayesian networks}, 
  year={2005},
  volume={35},
  number={1},
  pages={93-105},
  abstract={With the rapid development of pervasive and ubiquitous computing applications, intelligent user-assistance systems face challenges of ambiguous, uncertain, and multimodal sensory observations, user's changing state, and various constraints on available resources and costs in making decisions. We introduce a new probabilistic framework based on the dynamic Bayesian networks (DBNs) to dynamically model and recognize user's affective states and to provide the appropriate assistance in order to keep user in a productive state. We incorporate an active sensing mechanism into the DBN framework to perform purposive and sufficing information integration in order to infer user's affective state and to provide correct assistance in a timely and efficient manner. Experiments involving both synthetic and real data demonstrate the feasibility of the proposed framework as well as the effectiveness of the proposed active sensing strategy.},
  keywords={Bayesian methods;Systems engineering and theory;Context modeling;Ubiquitous computing;Intelligent systems;Intelligent networks;Intelligent sensors;Face detection;Costs;Active fusion;affective state detection;Bayesian networks (BNs);user assistance},
  doi={10.1109/TSMCA.2004.838454},
  ISSN={1558-2426},
  month={Jan},}@INPROCEEDINGS{8806449,
  author={Karslıoğlu, Nihan and Kaya, Heysem and Salah, Albert Ali},
  booktitle={2019 27th Signal Processing and Communications Applications Conference (SIU)}, 
  title={Movie Emotion Estimation with Multimodal Fusion and Synthetic Data Generation}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  abstract={In this work, we propose a method for automatic emotion recognition from movie clips. This problem has applications in indexing and retrieval of large movie and video collections, summarization of visual content, selection of emotioninvoking materials, and such. Our approach aims to estimate valence and arousal values automatically. We extract audio and visual features, summarize them via functionals, PCA, and Fisher vector encoding approaches. We used feature selection based on canonical correlation analysis. For classification, we used extreme learning machine and support vector machine. We tested our approach on the LIRIS-ACCEDE database with ground truth annotations. The class imbalance problem was solved by generating synthetic data. By fusing the best features at score and feature level, we obtain good results on this problem, especially for the valence prediction.},
  keywords={Principal component analysis;Gold;Mel frequency cepstral coefficient;Support vector machines;Dogs;Motion pictures;Feature extraction;Movie analysis;affective computing;multimodal fusion},
  doi={10.1109/SIU.2019.8806449},
  ISSN={2165-0608},
  month={April},}@ARTICLE{9693238,
  author={Zheng, Jiahao and Zhang, Sen and Wang, Zilu and Wang, Xiaoping and Zeng, Zhigang},
  journal={IEEE Transactions on Multimedia}, 
  title={Multi-Channel Weight-Sharing Autoencoder Based on Cascade Multi-Head Attention for Multimodal Emotion Recognition}, 
  year={2023},
  volume={25},
  number={},
  pages={2213-2225},
  abstract={Multimodal Emotion Recognition is challenging because of the heterogeneity gap among different modalities. Due to the powerful ability of feature abstraction, Deep Neural Networks (DNNs) have exhibited significant success in bridging the heterogeneity gap in cross-modal retrieval and generation tasks. In this work, a DNNs-based Multi-channel Weight-sharing Autoencoder with Cascade Multi-head Attention (MCWSA-CMHA) is proposed to generically address the affective heterogeneity gap in MER. Specifically, multimodal heterogeneity features are extracted by multiple independent encoders, and then a scalable heterogeneous feature fusion module (CMHA) is realized by connecting multiple multi-head attention modules in series. The core of the proposed algorithm is to reduce the heterogeneity between the output features of different encoders through the unsupervised training of MCWSA, and then to model the affective interactions between different modal features through the supervised training of CMHA. Experimental results demonstrate that the proposed MCWSA-CMHA achieves outperformance on two publicly available datasets compared with the state-of-the-art techniques. In addition, visualization experiments and approximation experiments are used to verify the effectiveness of each module in the proposed algorithm, and the experimental results show that the proposed MCWSA-CMHA can mine more emotion-related information among multimodal features compared with other fusion methods.},
  keywords={Feature extraction;Emotion recognition;Data models;Training;Task analysis;Decoding;Data mining;Multimodal emotion recognition (MER);autoencoder;multi-head attention mechanism},
  doi={10.1109/TMM.2022.3144885},
  ISSN={1941-0077},
  month={},}@ARTICLE{10807333,
  author={Xue, Junxiao and Wang, Jie and Liu, Xiaozhen and Zhang, Qian and Wu, Xuecheng},
  journal={Big Data Mining and Analytics}, 
  title={Affective Video Content Analysis: Decade Review and New Perspectives}, 
  year={2025},
  volume={8},
  number={1},
  pages={118-144},
  abstract={Video content is rich in semantics and has the ability to evoke various emotions in viewers. In recent years, with the rapid development of affective computing and the explosive growth of visual data, Affective Video Content Analysis (AVCA) as an essential branch of affective computing has become a widely researched topic. In this study, we comprehensively review the development of AVCA over the past decade, particularly focusing on the most advanced methods adopted to address the three major challenges of video feature extraction, expression subjectivity, and multimodal feature fusion. We first introduce the widely used emotion representation models in AVCA and describe commonly used datasets. We summarize and compare representative methods in the following aspects: (1) unimodal AVCA models, including facial expression recognition and posture emotion recognition; (2) multimodal AVCA models, including feature fusion, decision fusion, and attention-based multimodal models; and (3) model performance evaluation standards. Finally, we discuss future challenges and promising research directions, such as emotion recognition and public opinion analysis, human-computer interaction, and emotional intelligence.},
  keywords={Emotion recognition;Analytical models;Affective computing;Visualization;Reviews;Semantics;Psychology;Feature extraction;Data mining;Standards;affective computing;video emotion;video feature extraction;machine learning;emotional intelligence},
  doi={10.26599/BDMA.2024.9020048},
  ISSN={2097-406X},
  month={February},}@INPROCEEDINGS{10829282,
  author={Herur, Sanjana V and Shalini, M and Bhuvan, A J and Jain, Vanshika and Preethi, P},
  booktitle={2024 International Conference on Intelligent Systems and Advanced Applications (ICISAA)}, 
  title={Beyond Words: Multi-modal Chat Summarization using Text with Emoji and GIF Emotion Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a novel approach that signifies a methodological change in the way that a conversation is summarized in a chat. The term “Beyond Words” implies that the chat summarization process transcends conventional text-based techniques. It suggests that in addition to the textual content, the summarization takes into account extra elements like the emotions conveyed by emojis and GIFs. “Beyond Words” highlights a more thorough and inclusive method of summarizing chats by considering visual cues like emojis and GIFs that enhance communication in addition to written text. Our methodology aims to provide a more comprehensive and expressive representation of conversations by integrating these multimodal components. The paper describes the development and fine-tuning of transformer model using available familiar datasets. Furthermore, incorporating emotion detection from GIFs using a fine-tuned CNN pre-trained model, as well as emotion analysis of chat emojis, improve the summarization process. The results show that our approach is effective at capturing the essence of conversations, including not only words but also the emotive and visual dimensions inherent in modern digital communication. This project presents a fresh approach to chat summarization, allowing users to understand lengthy conversations in a clear and informative format.},
  keywords={Visualization;Emotion recognition;Analytical models;Text recognition;Oral communication;Transformers;Digital communication;Intelligent systems;Emojis;Chat Summarization;Multimodal Communication;Emotion Detection;Emoji Sentiment Analysis;GIF Emotion Recognition;Transformer Models},
  doi={10.1109/ICISAA62385.2024.10829282},
  ISSN={},
  month={Oct},}@ARTICLE{10206439,
  author={Shi, Kaize and Peng, Xueping and Lu, Hao and Zhu, Yifan and Niu, Zhendong},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Multiple Knowledge-Enhanced Meteorological Social Briefing Generation}, 
  year={2024},
  volume={11},
  number={2},
  pages={2002-2013},
  abstract={Frequent meteorological disasters present new challenges for decision-making in disaster response. As a timely and effective source of intelligent information, social media plays a vital role in detecting and monitoring these situations. Meteorological social briefings summarize valuable information from numerous social media posts, providing essential decision-support services. This article proposes a multi-knowledge-enhanced summarization (MKES) model for automatically generating meteorological social briefing content from multiple Sina Weibo posts. The MKES model consists of a summary generation module and a knowledge enhancement module. The knowledge enhancement module guides and constrains the summary generation process using meteorological events and geographical location knowledge, resulting in summaries that focus on describing specific knowledge from the source text. The MKES model outperforms baseline models in content evaluation, as measured by  $\text {ROUGE-1}$ ,  $\text {ROUGE-2}$ , and  $\text {ROUGE-L}$  scores, and in sentiment evaluation, as measured by  $F_{1}$  scores. Based on the MKES model, a framework for generating meteorological social briefings is developed, providing decision support services for the China Meteorological Administration (CMA).},
  keywords={Blogs;Computational modeling;Social networking (online);Decoding;Mathematical models;Semantics;Text recognition;Emergency services;Disasters;Controllable text generation;decision support service;emergency management;meteorological social briefing;natural disaster;social weather},
  doi={10.1109/TCSS.2023.3298252},
  ISSN={2329-924X},
  month={April},}@INPROCEEDINGS{5711541,
  author={Tjondronegoro, Dian and Tao, Xiaohui and Sasongko, Johannes and Lau, Cher Han},
  booktitle={2011 IEEE Workshop on Applications of Computer Vision (WACV)}, 
  title={Multi-modal summarization of key events and top players in sports tournament videos}, 
  year={2011},
  volume={},
  number={},
  pages={471-478},
  abstract={To detect and annotate the key events of live sports videos, we need to tackle the semantic gaps of audio-visual information. Previous work has successfully extracted semantic from the time-stamped web match reports, which are synchronized with the video contents. However, web and social media articles with no time-stamps have not been fully leveraged, despite they are increasingly used to complement the coverage of major sporting tournaments. This paper aims to address this limitation using a novel multimodal summarization framework that is based on sentiment analysis and players' popularity. It uses audiovisual contents, web articles, blogs, and commentators' speech to automatically annotate and visualize the key events and key players in a sports tournament coverage. The experimental results demonstrate that the automatically generated video summaries are aligned with the events identified from the official website match reports.},
  keywords={Videos;Games;Media;Text analysis;Speech;Blogs;Feature extraction},
  doi={10.1109/WACV.2011.5711541},
  ISSN={1550-5790},
  month={Jan},}@INPROCEEDINGS{10743383,
  author={Guo, Shuang and Ning, Bo},
  booktitle={2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)}, 
  title={A Review of Empathetic Conversational Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1279-1286},
  abstract={With the rapid development of natural language processing (NLP) technology, there has been increasing research on dialogue systems, as traditional dialogue systems have exhibited a series of flaws. As an important branch of dialogue systems, empathetic conversational systems have gained widespread attention from researchers. Research has shown that incorporating emotions and cognitive reasoning into dialogue systems can achieve superior results. In this paper, we introduce empathetic conversational systems from five aspects. Firstly, we discuss the concepts, definition, and development of empathetic conversational systems, and list several common datasets and standards. Additionally, based on an innovative classification of the main structure of empathetic dialogue system models, we summarize the current major methods and model structures. Finally, we discuss the shortcomings of previous work and future directions for empathetic conversational systems.},
  keywords={Measurement;Analytical models;Reviews;Knowledge based systems;Signal processing;Market research;Cognition;Natural language processing;Standards;empathetic conversational systems;natural language processing;dialogue system},
  doi={10.1109/ICSP62122.2024.10743383},
  ISSN={},
  month={April},}@INPROCEEDINGS{1221610,
  author={Hanjalic, A.},
  booktitle={2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)}, 
  title={Multimodal approach to measuring excitement in video}, 
  year={2003},
  volume={2},
  number={},
  pages={II-289},
  abstract={We present in this paper an approach to mimic the excitement that is evoked in a user while watching a video. The proposed approach is meant to enhance user's comfort when dealing with large amount of broadcasted digital television data reaching his home. For this reason, we deliberately avoid using any sensors placed on users: the simulation of user excitement is based here solely on cues that are available in the digital video stream, and that can be extracted by using standard audio and video processing tools as well as by observing the way video is edited. Relation between the extracted features and evoked excitement is drawn partly from psycho-physiological research and partly from analyzing the video generation practice. Our methodology is generic and can be employed broadly in the process of video abstraction and for revealing the affective characteristics of video content.},
  keywords={Digital video broadcasting;TV broadcasting;Digital TV;Streaming media;Games;Digital audio broadcasting;Data mining;Feature extraction;Psychology;Production},
  doi={10.1109/ICME.2003.1221610},
  ISSN={},
  month={July},}@INPROCEEDINGS{5349519,
  author={Scherer, Stefan and Fritzsch, Volker and Schwenker, Friedhelm},
  booktitle={2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops}, 
  title={Multimodal real-time conversation analysis using a novel process engine}, 
  year={2009},
  volume={},
  number={},
  pages={1-2},
  abstract={This contribution introduces a software framework enabling researchers to develop real-time pattern recognition and sensor fusion applications in an abstraction level above that of common programming languages in order to reduce and minimize programming errors and technical obstacles. Furthermore, a proof of concept using two separate instances of the process engine on different computers with audiovisual data processing is described. The scenario shows the capability of the engine to process data in realtime and synchronously on multiple machines, which are necessary features in large scale projects.},
  keywords={Engines;Information analysis;Prototypes;Java;Pattern analysis;Pattern recognition;Application software;Data processing;Emotion recognition;Automatic speech recognition},
  doi={10.1109/ACII.2009.5349519},
  ISSN={2156-8111},
  month={Sep.},}@ARTICLE{9477021,
  author={Cang, Xi Laura and Bucci, Paul and Rantala, Jussi and MacLean, Karon E.},
  journal={IEEE Transactions on Affective Computing}, 
  title={Discerning Affect From Touch and Gaze During Interaction With a Robot Pet}, 
  year={2023},
  volume={14},
  number={2},
  pages={1598-1612},
  abstract={Practical affect recognition needs to be efficient and unobtrusive in interactive contexts. One approach to a robust realtime system is to sense and automatically integrate multiple nonverbal sources. We investigated how users’ touch, and secondarily gaze, perform as affect-encoding modalities during physical interaction with a robot pet, in comparison to more-studied biometric channels. To elicit authentically experienced emotions, participants recounted two intense memories of opposing polarity in Stressed-Relaxed or Depressed-Excited conditions. We collected data (N=30) from a touch sensor embedded under robot fur (force magnitude and location), a robot-adjacent gaze tracker (location), and biometric sensors (skin conductance, blood volume pulse, respiration rate). Cross-validation of Random Forest classifiers achieved best-case accuracy for combined touch-with-gaze approaching that of biometric results: where training and test sets include adjacent temporal windows, subject-dependent prediction was 94% accurate. In contrast, subject-independent Leave-One-participant-Out predictions resulted in 30% accuracy (chance 25%). Performance was best where participant information was available in both training and test sets. Addressing computational robustness for dynamic, adaptive realtime interactions, we analyzed subsets of our multimodal feature set, varying sample rates and window sizes. We summarize design directions based on these parameters for this touch-based, affective, and hard, realtime robot interaction application.},
  keywords={Training;Emotion recognition;Biometrics (access control);Computational modeling;Biological system modeling;Tactile sensors;Skin;Affective touch;multimodal interaction;human-robot interaction;therapeutic robot;emotion classification},
  doi={10.1109/TAFFC.2021.3094894},
  ISSN={1949-3045},
  month={April},}@INPROCEEDINGS{9412656,
  author={Uddin, Md Taufeeq and Canavan, Shaun},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Quantified Facial Temporal-Expressiveness Dynamics for Affect Analysis}, 
  year={2021},
  volume={},
  number={},
  pages={3955-3962},
  abstract={The quantification of visual affect data (e.g. face images) is essential to build and monitor automated affect modeling systems efficiently. Considering this, this work proposes quantified facial Temporal-expressiveness Dynamics (TED) to quantify the expressiveness of human faces. The proposed algorithm leverages multimodal facial features by incorporating static and dynamic information to enable accurate measurements of facial expressiveness. We show that TED can be used for high-level tasks such as summarization of unstructured visual data, and expectation from and interpretation of automated affect recognition models. To evaluate the positive impact of using TED, a case study was conducted on spontaneous pain using the UNBC-McMaster spontaneous shoulder pain dataset. Experimental results show the efficacy of using TED for quantified affect analysis.},
  keywords={Visualization;Pain;Heuristic algorithms;Face recognition;Data models;Task analysis;Monitoring},
  doi={10.1109/ICPR48806.2021.9412656},
  ISSN={1051-4651},
  month={Jan},}@ARTICLE{9786615,
  author={Uddin, Md Azher and Joolee, Joolekha Bibi and Sohn, Kyung-Ah},
  journal={IEEE Transactions on Affective Computing}, 
  title={Deep Multi-Modal Network Based Automated Depression Severity Estimation}, 
  year={2023},
  volume={14},
  number={3},
  pages={2153-2167},
  abstract={Depression is a severe mental illness that impairs a person's capacity to function normally in personal and professional life. The assessment of depression usually requires a comprehensive examination by an expert professional. Recently, machine learning-based automatic depression assessment has received considerable attention for a reliable and efficient depression diagnosis. Various techniques for automated depression detection were developed; however, certain concerns still need to be investigated. In this work, we propose a novel deep multi-modal framework that effectively utilizes facial and verbal cues for an automated depression assessment. Specifically, we first partition the audio and video data into fixed-length segments. Then, these segments are fed into the Spatio-Temporal Networks as input, which captures both spatial and temporal features as well as assigns higher weights to the features that contribute most. In addition, Volume Local Directional Structural Pattern (VLDSP) based dynamic feature descriptor is introduced to extract the facial dynamics by encoding the structural aspects. Afterwards, we employ the Temporal Attentive Pooling (TAP) approach to summarize the segment-level features for audio and video data. Finally, the multi-modal factorized bilinear pooling (MFB) strategy is applied to fuse the multi-modal features effectively. An extensive experimental study reveals that the proposed method outperforms state-of-the-art approaches.},
  keywords={Depression;Feature extraction;Three-dimensional displays;Convolutional neural networks;Optical flow;Long short term memory;Encoding;Depression;spatio-temporal networks;volume local directional structural pattern;temporal attentive pooling;multi-modal factorized bilinear pooling},
  doi={10.1109/TAFFC.2022.3179478},
  ISSN={1949-3045},
  month={July},}@INPROCEEDINGS{9971974,
  author={Lylath, Shabeena and Rananavare, Laxmi B},
  booktitle={2022 IEEE 3rd Global Conference for Advancement in Technology (GCAT)}, 
  title={Efficient Approach for Autism Detection using deep learning techniques: A Survey}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Autism spectrum disorders (ASDs) have been identified by the problem caused in social communication along with the repetitive activities and interests. In recent times, cases of ASD increased around the world significantly. This document seeks to provide a complete overview of autistic children's diagnosis and treatment options. By, using various modalities, autistic diagnosis and therapeutic strategies are provided. Following that, a multimodal sensing technique for autism assessment is implemented. This research concludes with several suggestions regarding future research and difficulties. This paper displays the summarization as per current research in several fields such as autism diagnosis. Intervention theory and the applications with majorly focusing on ICF -based results.},
  keywords={Deep learning;Support vector machines;Autism;Multimodal sensors;Focusing;Epilepsy;Forestry;Autism Spectrum Disorder;K-nearest neighbor;Decision trees;Logistics Regression;Random Forests;Support Vector Machine},
  doi={10.1109/GCAT55367.2022.9971974},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8358500,
  author={Kawde, Piyush and Verma, Gyanendra K.},
  booktitle={2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon)}, 
  title={Multimodal affect recognition in V-A-D space using deep learning}, 
  year={2017},
  volume={},
  number={},
  pages={890-895},
  abstract={Now-a-days one of the most challenging problem is automatic affective state recognition. To identify emotions from physiological signals a knowledgeable and advanced learning algorithm is required that can represent high level abstraction of signals. This study presents the deployment of deep learning network (DLN) to determine unspecified feature interrelationship between input data. The DLN is implemented with two semi-supervised algorithms i.e.; Stack auto-encoder (SAE) and Deep Belief Network (DBN) individually on DEAP database for emotion state classification. A Bayesian inference classification based decision fusion method is used to improve the classification and the results has been improvised by almost 3% in case of valence and arousal of DBN and almost 15% and 6% in SAE than the individual classification results of SAE and DBN, however in case of dominance the result has been improved by almost 9% than SAE.},
  keywords={Physiology;Brain modeling;Feature extraction;Electroencephalography;Databases;Machine learning;Emotion recognition;Deep Belief Network;Stacked Auto-Encoder;Affect Recognition;Physiological signals;Bayesian inference classification;Valence;Arousal and Dominance},
  doi={10.1109/SmartTechCon.2017.8358500},
  ISSN={},
  month={Aug},}@ARTICLE{7945502,
  author={Noroozi, Fatemeh and Marjanovic, Marina and Njegus, Angelina and Escalera, Sergio and Anbarjafari, Gholamreza},
  journal={IEEE Transactions on Affective Computing}, 
  title={Audio-Visual Emotion Recognition in Video Clips}, 
  year={2019},
  volume={10},
  number={1},
  pages={60-75},
  abstract={This paper presents a multimodal emotion recognition system, which is based on the analysis of audio and visual cues. From the audio channel, Mel-Frequency Cepstral Coefficients, Filter Bank Energies and prosodic features are extracted. For the visual part, two strategies are considered. First, facial landmarks' geometric relations, i.e., distances and angles, are computed. Second, we summarize each emotional video into a reduced set of key-frames, which are taught to visually discriminate between the emotions. In order to do so, a convolutional neural network is applied to key-frames summarizing videos. Finally, confidence outputs of all the classifiers from all the modalities are used to define a new feature space to be learned for final emotion label prediction, in a late fusion/stacking fashion. The experiments conducted on the SAVEE, eNTERFACE'05, and RML databases show significant performance improvements by our proposed system in comparison to current alternatives, defining the current state-of-the-art in all three databases.},
  keywords={Emotion recognition;Visualization;Feature extraction;Databases;Face;Neural networks;Mel frequency cepstral coefficient;Multimodal emotion recognition;classifier fusion;data fusion;convolutional neural networks},
  doi={10.1109/TAFFC.2017.2713783},
  ISSN={1949-3045},
  month={Jan},}@ARTICLE{8999746,
  author={Escalante, Hugo Jair and Kaya, Heysem and Salah, Albert Ali and Escalera, Sergio and Güçlütürk, Yağmur and Güçlü, Umut and Baró, Xavier and Guyon, Isabelle and Junior, Julio C. S. Jacques and Madadi, Meysam and Ayache, Stephane and Viegas, Evelyne and Gürpınar, Furkan and Wicaksana, Achmadnoer Sukma and Liem, Cynthia C. S. and van Gerven, Marcel A. J. and van Lier, Rob},
  journal={IEEE Transactions on Affective Computing}, 
  title={Modeling, Recognizing, and Explaining Apparent Personality From Videos}, 
  year={2022},
  volume={13},
  number={2},
  pages={894-911},
  abstract={Explainability and interpretability are two critical aspects of decision support systems. Despite their importance, it is only recently that researchers are starting to explore these aspects. This paper provides an introduction to explainability and interpretability in the context of apparent personality recognition. To the best of our knowledge, this is the first effort in this direction. We describe a challenge we organized on explainability in first impressions analysis from video. We analyze in detail the newly introduced data set, evaluation protocol, proposed solutions and summarize the results of the challenge. We investigate the issue of bias in detail. Finally, derived from our study, we outline research opportunities that we foresee will be relevant in this area in the near future.},
  keywords={Visualization;Interviews;Computational modeling;Videos;Computer vision;Face;Predictive models;Explainable computer vision;first impressions;personality analysis;multimodal information;algorithmic accountability},
  doi={10.1109/TAFFC.2020.2973984},
  ISSN={1949-3045},
  month={April},}
