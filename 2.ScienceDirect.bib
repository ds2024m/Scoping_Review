@article{ALEXANDER201573,
title = {Glenn Gould and the Rhetorics of Sound},
journal = {Computers and Composition},
volume = {37},
pages = {73-89},
year = {2015},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2015.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S8755461515000493},
author = {Jonathan Alexander},
keywords = {Multimodality, Sound, Sound recording, Sound and Materiality, Glenn Gould, Radio documentary, Sound essay},
abstract = {Given the increased attention to working with sound in multimodal and multimedia compositions, this essay summarizes current pedagogical research and scholarly conversations and then argues for more attention to the work of Canadian pianist and recording artist Glenn Gould, who was amongst the foremost artists of his time in critically thinking about and productively expanding the possibilities of sound recording and manipulation. Gould's own voice is a key feature of many of his recordings, and his brilliant radio documentaries serve as challenging models for what contemporary compositionists might do with sound and voice in the teaching of multimodal composing. Indeed, Gould anticipated so much contemporary media production, particularly a “do-it-yourself” aesthetic, from which we can still learn. Moreover, as Gould was primarily a musician and sound artist, his insights into and practice with working with sound and voice treat both sound and voice as their own material media; they are not, for Gould, metaphors and stand-ins for textual meaning making and, as such, his work might inform a multimodal compositional pedagogy that takes seriously the particular affordances of sound and voice. Attention to such work might help us consider what can be done with sound and voice in the production of multimedia “texts” where sound and voice act beyond the textual—not just as metaphors for textual meaning making, but as materialities with their own particular rhetorical and affective affordances and dimensions.}
}
@article{VAINIO2025104754,
title = {Pitch-based correspondences related to abstract concepts},
journal = {Acta Psychologica},
volume = {253},
pages = {104754},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.104754},
url = {https://www.sciencedirect.com/science/article/pii/S0001691825000678},
author = {L. Vainio and A. Wikström and M. Vainio},
keywords = {Pitch-based correspondence, Abstract concepts, Language, Speech, Cross-modal processes},
abstract = {Previous investigations have shown pitch-based correspondences with various perceptual and conceptual attributes. The present study reveals two novel pitch-based correspondences with highly abstract concepts. Three experiments with varying levels of implicitness of the association task showed that the concepts of future and in are associated with high-pitch sounds, while past and out are associated with low-pitch sounds. Hence, pitch-based correspondences can be observed even with temporal concepts that cannot be unambiguously represented in any perceptual format, at least, without spatial metaphorization. The correspondence effects were even more robust with the abstract temporal concepts of future/past than with more concrete spatial concepts of in/out. We propose that these effects might emerge from semantic multimodal abstraction processes mediated by affective dimensions of particular concepts.}
}
@article{BONIARDI2021118613,
title = {Commuting by car, public transport, and bike: Exposure assessment and estimation of the inhaled dose of multiple airborne pollutants},
journal = {Atmospheric Environment},
volume = {262},
pages = {118613},
year = {2021},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2021.118613},
url = {https://www.sciencedirect.com/science/article/pii/S1352231021004350},
author = {Luca Boniardi and Francesca Borghi and Serena Straccini and Giacomo Fanti and Davide Campagnolo and Laura Campo and Luca Olgiati and Simone Lioi and Andrea Cattaneo and Andrea Spinazzè and Domenico Maria Cavallo and Silvia Fustinoni},
keywords = {Travel mode, Personal exposure, Traffic related air pollution (TRAP), Time-activity pattern, Microenvironment, Inhalation dose},
abstract = {It is known that air pollution affects human health and commuting environments are significant contributors to the total daily personal exposure to air pollution. With this study we aim at assessing and comparing personal exposure to traffic-related air pollution and related estimated inhaled dose of different typical commuter profiles in real-world scenarios focusing on multimodal commuting and different transport microenvironments. A car driver and two multimodal commuters (cyclist/public transport and pedestrian/public transport commuters), which reached the same destination in the city center, leaving from the same starting point in the metropolitan area of the city, were investigated in the metropolitan area of Milan, Italy. Real-time monitors for the measurement of size-fractionated particulate matter (PM - Aerocet 831-Met One Instrument Inc., Grant Pass, Oregon, USA), equivalent Black Carbon (eBC - Aethlabs, San Francisco, CA, USA), and NO2 (CairClip NO2, Cairpol; La Roche Blanche, France) and time-integrated samplers for the measurement of NO2 and benzene were used. Inhaled dose was estimated as well by applying estimated pulmonary ventilation rates to 1-min exposure measures. The Bootstrapping method was used to resample data and perform more robust comparisons among commuters and microenvironments (MEs). Results of the study could be summarized as follows: (i) travel times of the cyclist were the shortest; (ii) the highest concentrations and estimated inhaled doses were found during morning rush hour; (iii) the cyclist and the car commuter were exposed to the highest overall median values of fractionated PM and benzene, respectively; (iv) cycling MEs presented the highest median concentrations of PM, while the overall median concentration of eBC was higher commuting by car than walking and cycling; (v) the cyclist was the commuter with the highest overall median estimated inhaled dose for fractionated PM, while benzene was the highest for the car driver, and NO2 for the pedestrian; (vi) exposure differences both among microenvironments and commuters diverged comparing morning and evening commuting. Our results suggest that a multi-pollutant approach is necessary to better represent the complexity of personal exposure to traffic-related air pollution in multimodal mobility studies. Moreover, shifting from car driving to multimodal mobility is already a valuable choice for the metropolitan area of Milan considering both exposure and commuting time. However, to further reduce personal exposure and especially estimated inhaled dose of multimodal and active commuters, exposure mitigation interventions in selected MEs, traffic calming, and innovative mobility policies are still needed.}
}
@article{SHAH2016102,
title = {Leveraging multimodal information for event summarization and concept-level sentiment analysis},
journal = {Knowledge-Based Systems},
volume = {108},
pages = {102-109},
year = {2016},
note = {New Avenues in Knowledge Bases for Natural Language Processing},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116301101},
author = {Rajiv Ratn Shah and Yi Yu and Akshay Verma and Suhua Tang and Anwar Dilawar Shaikh and Roger Zimmermann},
keywords = {Multimedia summarization, Semantics analysis, Sentics analysis, Multimodal analysis, Multimedia-related services},
abstract = {The rapid growth in the amount of user-generated content (UGCs) online necessitates for social media companies to automatically extract knowledge structures (concepts) from photos and videos to provide diverse multimedia-related services. However, real-world photos and videos are complex and noisy, and extracting semantics and sentics from the multimedia content alone is a very difficult task because suitable concepts may be exhibited in different representations. Hence, it is desirable to analyze UGCs from multiple modalities for a better understanding. To this end, we first present the EventBuilder system that deals with semantics understanding and automatically generates a multimedia summary for a given event in real-time by leveraging different social media such as Wikipedia and Flickr. Subsequently, we present the EventSensor system that aims to address sentics understanding and produces a multimedia summary for a given mood. It extracts concepts and mood tags from visual content and textual metadata of UGCs, and exploits them in supporting several significant multimedia-related services such as a musical multimedia summary. Moreover, EventSensor supports sentics-based event summarization by leveraging EventBuilder as its semantics engine component. Experimental results confirm that both EventBuilder and EventSensor outperform their baselines and efficiently summarize knowledge structures on the YFCC100M dataset.}
}
@article{ZIJUN2024105158,
title = {Multi-task disagreement-reducing multimodal sentiment fusion network},
journal = {Image and Vision Computing},
volume = {149},
pages = {105158},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105158},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624002634},
author = {Wang Zijun and Jiang Naicheng and Chao Xinyue and Sun Bin},
keywords = {Multimodal sentiment analysis, Multimodal fusion, Sentiment disagreement, Multi-task learning},
abstract = {Existing multimodal sentiment analysis models can effectively capture sentimental commonalities between different modalities and possess high sentimental acquisition capability. However, there are still shortcomings in the model's analysis and recognition abilities when dealing with samples that exhibit sentimental polarity disagreement between different modalities. Additionally, the dominance of the text modality in multimodal models, particularly those pre-trained with BERT, can hinder the learning of other modalities due to its richer semantic information. This issue becomes particularly pronounced in cases where there is a conflict between multimodal and textual sentimental polarities, often leading to suboptimal analytical results. Besides, the classification ability of each modality is also suppressed by single-task learning. In this paper, We propose a Multi-Task disagreement-Reducing Multimodal Sentiment Fusion Network (MtDr-MSF), designed to enhance the semantic information of non-text modalities and reduce the dominant impact of the textual modality on the model, and to improve the learning capabilities of unimodal networks. We conducted experiments on multimodal sentiment analysis datasets, CMU-MOSI, CMU-MOSEI, and CH-SIMS. The results show that our method outperforms the current SOTA method.}
}
@article{LEE2024105048,
title = {Becoming epistemically active in online reading: Facilitating elementary school students’ multimodal multiple document reading via sourcing organizers},
journal = {Computers & Education},
volume = {216},
pages = {105048},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105048},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524000629},
author = {Yuan-Hsuan Lee and Jing-Ya Jhang and Huang-Yao Hong},
keywords = {Online multimodal and multiple document reading, Text integration, Source-content link, Reading interest, Reading ability},
abstract = {In the AI era, it has become crucial to evaluate information found on the Internet critically. This research aimed to investigate the impact of a sourcing organizer on sixth graders' online multimodal and multiple document reading (MMDR) abilities, focusing on aspects such as source-content link and text integration in relation to reading on the Internet. Cognitive and affective factors associated with MMDR were examined. The study involved 52 sixth-graders (55.77% males) from two typical elementary school classes in the northern region of Taiwan. Two intact classes were randomly assigned to either the experimental or control group with the quasi-experimental design. The experimental group (n = 26) received a pre-outlined sourcing organizer, guiding them to record the article title, author, publication date, website name, and major assertions from six assigned multimodal texts. In contrast, the control group (n = 26) received a regular organizer, prompting them to summarize the main ideas from the same six assigned multimodal texts. The study's findings indicated that employing sourcing organizers positively impacted students' performance in text integration. However, it was observed that both groups, regardless of whether they used regular organizers or sourcing organizers, experienced benefits in terms of source-content links. Furthermore, reading ability emerged as the sole significant predictor for source-content links, whereas both reading ability and the use of sourcing organizers predicted text integration. The implications of these findings were discussed to provide insights into instructional strategies to develop online MMDR competencies in elementary students.}
}
@article{KIM2024107792,
title = {Integrating visual and community environments in a motorcycle crash and casualty estimation},
journal = {Accident Analysis & Prevention},
volume = {208},
pages = {107792},
year = {2024},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2024.107792},
url = {https://www.sciencedirect.com/science/article/pii/S0001457524003373},
author = {Yujin Kim and Hwasoo Yeo and Lisa Lim and Byeongjoon Noh},
keywords = {Motorcycle crash, Urban environment planning, Data cube modeling, Multimodal deep learning, Explainable AI},
abstract = {Motorcycle crashes pose a serious problem because their probability of causing casualties is greater than that of passenger vehicle crashes. Therefore, accurately identifying the factors that influence motorcycle crashes is essential for enhancing traffic safety and public health. The aim of this study was to address three major research gaps: first, existing studies have relatively overlooked the built environment in relation to visual factors; second, existing crash prediction models have not fully reflected the differences in built environment characteristics between areas with frequent motorcycle crashes and areas with frequent casualties; and third, multidimensional analysis for variable selection is limited, and the interpretability of the models is insufficient. Therefore, this study proposes a comprehensive framework for motorcycle crash and casualty estimation. The framework uses a data cube model incorporating OLAP operations to provide deeper insights into crash influencing factors at different levels of abstraction. We also utilized the XGBoost model to predict motorcycle high crash spots and casualty risk and integrate visual factors extracted from Google Street View images and community-level urban environments into the model. SHAP techniques were used to analyze and interpret the global and local feature importance of the models. Our results revealed that the factors affecting areas with frequent motorcycle crashes and the factors that affect casualties differ. In particular, visual factors such as vegetation and the sky ratio are important for estimating casualties. We aim to provide practical guidelines for a safe environment for motorcycle crashes.}
}
@article{ULLAH202438,
title = {Demographic Patterns and Clinicopathological Analysis of Sarcomatoid Renal Cell Carcinoma in US Population},
journal = {Clinical Genitourinary Cancer},
volume = {22},
number = {1},
pages = {38-46},
year = {2024},
issn = {1558-7673},
doi = {https://doi.org/10.1016/j.clgc.2023.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1558767323001799},
author = {Asad Ullah and Abdul Qahar Khan Yasinzai and Om V. Sakhalkar and Kue Tylor Lee and Imran Khan and Bisma Tareen and Agha Wali and Abdul Waheed and Jaffar Khan and Gul Andam and Kaleemullah Kakar and Saleh Heneidi and Nabin R. Karki},
keywords = {SEER, Survival outcomes, Metastasis, Multivariate, Multimodal therapy},
abstract = {Background
Sarcomatoid renal cell carcinoma (RCC) is defined by the presence of any amount of sarcomatoid components admixed with other RCC histologic subtypes. Our investigation utilizes a large, diverse set of sarcomatoid RCC patients to summarize clinical, demographic, and pathological factors along with demographic disparities that may affect the prognosis and survival of sarcomatoid RCC patients.
Methods
The Surveillance, Epidemiology, and End Results (SEER) database was employed to compile data from 2000 to 2018 from 2695 patients diagnosed with sarcomatoid RCC.
Results
The mean age for sarcomatoid RCC diagnosis is 62.8 years. Males (68.2%) and White patients (82.6%) were more likely to be diagnosed with sarcomatoid RCC. Among the 64.4% of tumors with known size, 35.4% were less than 7 cm, 27.6% were 7.1 to 10 cm, and 36.4% were larger than 10 cm. Among the 95.8% of patients with known stage, 15.3% were localized, 28.9% were regionalized, and 55.8% were found in distant sites. Among the 44.2% of cases with known metastases site, lung was found to be the most common metastatic site.. Surgery was the most common treatment (70.9%). While the overall 5-year survival was 18.1%, it was 27.1% among patients who underwent surgery. Independent risk factors for mortality include age > 60 years, distant stage, and tumor size > 10 cm, per our multivariate analysis.
Conclusion
Sarcomatoid RCC most commonly affects White males in their seventh decade. Increased age, distant stage, and size > 10 cm tumor size have associations with unfavorable prognosis. Surgery is associated with better survival outcomes in localized disease and multimodal therapy (surgery with adjuvant chemoradiation was associated with better survival.)}
}
@article{ROZANSKA2019411,
title = {Multimodal sentiment analysis applied to interaction between patients and a humanoid robot Pepper},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {27},
pages = {411-414},
year = {2019},
note = {16th IFAC Conference on Programmable Devices and Embedded Systems PDES 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.696},
url = {https://www.sciencedirect.com/science/article/pii/S240589631932645X},
author = {Agnieszka Rozanska and Michal Podpora},
keywords = {Humanoid Robots, Human-Machine Interaction, Multimodal Sentiment Analysis, Pepper Robot, Nonverbal Communication},
abstract = {In this paper authors briefly summarize the concept, motivation and exemplary implementation of an IoT-based external emotion sensor, designed for the purpose of Multimodal Sentiment Analysis. The device includes Raspberry Pi, a camera and a microphone, and serves as a standalone source of information about interlocutor’s emotions, facial expressions, body language, etc. by analyzing audio, text and video.}
}
@article{CHENG2022107298,
title = {Using multimodal remote sensing data to estimate regional-scale soil moisture content: A case study of Beijing, China},
journal = {Agricultural Water Management},
volume = {260},
pages = {107298},
year = {2022},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2021.107298},
url = {https://www.sciencedirect.com/science/article/pii/S0378377421005758},
author = {Minghan Cheng and Binbin Li and Xiyun Jiao and Xiao Huang and Haiyan Fan and Rencai Lin and Kaihua Liu},
keywords = {Soil moisture content, Multimodality, Multispectral, Thermal infrared, Random Forest},
abstract = {An accurate regional estimate of soil moisture content (SMC) is important for water management and drought monitoring. Traditional ground measurement methods of SMC are limited by the disadvantages of high cost and small scale. The development of remote sensing (RS) technology provides a cost-effective tool for estimating SMC at regional scale. However, the estimation of SMC by using the combination of multiple sensors has yet to be thoroughly discussed. Furthermore, the way in which vegetation types, fraction of vegetation coverage (FVC) and soil layer depth affect the SMC-estimation performance remains unclear. Therefore, the objectives in this study are to (1) evaluate the SMC-estimation performance provided by Landsat-8 data and random forest regression (RFR) algorithm; (2) discuss the accuracy of RS-based method for SMC estimation at different soil layer depths, and (3) explore how vegetation types and FVC affect the performance of SMC-estimation. The results can be summarized as: (1) the SMC estimation performance of multispectral (MS)- and thermal infrared (TIR)-based indices single used were comparable, in which TIR-based indices performed better in shallow soil layer while MS-based indices performed better in deep soil layer. Generally, MS- and TIR-based indices jointly used outperformed the index single used. (2) the accuracy of the proposed method for estimating SMC decreased with soil depth. (3) the proposed method performed greatest in grassland with relatively low height among the three vegetation types. Moreover, the SMC estimation in moderate vegetation coverage (FVC ranged from 0.3 to 0.5) was best. These results indicate that RS-based multimodal data combined with RFR could provide relatively repeatable and accurate SMC estimation. This approach can thus be used for the regional SMC monitoring and water resources management.}
}
@article{KONSTANDOPOULOS2000262,
title = {Deposit growth dynamics: particle sticking and scattering phenomena},
journal = {Powder Technology},
volume = {109},
number = {1},
pages = {262-277},
year = {2000},
issn = {0032-5910},
doi = {https://doi.org/10.1016/S0032-5910(99)00242-9},
url = {https://www.sciencedirect.com/science/article/pii/S0032591099002429},
author = {Athanasios G. Konstandopoulos},
keywords = {Deposit growth dynamics, Particle sticking, Scattering phenomena},
abstract = {A common phenomenon in deposition processes is the impaction of a particle on a pre-deposited particle in a deposit (e.g., as in thin film growth via aerosol routes or in gas-side fouling of heat exchange equipment). The fate of an incident particle (i.e., whether it sticks on the deposit or it escapes) affects the net deposition rate, as well as the resulting deposit microstructure, phenomena that we have been studying, using discrete particle computer simulations. Here, we summarize the sticking behavior of impacting particles in terms of appropriate macroscopic “boundary conditions” that can be used in continuum level simulations of the dynamics of deposit growth. In addition, we study the properties of rebounding/scattered particles from a “rough” particulate deposit surface, in terms of rebounding linear and angular velocities and scattering angle distributions. Interestingly enough, the rebounding velocity distributions exhibit a multimodal character which becomes less pronounced with the degree of “rigidity” of the deposit, reflecting the influence of local microstructural details (coordination number distribution) on the scattering process. Scattering angle distributions are unimodal, resembling a “distorted” sinusoidal. The remaining challenge is to develop, with further parametric studies, appropriate “boundary conditions” for the scattering quantities of interest as well, and open the door to continuum level simulations of macroscopic systems in realistic geometries.}
}
@article{SOLEYMANI20173,
title = {A survey of multimodal sentiment analysis},
journal = {Image and Vision Computing},
volume = {65},
pages = {3-14},
year = {2017},
note = {Multimodal Sentiment Analysis and Mining in the Wild Image and Vision Computing},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0262885617301191},
author = {Mohammad Soleymani and David Garcia and Brendan Jou and Björn Schuller and Shih-Fu Chang and Maja Pantic},
keywords = {Sentiment, Affect, Sentiment analysis, Human behavior analysis, Computer vision, Affective computing},
abstract = {Sentiment analysis aims to automatically uncover the underlying attitude that we hold towards an entity. The aggregation of these sentiment over a population represents opinion polling and has numerous applications. Current text-based sentiment analysis rely on the construction of dictionaries and machine learning models that learn sentiment from large text corpora. Sentiment analysis from text is currently widely used for customer satisfaction assessment and brand perception analysis, among others. With the proliferation of social media, multimodal sentiment analysis is set to bring new opportunities with the arrival of complementary data streams for improving and going beyond text-based sentiment analysis. Since sentiment can be detected through affective traces it leaves, such as facial and vocal displays, multimodal sentiment analysis offers promising avenues for analyzing facial and vocal expressions in addition to the transcript or textual content. These approaches leverage emotion recognition and context inference to determine the underlying polarity and scope of an individual's sentiment. In this survey, we define sentiment and the problem of multimodal sentiment analysis and review recent developments in multimodal sentiment analysis in different domains, including spoken reviews, images, video blogs, human–machine and human–human interactions. Challenges and opportunities of this emerging field are also discussed leading to our thesis that multimodal sentiment analysis holds a significant untapped potential.}
}
@article{FOUCHER2024114417,
title = {Identifying dynamic restructuring effects in nanocatalysts by combining in situ scanning transmission electron microscopy and in situ X-ray absorption spectroscopy: A review},
journal = {Catalysis Today},
volume = {428},
pages = {114417},
year = {2024},
issn = {0920-5861},
doi = {https://doi.org/10.1016/j.cattod.2023.114417},
url = {https://www.sciencedirect.com/science/article/pii/S0920586123004418},
author = {Alexandre C. Foucher and Eric A. Stach},
keywords = {In situ, Scanning transmission electron microscopy, X-ray absorption spectroscopy, Multimodal characterization},
abstract = {Monometallic and bimetallic nanostructures are of great importance to heterogeneous catalysis as the interaction with a support or the combination of metals results in a precise tuning of chemical properties. However, these nanostructures are susceptible to undergoing dramatic changes in morphology and composition in a reactive environment, which will ultimately affect the properties of the catalyst. Unfortunately, these changes are often not well understood, especially at the atomic level. This work reviews combined in situ scanning transmission electron microscopy and in situ X-ray absorption spectroscopy approaches to gain an atomistic understanding of metallic structures in gaseous environments. Detailed understanding of the structural changes in nanostructures with these two characterization techniques is then used to guide the rational designs of active and stable catalysts.}
}
@article{XIE2024111670,
title = {Video summarization via knowledge-aware multimodal deep networks},
journal = {Knowledge-Based Systems},
volume = {293},
pages = {111670},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111670},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124003058},
author = {Jiehang Xie and Xuanbai Chen and Sicheng Zhao and Shao-Ping Lu},
keywords = {Video summarization, Multimodal information, External knowledge},
abstract = {Video summarization has unprecedented importance in facilitating the rapid browsing, retrieval, and comprehension of large numbers of videos. Benefiting from possessing rich prior knowledge of the raw video and the capability to filter less crucial frames by employing multimodal information, humans can condense a lengthy video into a compact and reasonable video summary. However, existing automated video summarization approaches struggle to determine which shots in a video are significant concurrently and robustly, which is detrimental to the generation of high-quality summaries. To improve the quality of video summaries further, drawing inspiration from human abilities, we propose a novel video summarization approach based on a knowledge-aware multimodal network (KAMN). In particular, we present a knowledge-based encoder to obtain the corresponding representation for each frame. This representation is composed of captured descriptive content and affections, which are retrieved from large-scale external knowledge bases. Owing to these knowledge bases, rich implicit knowledge is provided to better understand the viewed video. Moreover, to integrate the visual, audio, and implicit knowledge features more effectively and to identify valuable information across different modalities further, we design a fusion module to learn these multimodal feature relationships more thoroughly. KAMN operates in both unsupervised and supervised training modes. Objective quantitative experiments and subjective user studies were conducted using four publicly available datasets. The results verified the effectiveness of the proposed modules and demonstrated the superior performance yielded by our framework.}
}
@article{YIN201793,
title = {Recognition of emotions using multimodal physiological signals and an ensemble deep learning model},
journal = {Computer Methods and Programs in Biomedicine},
volume = {140},
pages = {93-110},
year = {2017},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169260716305090},
author = {Zhong Yin and Mengyuan Zhao and Yongxiong Wang and Jingdong Yang and Jianhua Zhang},
keywords = {Emotion recognition, Affective computing, Physiological signals, Deep learning, Ensemble learning},
abstract = {Background and Objective
Using deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions.
Methods
In this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological-data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states.
Results
DEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%.
Conclusions
The superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances.}
}
@incollection{RODRIGUE201173,
title = {Chapter 5 - The Cognitive Consequences of Structural Changes to the Aging Brain},
editor = {K. Warner Schaie and Sherry L. Willis},
booktitle = {Handbook of the Psychology of Aging (Seventh Edition)},
publisher = {Academic Press},
edition = {Seventh Edition},
address = {San Diego},
pages = {73-91},
year = {2011},
series = {Handbooks of Aging},
isbn = {978-0-12-380882-0},
doi = {https://doi.org/10.1016/B978-0-12-380882-0.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012380882000005X},
author = {Karen M. Rodrigue and Kristen M. Kennedy},
abstract = {Publisher Summary
This chapter reviews the history of the morphometric research on normal brain aging and examines in detail, the recently developed techniques for measuring the brain's structure and exploring structure- function association studies conducted within each neuroimaging modality in the context of aging. For comparison purposes, it reviews studies with participants showing “normal” aging, with a methodology in each technique following the most standard protocols. Because the state of the brain's structure affects the quality of its function, the study of changes in brain structure has proven fruitful in understanding cognitive aging. Brain's function ostensibly relies on the brain's structural integrity. Therefore, many studies have embarked on relating age differences in regional volume to their putative underlying function. The resulting literature is mixed in its findings because the brain's volume fluctuates over decades (incorporating growth and shrinkage), and because middle and older aged adults are probably able to compensate for degradation to the brain's structure over time or rely upon cognitive reserve resources to compensate for declines in structural integrity. Further, structure-cognition associations are not easily replicated and are sensitive to type of cognitive assessment and to sample composition. In this context, the chapter summarizes the volume cognition studies by cognitive domain. It concludes with a look toward needed future research directions with a particular focus on combined multimodality structure–function studies and a more careful selection and characterization of the health indices of the participants selected for cognitive aging studies.}
}
@article{JIANG2020209,
title = {A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition},
journal = {Information Fusion},
volume = {53},
pages = {209-221},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519301381},
author = {Yingying Jiang and Wei Li and M. Shamim Hossain and Min Chen and Abdulhameed Alelaiwi and Muneer Al-Hammadi},
keywords = {Artificial intelligence, Multimodal information fusion, Data-driven emotion recognition},
abstract = {With the rapid development of artificial intelligence and mobile Internet, the new requirements for human-computer interaction have been put forward. The personalized emotional interaction service is a new trend in the human-computer interaction field. As a basis of emotional interaction, emotion recognition has also introduced many new advances with the development of artificial intelligence. The current research on emotion recognition mostly focuses on single-modal recognition such as expression recognition, speech recognition, limb recognition, and physiological signal recognition. However, the lack of the single-modal emotional information and vulnerability to various external factors lead to lower accuracy of emotion recognition. Therefore, multimodal information fusion for data-driven emotion recognition has been attracting the attention of researchers in the affective computing filed. This paper reviews the development background and hot spots of the data-driven multimodal emotion information fusion. Considering the real-time mental health monitoring system, the current development of multimodal emotion data sets, the multimodal features extraction, including the EEG, speech, expression, text features, and multimodal fusion strategies and recognition methods are discussed and summarized in detail. The main objective of this work is to present a clear explanation of the scientific problems and future research directions in the multimodal information fusion for data-driven emotion recognition field.}
}
@article{HAUEIS2021117846,
title = {Multiscale modeling of cortical gradients: The role of mesoscale circuits for linking macro- and microscale gradients of cortical organization and hierarchical information processing},
journal = {NeuroImage},
volume = {232},
pages = {117846},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.117846},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921001233},
author = {Philipp Haueis},
abstract = {The gradient concept in neuroscience describes systematic and continuous progressions of features of cortical organization across the entire cortex. Recent multimodal studies revealed a macroscale gradient from primary sensory to transmodal association areas which is linked to increasing representational abstraction along the cortical hierarchy, and which is paralleled by microscale gradients of cytoarchitecture and gene expression profiles. Convergent or divergent evidence from these multimodal studies is then used to support inferences about the existence of one common or multiple scale-specific gradients of hierarchical information processing. This paper evaluates the validity of such inferences within the framework of multiscale modeling. In branches of physics and biology where multiscale modeling techniques are used, the simple averaging of microscale details can introduce errors in macroscale modeling if it ignores structures at the intermediate mesoscales of organization which affect system behavior. Conversely, information about mesoscale structures can be used to determine which microscale details are actually relevant to macroscale behavior. In this paper, I similarly argue that multiscale modeling of cortical gradients needs to take organization of mesoscale circuits into account if it affects the structure-function relation that the models describe. Information about these circuits provides crucial evidence for evaluating inferences from micro- and macroscale data to the role of cortical gradients in hierarchical information processing. My application of the multiscale modeling framework reveals that the gradient concept tracks multiple overlapping progressions of cortical properties, rather than one overall gradient of hierarchical information processing. I support this argument by proposing a mesoscale gradient of connectivity which describes architectural differences between granular and agranular circuits, and which helps us better understand the relation between neural connectivity and hierarchical information processing.}
}
@article{MA2025110004,
title = {Multimodal emotion recognition by fusing complementary patterns from central to peripheral neurophysiological signals across feature domains},
journal = {Engineering Applications of Artificial Intelligence},
volume = {143},
pages = {110004},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110004},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625000041},
author = {Zhuang Ma and Ao Li and Jiehao Tang and Jianhua Zhang and Zhong Yin},
keywords = {Multimodal integration, Biomedical signal processing, Affective human-machine interaction, Application of artificial intelligence, Implemented artificial intelligence},
abstract = {The implementation and application of artificial intelligence are propelling various advanced affective computing frameworks. Automatic recognition of emotions using multimodal physiological signals enhances the efficiency of systems such as health-care applications, pilot cognitive state monitoring, and passive brain-computer interfaces. However, challenges remain in capturing topological-frequency patterns in diverse electroencephalogram (EEG) electrode layouts, uncovering coupling dynamics across adjacent peripheral modalities, and integrating complementary affective patterns from the feature to modality levels. To address these issues, we propose a central-to-peripheral complementary integration network that employs hybrid encoders to extract and integrate affective patterns from EEG and peripheral signals. For EEG, the model unifies features from different channels into a single map to extract local-to-global representations, while for peripheral signals, adjacent cross-modal information is embedded into global affective patterns. These abstractions are systematically aggregated for emotion recognition by aligning affective relevance across domains and modalities within the central and peripheral nervous systems. The proposed model was evaluated on four publicly available multimodal databases using a leave-one-subject-out cross-validation approach. On the Database for Emotion Analysis using Physiological signal (DEAP), the binary recognition accuracy for valence and arousal scales was 75.00% and 77.33%, respectively. On the Human-Computer Interaction (HCI) database, the corresponding binary accuracies were 78.78% and 75.38%. For the SJTU Emotion EEG Datasets IV and V (SEED-IV and SEED-V), the four-class and five-class accuracies were 71.94% and 84.83%, respectively. These results validate the robustness and remarkable generalization capability of the proposed method.}
}
@article{ALVAREZNEBREDA201833,
title = {Recommendations for Preoperative Management of Frailty from the Society for Perioperative Assessment and Quality Improvement (SPAQI)},
journal = {Journal of Clinical Anesthesia},
volume = {47},
pages = {33-42},
year = {2018},
issn = {0952-8180},
doi = {https://doi.org/10.1016/j.jclinane.2018.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0952818018301995},
author = {Maria Loreto Alvarez-Nebreda and Nathalie Bentov and Richard D. Urman and Sabeena Setia and Joe Chin-Sun Huang and Kurt Pfeifer and Katherine Bennett and Thuan D. Ong and Deborah Richman and Divya Gollapudi and G. {Alec Rooke} and Houman Javedan},
keywords = {Frailty, Assessment, Screening, Preoperative, Outcomes, Tool},
abstract = {Frailty is an age-related, multi-dimensional state of decreased physiologic reserve that results in diminished resiliency and increased vulnerability to stressors. It has proven to be an excellent predictor of unfavorable health outcomes in the older surgical population. There is agreement in recommending that a frailty evaluation should be part of the preoperative assessment in the elderly. However, the consensus is still building with regards to how it should affect perioperative care. The Society for Perioperative Assessment and Quality Improvement (SPAQI) convened experts in the fields of gerontology, anesthesiology and preoperative assessment to outline practical steps for clinicians to assess and address frailty in elderly patients who require elective intermediate or high risk surgery. These recommendations summarize evidence-based principles of measuring and screening for frailty, as well as basic interventions that can help improve patient outcomes.}
}
@article{KAYA201766,
title = {Video-based emotion recognition in the wild using deep transfer learning and score fusion},
journal = {Image and Vision Computing},
volume = {65},
pages = {66-75},
year = {2017},
note = {Multimodal Sentiment Analysis and Mining in the Wild Image and Vision Computing},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2017.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0262885617300367},
author = {Heysem Kaya and Furkan Gürpınar and Albert Ali Salah},
keywords = {EmotiW, Emotion recognition in the wild, Multimodal fusion, Convolutional neural networks, Kernel extreme learning machine, Partial least squares},
abstract = {Multimodal recognition of affective states is a difficult problem, unless the recording conditions are carefully controlled. For recognition “in the wild”, large variances in face pose and illumination, cluttered backgrounds, occlusions, audio and video noise, as well as issues with subtle cues of expression are some of the issues to target. In this paper, we describe a multimodal approach for video-based emotion recognition in the wild. We propose using summarizing functionals of complementary visual descriptors for video modeling. These features include deep convolutional neural network (CNN) based features obtained via transfer learning, for which we illustrate the importance of flexible registration and fine-tuning. Our approach combines audio and visual features with least squares regression based classifiers and weighted score level fusion. We report state-of-the-art results on the EmotiW Challenge for “in the wild” facial expression recognition. Our approach scales to other problems, and ranked top in the ChaLearn-LAP First Impressions Challenge 2016 from video clips collected in the wild.}
}
@incollection{SMITH201989,
title = {Chapter 6 - The role of anterior and midcingulate cortex in emotional awareness: A domain-general processing perspective},
editor = {Brent A. Vogt},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {166},
pages = {89-101},
year = {2019},
booktitle = {Cingulate Cortex},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-444-64196-0.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444641960000066},
author = {Ryan Smith and Geoffrey L. Ahern and Richard D. Lane},
keywords = {Emotion, Emotional awareness, Emotion-cognition interactions, Anterior cingulate cortex, Midcingulate cortex, Predictive processing, Visceromotor control, Motivation, Reinforcement learning, Action selection},
abstract = {The cingulate cortex has been implicated in a wide range of overlapping cognitive, affective, skeletomotor, and visceromotor functions. In this chapter, we focus on the role of the anterior and midcingulate cortex (ACC and MCC) in facilitating a person's ability to recognize and understand his or her own emotions. Here, we illustrate how this ability—often referred to as “emotional awareness” (EA)—may require integration across each of the aforementioned functions. To appropriately situate the role of the cingulate in EA, we first summarize a number of studies that have highlighted ACC/MCC engagement in the context of emotion. We then describe prominent domain-general views of the ACC (in interaction with MCC), which together suggest that it may serve as a hub within a high-level visceromotor control system. This high-level system functions to predict and mobilize the required metabolic resources in a given situation via the integration of multimodal information available from both sensory cortices and memory. Based on this work, we show that EA can be seen as an important consequence of this integrative process and how it can help to explain the adaptive nature of such advanced emotional capacities. We close by briefly considering the potential clinical relevance of understanding ACC/MCC function and its specific role in emotion and awareness.}
}
@article{BENTZEN2004320,
title = {High-tech in radiation oncology: should there be a ceiling?},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {58},
number = {2},
pages = {320-330},
year = {2004},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2003.09.057},
url = {https://www.sciencedirect.com/science/article/pii/S0360301603019965},
author = {Søren M Bentzen},
keywords = {High-precision RT, Target volume, Biological assays, Bio-imaging, Evidence-based RT},
abstract = {Purpose
To analyze some of the limitations to improvement of the outcome of radiotherapy (RT) expected from the introduction of sophisticated treatment planning and delivery technology.
Methods and materials
Several recent examples from the literature were analyzed in some detail. Mathematical modeling techniques were used to assess the likely clinical impact of new technologies or biologic principles. The findings of recent randomized controlled trials of RT for prostate, breast, and rectal cancer were analyzed from the perspective of cost-effectiveness and therapeutic gain.
Results
The main findings of the analyses may be summarized as follows. Dosimetric precision should aim for a <2% patient-to-patient variability in the delivered dose. Imprecision in clinical target volume definition remains an obstacle for high-precision RT. Functional imaging and novel biologic assays may facilitate a move from a clinical target volume to the real target volume. Improved target volume coverage is mainly important if RT has high effectiveness. Radiation oncology is increasingly becoming evidence based. However, there is still a long way to go. Hypofractionation in adjuvant RT for breast cancer may represent a favorable balance between cost and benefit. Treatment complications are potentially associated with both suffering and high cost. The identification of high-risk patients would improve the cost-effectiveness of high-tech RT aimed at avoiding complications. Conformal RT may allow the introduction of hypofractionation, which, again, could potentially save resources. With improvement in surgery and more screening-detected cancer cases, the number needed to treat increases, and this will directly affect the cost-effectiveness of high-tech RT unless efficient patient selection can be developed.
Conclusion
Sustained technological refinement is only likely to be cost-effective if the clinical and biologic understanding of patient-to-patient variability in the risk of specific types of failure and the optimal multimodality approach to handle these risks is developed at the same time. Mathematical modeling together with methods from health technology assessment and health economics are useful complements to standard methods from evidence-based medicine. Progress in functional imaging and in basic and clinical cancer biology is likely to provide the tools required for individualized risk-adapted RT.}
}
@article{PULVERMULLER201386,
title = {Semantic embodiment, disembodiment or misembodiment? In search of meaning in modules and neuron circuits},
journal = {Brain and Language},
volume = {127},
number = {1},
pages = {86-103},
year = {2013},
issn = {0093-934X},
doi = {https://doi.org/10.1016/j.bandl.2013.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0093934X13001132},
author = {Friedemann Pulvermüller},
keywords = {Action perception circuit, Cell assembly, Concept, Mirror neuron, Memory cell, Meaning, Semantic category, Semantics},
abstract = {“Embodied” proposals claim that the meaning of at least some words, concepts and constructions is grounded in knowledge about actions and objects. An alternative “disembodied” position locates semantics in a symbolic system functionally detached from sensorimotor modules. This latter view is not tenable theoretically and has been empirically falsified by neuroscience research. A minimally-embodied approach now claims that action–perception systems may “color”, but not represent, meaning; however, such minimal embodiment (misembodiment?) still fails to explain why action and perception systems exert causal effects on the processing of symbols from specific semantic classes. Action perception theory (APT) offers neurobiological mechanisms for “embodied” referential, affective and action semantics along with “disembodied” mechanisms of semantic abstraction, generalization and symbol combination, which draw upon multimodal brain systems. In this sense, APT suggests integrative-neuromechanistic explanations of why both sensorimotor and multimodal areas of the human brain differentially contribute to specific facets of meaning and concepts.}
}
@article{ZHU2024110584,
title = {Emotion-aware hierarchical interaction network for multimodal image aesthetics assessment},
journal = {Pattern Recognition},
volume = {154},
pages = {110584},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110584},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324003352},
author = {Tong Zhu and Leida Li and Pengfei Chen and Jinjian Wu and Yuzhe Yang and Yaqian Li},
keywords = {Image aesthetics assessment, Multimodal learning, Image emotion analysis},
abstract = {Image aesthetics assessment (IAA) has attracted increasing attention recently but is still challenging due to its high abstraction and complexity. Intuitively, image emotion and aesthetics are both human subjective feelings evoked by visual content. Previous researches have demonstrated that image aesthetics has intrinsic relationships with emotion. In fact, human emotional experience has potential impact on the perception of image aesthetics. Therefore, emotion information can be exploited to enhance aesthetic representation learning. Inspired by this, this paper presents an Emotion-Aware Hierarchical Interaction NETwork (EAHI-NET) for multimodal image aesthetics assessment, which explores both intra-modal and inter-modal interactions between aesthetics and emotions hierarchically. Specifically, we first propose the intra-modal emotion-aesthetics interaction module to obtain emotion-enhanced visual and textual aesthetic representations respectively. Then we propose the inter-modal feature enhancement to obtain the cross-modal aesthetic and emotion features. Finally, we design the inter-modal emotion-aesthetics interaction module to further investigate the cross-modal interplay between aesthetics and emotion, based on which hierarchical feature representations are achieved for multimodal IAA. The extensive experiments show that the proposed method can outperform the state-of-the-arts on multimodal AVA and Photo.net datasets.}
}
@incollection{SIRIANNI2015565,
title = {Chapter Nineteen - Chronic Pain Syndromes, Mechanisms, and Current Treatments},
editor = {Theodore J. Price and Gregory Dussor},
series = {Progress in Molecular Biology and Translational Science},
publisher = {Academic Press},
volume = {131},
pages = {565-611},
year = {2015},
booktitle = {Molecular and Cell Biology of Pain},
issn = {1877-1173},
doi = {https://doi.org/10.1016/bs.pmbts.2015.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877117315000058},
author = {Justin Sirianni and Mohab Ibrahim and Amol Patwardhan},
keywords = {Chronic pain, Cancer, Fibromyalgia, Mechanisms, Treatments},
abstract = {Although acute pain is a physiological response warning the human body of possible harm, chronic pain can be a pathological state associated with various diseases or a disease in itself. In the United States alone, around one-third of the population has experienced a chronic pain condition and annual cost to the society is in the range of 500–600 billion dollars.1 It should be noted that if at all this is a very modest estimate, it surpasses the costs associated with cancer, heart disease, and diabetes combined.1 Unfortunately, despite these humongous costs, the treatment of chronic pain is inadequate.1 Chronic pain affects individuals in a variety of forms, and below we highlight some of the most common chronic pain conditions seen in a pain clinic. Most of these disorders are difficult to treat and typically require multimodal therapy including pharmacotherapy, behavioral modification, and targeted interventions. We have summarized the scope of each disorder, clinical features, proposed mechanisms, and current therapies for them (Table 1).}
}