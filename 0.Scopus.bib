Scopus
EXPORT DATE: 14 February 2025

@ARTICLE{Cang20231598,
	author = {Cang, Xi Laura and Bucci, Paul and Rantala, Jussi and MacLean, Karon E.},
	title = {Discerning Affect From Touch and Gaze During Interaction With a Robot Pet},
	year = {2023},
	journal = {IEEE Transactions on Affective Computing},
	volume = {14},
	number = {2},
	pages = {1598 – 1612},
	doi = {10.1109/TAFFC.2021.3094894},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110912919&doi=10.1109%2fTAFFC.2021.3094894&partnerID=40&md5=036707bfe3e69e0d5e4f1c52812d2309},
	affiliations = {University of British Columbia, Vancouver, V6T 1Z4, BC, Canada; University of Tampere, Tampere, 33100, Finland},
	abstract = {Practical affect recognition needs to be efficient and unobtrusive in interactive contexts. One approach to a robust realtime system is to sense and automatically integrate multiple nonverbal sources. We investigated how users' touch, and secondarily gaze, perform as affect-encoding modalities during physical interaction with a robot pet, in comparison to more-studied biometric channels. To elicit authentically experienced emotions, participants recounted two intense memories of opposing polarity in Stressed-Relaxed or Depressed-Excited conditions. We collected data (N=30) from a touch sensor embedded under robot fur (force magnitude and location), a robot-adjacent gaze tracker (location), and biometric sensors (skin conductance, blood volume pulse, respiration rate). Cross-validation of Random Forest classifiers achieved best-case accuracy for combined touch-with-gaze approaching that of biometric results: where training and test sets include adjacent temporal windows, subject-dependent prediction was 94% accurate. In contrast, subject-independent Leave-One-participant-Out predictions resulted in 30% accuracy (chance 25%). Performance was best where participant information was available in both training and test sets. Addressing computational robustness for dynamic, adaptive realtime interactions, we analyzed subsets of our multimodal feature set, varying sample rates and window sizes. We summarize design directions based on these parameters for this touch-based, affective, and hard, realtime robot interaction application.  © 2010-2012 IEEE.},
	author_keywords = {Affective touch; emotion classification; human-robot interaction; multimodal interaction; therapeutic robot},
	keywords = {Biometrics; Classification (of information); Decision trees; Emotion Recognition; Eye tracking; Interactive computer systems; Machine design; Robots; Affect recognition; Condition; Encodings; Force magnitude; Nonverbals; Physical interactions; Real - Time system; Test sets; Touch sensors; Training sets; Real time systems},
	correspondence_address = {X.L. Cang; University of British Columbia, Vancouver, V6T 1Z4, Canada; email: cang@cs.ubc.ca},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {19493045},
	language = {English},
	abbrev_source_title = {IEEE Trans. Affective Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Kawde2018890,
	author = {Kawde, Piyush and Verma, Gyanendra K.},
	title = {Multimodal affect recognition in V-A-D space using deep learning},
	year = {2018},
	journal = {Proceedings of the 2017 International Conference On Smart Technology for Smart Nation, SmartTechCon 2017},
	pages = {890 – 895},
	doi = {10.1109/SmartTechCon.2017.8358500},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048042200&doi=10.1109%2fSmartTechCon.2017.8358500&partnerID=40&md5=53ea7035e98dab5070dc4d62cd48babe},
	affiliations = {Dept. of Computer Engineering, NIT Kurukshetra, Haryana, India},
	abstract = {Now-A-days one of the most challenging problem is automatic affective state recognition. To identify emotions from physiological signals a knowledgeable and advanced learning algorithm is required that can represent high level abstraction of signals. This study presents the deployment of deep learning network (DLN) to determine unspecified feature interrelationship between input data. The DLN is implemented with two semi-supervised algorithms i.e.; Stack auto-encoder (SAE) and Deep Belief Network (DBN) individually on DEAP database for emotion state classification. A Bayesian inference classification based decision fusion method is used to improve the classification and the results has been improvised by almost 3% in case of valence and arousal of DBN and almost 15% and 6% in SAE than the individual classification results of SAE and DBN, however in case of dominance the result has been improved by almost 9% than SAE. © 2017 IEEE.},
	author_keywords = {Affect Recognition; Arousal and Dominance; Bayesian inference classification; Deep Belief Network; Physiological signals; Stacked Auto-Encoder; Valence},
	keywords = {Bayesian networks; Biomedical signal processing; Classification (of information); Inference engines; Learning algorithms; Physiology; Signal encoding; Affect recognition; Arousal and Dominance; Auto encoders; Bayesian inference; Deep belief networks; Physiological signals; Valence; Deep learning},
	editor = {Kodabagi M.M. and Manvi S.S. and Hulipalled V.R. and Niranjan S.K.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153860568-4},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Smart Technol. Smart Nation, SmartTechCon},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2017 International Conference On Smart Technology for Smart Nation, SmartTechCon 2017; Conference date: 17 August 2017 through 19 August 2017; Conference code: 136447}
}

@ARTICLE{Harakawa201716963,
	author = {Harakawa, Ryosuke and Ogawa, Takahiro and Haseyama, Miki},
	title = {Extracting Hierarchical Structure of Web Video Groups Based on Sentiment-Aware Signed Network Analysis},
	year = {2017},
	journal = {IEEE Access},
	volume = {5},
	pages = {16963 – 16973},
	doi = {10.1109/ACCESS.2017.2741098},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028507496&doi=10.1109%2fACCESS.2017.2741098&partnerID=40&md5=b16c98f06247bc198751de11d026bb77},
	affiliations = {Graduate School of Information Science and Technology, Hokkaido University, Sapporo, 060-0814, Japan},
	abstract = {Sentiment in multimedia contents has an influence on their topics, since multimedia contents are tools for social media users to convey their sentiment. Performance of applications such as retrieval and recommendation will be improved if sentiment in multimedia contents can be estimated; however, there have been few works in which such applications were realized by utilizing sentiment analysis. In this paper, a novel method for extracting the hierarchical structure of Web video groups based on sentiment-aware signed network analysis is presented to realize Web video retrieval. First, the proposed method estimates latent links between Web videos by using multimodal features of contents and sentiment features obtained from texts attached to Web videos. Thus, our method enables construction of a signed network that reflects not only similarities but also positive and negative relations between topics of Web videos. Moreover, an algorithm to optimize a modularity-based measure, which can adaptively adjust the balance between positive and negative edges, was newly developed. This algorithm detects Web video groups with similar topics at multiple abstraction levels; thus, successful extraction of the hierarchical structure becomes feasible. By providing the hierarchical structure, users can obtain an overview of many Web videos and it becomes feasible to successfully retrieve the desired Web videos. Results of experiments using a new benchmark dataset, YouTube-8M, validate the contributions of this paper, i.e., 1) the first attempt to utilize sentiment analysis for Web video grouping and 2) a novel algorithm for analyzing a weighted signed network derived from sentiment and multimodal features. © 2013 IEEE.},
	author_keywords = {network analysis; sentiment analysis; signed network; video clustering; Video retrieval},
	keywords = {Clustering algorithms; Electric network analysis; Extraction; Feature extraction; File editors; Media streaming; Multimedia services; Multimedia systems; Tools; Algorithm design and analysis; Multi-media communications; Sentiment analysis; Signed networks; Social network services; Streaming media; Video clustering; Video retrieval; Data mining},
	correspondence_address = {R. Harakawa; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, 060-0814, Japan; email: harakawa@lmd.ist.hokudai.ac.jp},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Ullah20174973,
	author = {Ullah, Mohammad Aman and Azman, Norhidayah and Islam, Mohammad Monirul and Zaki, Zulkifly Mohd},
	title = {Multimodal sentiment analysis: A study towards future directions},
	year = {2017},
	journal = {Advanced Science Letters},
	volume = {23},
	number = {5},
	pages = {4973 – 4976},
	doi = {10.1166/asl.2017.8979},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023757700&doi=10.1166%2fasl.2017.8979&partnerID=40&md5=c2ed0cf5ccf99699a5c104a618c6fa21},
	affiliations = {Faculty of Science and Technology, University Sains Islam Malaysia (USIM), Bandar Baru Nilai, Nilai, 71800, Negeri Sembilan, Malaysia; Faculty of Science and Engineering, International Islamic University Chittagong, Kumira, Sitakunda, Bangladesh},
	abstract = {The medium of user expressions on the Web, especially in social media such as Facebook and Twitter, have evolved from text to multimedia such as images, audio, and video. This multimodal form of expression has become an important information resource for different organizations and institutions in their decision-making and setting strategies. But, the scatter in multimodal sentiments of the users poses a demand for more analysis to derive useful data. This analysis is known as Multimodal Sentiment Analysis (MSA). This paper presents a comprehensive overview for future researchers on recent research of different modes; individually and together to find the gaps in terms of tasks, approaches theories and applications used. The study showed that each and every mode presents different problematic issues which have not been fully solved yet, such as feature points of a face, voice clarity in audio and so on, and are great research areas for future work. There seems to be no single approach, theory, and tool which can support MSA. Moreover, this paper categorizes and summarizes recent work on the basis of tasks, approaches, and theories. © 2017 American Scientific Publishers All rights reserved.},
	author_keywords = {Categorization; Multimodal; Polarity; Sentiment Analysis; Summarization},
	publisher = {American Scientific Publishers},
	issn = {19366612},
	language = {English},
	abbrev_source_title = {Adv. Sci. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yin201793,
	author = {Yin, Zhong and Zhao, Mengyuan and Wang, Yongxiong and Yang, Jingdong and Zhang, Jianhua},
	title = {Recognition of emotions using multimodal physiological signals and an ensemble deep learning model},
	year = {2017},
	journal = {Computer Methods and Programs in Biomedicine},
	volume = {140},
	pages = {93 – 110},
	doi = {10.1016/j.cmpb.2016.12.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006761745&doi=10.1016%2fj.cmpb.2016.12.005&partnerID=40&md5=86821ef94228f379c965b47e64c4c597},
	affiliations = {Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, 200093, China; School of Social Sciences, University of Shanghai for Science and Technology, Shanghai, 200093, China; Department of Automation, East China University of Science and Technology, Shanghai, China},
	abstract = {Background and Objective Using deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions. Methods In this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological-data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states. Results DEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%. Conclusions The superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances. © 2016 Elsevier Ireland Ltd},
	author_keywords = {Affective computing; Deep learning; Emotion recognition; Ensemble learning; Physiological signals},
	keywords = {Emotions; Humans; Learning; Models, Psychological; Abstracting; Biomedical signal processing; Graphic methods; Learning systems; Physiological models; Physiology; Affective Computing; Emotion recognition; Ensemble learning; Feature representation; Physiological features; Physiological signals; Recognition of emotion; Recognizing Human Emotion; arousal; Article; data base; deep learning; emotion; human computer interaction; learning; learning algorithm; motivation; recognition; human; psychological model; Deep learning},
	publisher = {Elsevier Ireland Ltd},
	issn = {01692607},
	coden = {CMPBE},
	pmid = {28254094},
	language = {English},
	abbrev_source_title = {Comput. Methods Programs Biomed.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 334}
}

@CONFERENCE{Schroder2015456,
	author = {Schroder, Marc and Bevacqua, Elisabetta and Cowie, Roddy and Eyben, Florian and Gunes, Hatice and Heylen, DIrk and Ter Maat, Mark and McKeown, Gary and Pammi, Sathish and Pantic, Maja and Pelachaud, Catherine and Schuller, Bjorn and De Sevin, Etienne and Valstar, Michel and Wollmer, Martin},
	title = {Building autonomous sensitive artificial listeners (Extended abstract)},
	year = {2015},
	journal = {2015 International Conference on Affective Computing and Intelligent Interaction, ACII 2015},
	pages = {456 – 462},
	doi = {10.1109/ACII.2015.7344610},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964036733&doi=10.1109%2fACII.2015.7344610&partnerID=40&md5=20ff7ebb594682c082dbc1cddbe856bc},
	affiliations = {AudEERING UG, Gilching, Germany; Imperial College London, United Kingdom; University of Nottingham, United Kingdom; Queen Mary University London, United Kingdom; National Engineering School of Brest, France; Queen's University Belfast, United Kingdom; University of Twente, Netherlands; German Research Centre for Artificial Intelligence (DFKI), Germany; CNRSLTCI, Telecom ParisTech, Paris, France; Technische Universität München, Germany},
	abstract = {This paper describes a substantial effort to build a real-time interactive multimodal dialogue system with a focus on emotional and non-verbal interaction capabilities. The work is motivated by the aim to provide technology with competences in perceiving and producing the emotional and non-verbal behaviours required to sustain a conversational dialogue. We present the Sensitive Artificial Listener (SAL) scenario as a setting which seems particularly suited for the study of emotional and non-verbal behaviour, since it requires only very limited verbal understanding on the part of the machine. This scenario allows us to concentrate on non-verbal capabilities without having to address at the same time the challenges of spoken language understanding, task modeling etc. We first summarise three prototype versions of the SAL scenario, in which the behaviour of the Sensitive Artificial Listener characters was determined by a human operator. These prototypes served the purpose of verifying the effectiveness of the SAL scenario and allowed us to collect data required for building system components for analysing and synthesising the respective behaviours. We then describe the fully autonomous integrated real-time system we created, which combines incremental analysis of user behaviour, dialogue management, and synthesis of speaker and listener behaviour of a SAL character displayed as a virtual agent. We discuss principles that should underlie the evaluation of SAL-type systems. Since the system is designed for modularity and reuse, and since it is publicly available, the SAL system has potential as a joint research tool in the affective computing research community. © 2015 IEEE.},
	keywords = {Intelligent computing; Interactive computer systems; Modeling languages; Real time systems; Speech; Speech processing; Speech recognition; Affective Computing; Dialogue management; Extended abstracts; Incremental analysis; Multimodal dialogue systems; Non-verbal behaviours; Spoken language understanding; Verbal interaction; Behavioral research},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147999953-8},
	language = {English},
	abbrev_source_title = {Int. Conf. Affect. Comput. Intell. Interact., ACII},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2015 International Conference on Affective Computing and Intelligent Interaction, ACII 2015; Conference date: 21 September 2015 through 24 September 2015; Conference code: 118715}
}

@BOOK{Modak202481,
	author = {Modak, Santanu and Ghosh, Subhasmita},
	title = {Transfer learning with joint fine-tuning for multimodal sentiment analysis},
	year = {2024},
	journal = {Deep Learning Concepts in Operations Research},
	pages = {81 – 88},
	doi = {10.1201/9781003433309-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203034544&doi=10.1201%2f9781003433309-8&partnerID=40&md5=42d8d42d9bb7152f56e2ccf7ad26f8ea},
	affiliations = {Bengal College of Engineering and Technology, Durgapur, India; Surendranath Evening College, Sealdah, India},
	abstract = {This study offers a fresh solution to the issue of natural language inference. The suggested method is built on a deep learning model that can predict the relationship of entailment between two sentences. The model is able to produce state-of-the-art results on a variety of benchmark datasets because it was trained on a large corpus of natural language inference data. The proposed approach has several advantages over previous approaches. It can first learn from a substantial frame of data, which enables it to derive to modish examples. Furthermore, it can depict the meaning of sentences in a way that is consistent with the entailment relationship that exists between them. As a result, it can predict events more precisely.Countless prospective applications are available for the suggested method. It can be used to enhance the performance of activities requiring natural language processing, including summarization, question-answering, and machine translation. New tools for human-computer interaction, such chat bots and virtual assistants, can also be created using it. The findings of this study imply that the suggested strategy is a promising new technique for inferring meaning from natural language. It has a number of potentials uses and can produce state-of-the-art outcomes on a range of benchmark datasets. © 2025 Taylor & Francis Group, LLC. All rights reserved.},
	publisher = {CRC Press},
	isbn = {978-104010236-7; 978-103255379-5},
	language = {English},
	abbrev_source_title = {Deep Learn. Concepts in Oper. Res.},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Potamianos201427,
	author = {Potamianos, Alexandros},
	title = {Cognitive multimodal processing: From signal to behavior},
	year = {2014},
	journal = {RFMIR 2014 - Proceedings of the 2014 ACM Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges, Co-located with ICMI 2014},
	pages = {27 – 34},
	doi = {10.1145/2666253.2666264},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919338688&doi=10.1145%2f2666253.2666264&partnerID=40&md5=f338e3322e34630cd9d9a40b1845ba27},
	affiliations = {School of Electrical and Computer Engineering, National Tech. Univ. of Athens, Zografou, 15780, Greece},
	abstract = {Affective computing, social and behavioral signal processing are emerging research disciplines that attempt to automatically label the emotional, social and cognitive state of humans using features extracted from audio-visual streams. I argue that this monumental task cannot succeed unless the particularities of the human cognitive processing are incorporated into our models, especially given that often the quantities we are called to model are either biased cognitive abstractions of the real world or altogether fictional creations of our cognition. A variety of cognitive processes that make computational modeling especially challenging are outlined herein, notably: 1) (joint) attention and saliency, 2) common ground, conceptual semantic spaces and representation learning, 3) fusion across time, modalities and cognitive representation layers, and 4) dual-system processing (system one vs. system two) and cognitive decision nonlinearities. The grand challenges are outlined and examples are given illustrating how to design models that are both high-performing and respect basic cognitive organization principles. It is shown that such models can achieve good generalization and representation power, as well as model cognitive biases, a prerequisite for modeling and predicting human behavior.},
	author_keywords = {Affective computing; Behavioral signal processing; Cognitive models; Concept representations; Distributional semantic models; Dual process theory; Multimodal fusion; Social signal processing},
	keywords = {Behavioral research; Computation theory; Semantics; Affective Computing; Cognitive model; Concept representations; Dual-process theories; Multi-modal fusion; Semantic Model; Social signal processing; Signal processing},
	publisher = {Association for Computing Machinery},
	isbn = {978-145030615-7},
	language = {English},
	abbrev_source_title = {RFMIR - Proc. ACM Roadmapping Future Multimodal Interact. Res. Incl. Bus. Oppor. Challenges, Co-located ICMI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2014 ACM Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges, RFMIR 2014; Conference date: 16 November 2014 through 16 November 2014; Conference code: 109466}
}

@CONFERENCE{Zhang20225739,
	author = {Zhang, Jie and Zhao, Yin and Qian, Kai},
	title = {Enlarging the Long-time Dependencies via RL-based Memory Network in Movie Affective Analysis},
	year = {2022},
	journal = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia},
	pages = {5739 – 5750},
	doi = {10.1145/3503161.3548076},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151141798&doi=10.1145%2f3503161.3548076&partnerID=40&md5=8651d77914655102380885aad01b4566},
	affiliations = {Alibaba Group, Beijing, China},
	abstract = {Affective analysis of movies heavily depends on the causal understanding of the story with long-time dependencies. Limited by the existing sequence models such as LSTM, Transformer, etc., current works generally split the movies into dependent clips and predict the affective impacts (Valence/Arousal) independently, ignoring the long historical impacts across the clips. In this paper, we introduce a novel Reinforcement learning based Memory Net (RMN) for this task, which facilitates the prediction of the current clip to rely on the possible related historical clips of this movie. Compared with LSTM, the proposed method solves the long-time dependencies from two aspects. First, we introduce a readable and writable memory bank to store useful historical information, which solves the problem of the restricted memory unit for LSTM. However, the traditional parameters' update scheme of the memory network, when applied for long sequence prediction, still needs to store the gradients for long sequences. It suffers from gradient vanishing and exploding, similar to the issues of backpropagation through time (BPTT). For this problem, we introduce a reinforcement learning framework in the memory write operation. The memory updating scheme of the framework is optimized via one-step temporal difference, modeling the long-time dependencies using both the policy and value networks. Experiments on the LIRIS-ACCEDE dataset show that our method achieves significant performance gains over the existing methods. Besides, we also apply our method to other long sequence prediction tasks, such as music emotion recognition and video summarization, and also achieve state-of-the-art on those tasks. © 2022 ACM.},
	author_keywords = {affective analysis; memory network; multimodal; reinforcement learning; temporal dependency},
	keywords = {Emotion Recognition; Forecasting; Long short-term memory; Motion pictures; 'current; Affective analyse; Long sequences; Memory network; Multi-modal; Reinforcement learnings; Sequence models; Sequence prediction; Temporal dependency; Time dependency; Reinforcement learning},
	correspondence_address = {Y. Zhao; Alibaba Group, Beijing, China; email: yinzhao.zy@alibaba-inc.com},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145039203-7},
	language = {English},
	abbrev_source_title = {MM - Proc. ACM Int. Conf. Multimed.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 30th ACM International Conference on Multimedia, MM 2022; Conference date: 10 October 2022 through 14 October 2022; Conference code: 183235}
}

@ARTICLE{Li20196939,
	author = {Li, Zuhe and Fan, Yangyu and Jiang, Bin and Lei, Tao and Liu, Weihua},
	title = {A survey on sentiment analysis and opinion mining for social multimedia},
	year = {2019},
	journal = {Multimedia Tools and Applications},
	volume = {78},
	number = {6},
	pages = {6939 – 6967},
	doi = {10.1007/s11042-018-6445-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051683643&doi=10.1007%2fs11042-018-6445-z&partnerID=40&md5=f764d4746ad8bed3b9c3cdd05ae85d00},
	affiliations = {School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, 450002, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, 710072, China; College of Electronical and Information Engineering, Shaanxi University of Science and Technology, Xi’an, 710021, China; Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, 710119, China},
	abstract = {Social media sentiment analysis (also known as opinion mining) which aims to extract people’s opinions, attitudes and emotions from social networks has become a research hotspot. Conventional sentiment analysis concentrates primarily on the textual content. However, multimedia sentiment analysis has begun to receive attention since visual content such as images and videos is becoming a new medium for self-expression in social networks. In order to provide a reference for the researchers in this active area, we give an overview of this topic and describe the algorithms of sentiment analysis and opinion mining for social multimedia. Having conducted a brief review on textual sentiment analysis for social media, we present a comprehensive survey of visual sentiment analysis on the basis of a thorough investigation of the existing literature. We further give a summary of existing studies on multimodal sentiment analysis which combines multiple media channels. We finally summarize the existing benchmark datasets in this area, and discuss the future research trends and potential directions for multimedia sentiment analysis. This survey covers 100 articles during 2008–2018 and categorizes existing studies according to the approaches they adopt. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Multimedia sentiment; Opinion mining; Sentiment analysis; Social media},
	keywords = {Behavioral research; Sentiment analysis; Social networking (online); Surveys; Active area; Benchmark datasets; Media channel; Multimedia sentiment; Research trends; Social media; Textual content; Visual content; Data mining},
	correspondence_address = {Z. Li; School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, 450002, China; email: zuheli@126.com},
	publisher = {Springer New York LLC},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 113}
}

@CONFERENCE{Spaulding20181781,
	author = {Spaulding, Samuel},
	title = {Personalized robot tutors that learn from multimodal data},
	year = {2018},
	journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
	volume = {3},
	pages = {1781 – 1783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054731801&partnerID=40&md5=570c51b3cbbd8493f091b7af0ee5955e},
	affiliations = {MIT Media Lab, Cambrige, MA, United States},
	abstract = {As the cost of sensors decreases and ability to model and learn from multi-modal data increases, researchers are exploring how to use the unique qualities of physically embodied robots to help engage students and promote learning. These robots are designed to emulate the emotive, perceptual, and empathic abilities of human teachers, and are capable of replicating some of the benefits of one-on-one tutoring from human teachers. My thesis research focuses on developing methods for robots to analyze and integrate multimodal data including speech, facial expressions, and task performance to build rich models of the user's knowledge and preferences. These student models are then used to provide personalized educational experiences, such as optimal curricular sequencing, or leaning preferences for educational style. In this abstract, we summarize past projects in this area and discuss applications such as learning from affective signals and model transfer across tasks. © 2018 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.},
	author_keywords = {Human-robot interaction; Multimodal interaction; Social robotics},
	keywords = {Autonomous agents; Modal analysis; Multi agent systems; Robotics; Students; Teaching; Educational experiences; Facial Expressions; Models of the user; Multi-modal data; Multi-Modal Interactions; One-on-one tutoring; Social robotics; Task performance; Human robot interaction},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)},
	issn = {15488403},
	isbn = {978-151086808-3},
	language = {English},
	abbrev_source_title = {Proc. Int. Joint Conf. Auton. Agents Multiagent Syst., AAMAS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 17th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2018; Conference date: 10 July 2018 through 15 July 2018; Conference code: 139890}
}

@CONFERENCE{Girard2023,
	author = {Girard, Jeffrey M. and Tie, Yanmei and Liebenthal, Einat},
	title = {DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis},
	year = {2023},
	journal = {2023 11th International Conference on Affective Computing and Intelligent Interaction, ACII 2023},
	doi = {10.1109/ACII59096.2023.10388135},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184661577&doi=10.1109%2fACII59096.2023.10388135&partnerID=40&md5=01b83276292b03f475e4a075f820da19},
	affiliations = {University of Kansas, Department of Psychology, Lawrence, KS, United States; Harvard Medical School, Brigham and Women's Hospital, Boston, MA, United States; Harvard Medical School, McLean Hospital, Belmont, MA, United States},
	abstract = {In this paper, we describe the design, collection, and validation of a new video database that includes holistic and dynamic emotion ratings from 83 participants watching 22 affective movie clips. In contrast to previous work in Affective Computing, which pursued a single 'ground truth' label for the affective content of each moment of each video (e.g., by averaging the ratings of 2 to 7 trained participants), we embrace the subjectivity inherent to emotional experiences and provide the full distribution of all participants' ratings (with an average of 76.7 raters per video). We argue that this choice represents a paradigm shift with the potential to unlock new research directions, generate new hypotheses, and inspire novel methods in the Affective Computing community. We also describe several interdisciplinary use cases for the database: to provide dynamic norms for emotion elicitation studies (e.g., in psychology, medicine, and neuroscience), to train and test affective content analysis algorithms (e.g., for dynamic emotion recognition, video summarization, and movie recommendation), and to study subjectivity in emotional reactions (e.g., to identify moments of emotional ambiguity or ambivalence within movies, identify predictors of subjectivity, and develop personalized affective content analysis algorithms).  © 2023 IEEE.},
	author_keywords = {affective computing; content analysis; database; emotion elicitation; movie clips; multimodal; subjectivity},
	keywords = {Emotion Recognition; Motion pictures; Video recording; Affective Computing; Analysis algorithms; Content analysis; Emotion elicitation; Emotional experiences; Ground truth; Movie clips; Multi-modal; Subjectivity; Video database; Database systems},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835032743-4},
	language = {English},
	abbrev_source_title = {Int. Conf. Affect. Comput. Intell. Interact., ACII},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 11th International Conference on Affective Computing and Intelligent Interaction, ACII 2023; Conference date: 10 September 2023 through 13 September 2023; Conference code: 196763; All Open Access, Green Open Access}
}

@CONFERENCE{Djokic2021,
	author = {Djokic, Vesna G. and Shutova, Ekaterina and Fiebrink, Rebecca},
	title = {MetaVR: Understanding metaphors in the mind and relation to emotion through immersive, spatial interaction},
	year = {2021},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	doi = {10.1145/3411763.3451565},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105811113&doi=10.1145%2f3411763.3451565&partnerID=40&md5=57fcdd36ae2a93170a0a5f733c822ef8},
	affiliations = {Goldsmiths University of London, United Kingdom and University of Amsterdam, Netherlands},
	abstract = {Metaphorical thinking acts as a bridge between embodiment and abstraction and helps to flexibly organize human knowledge and behavior. Yet its role in embodied human-computer interface design, and its potential for supporting goals such as self-awareness and well-being, have not been extensively explored in the HCI community. We have designed a system called MetaVR to support the creation and exploration of immersive, multimodal metaphoric experiences, in which people's bodily actions in the physical world are linked to metaphorically relevant actions in a virtual reality world. As a team of researchers in interaction, neuroscience, and linguistics, we have created MetaVR to support research exploring the impact of such metaphoric interactions on human emotion and well-being. We have used MetaVR to create a proof-of-concept interface for immersive, spatial interactions underpinned by the WELL-BEING is VERTICALITY conceptual mapping - the known association of g€good'=g€up' and g€bad'=g€down'. Researchers and developers can currently interact with this proof of concept to configure various metaphoric interactions or personifications that have positive associations (e.g., g€being like a butterfly' or g€being like a flower') and also involve vertical motion (e.g., a butterfly might fly upwards, or a flower might bloom upwards). Importantly, the metaphoric interactions supported in MetaVR do not link human movement to VR actions in one-to-one ways, but rather use abstracted relational mappings in which events in VR (e.g., the blooming of a virtual flower) are contingent not merely on a "correct"gesture being performed, but on aspects of verticality exhibited in human movement (e.g., in a very simple case, the time a person's hands spend above some height threshold). This work thus serves as a small-scale vehicle for us to research how such interactions may impact well-being. Relatedly, it highlights the potential of using virtual embodied interaction as a tool to study cognitive processes involved in more deliberate/functional uses of metaphor and how this relates to emotion processing. By demonstrating MetaVR and metaphoric interactions designed with it at CHI Interactivity, and by offering the MetaVR tool to other researchers, we hope to inspire new perspectives, discussion, and research within the HCI community about the role that such metaphoric interaction may play, in interfaces designed for well-being and beyond. © 2021 Owner/Author.},
	author_keywords = {Multimodal Affective Computing; Multimodal Interaction for Well-Being},
	keywords = {Association reactions; Human engineering; Mapping; Virtual reality; Cognitive process; Conceptual mapping; Embodied interaction; Emotion processing; Human computer interfaces; Proof of concept; Relational mapping; Spatial interaction; Behavioral research},
	publisher = {Association for Computing Machinery},
	isbn = {978-145038095-9},
	language = {English},
	abbrev_source_title = {Conf Hum Fact Comput Syst Proc},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2021 CHI Conference on Human Factors in Computing Systems: Making Waves, Combining Strengths, CHI EA 2021; Conference date: 8 May 2021 through 13 May 2021; Conference code: 168787; All Open Access, Green Open Access}
}

@CONFERENCE{Raj2024,
	author = {Raj, Ankit and Raj, Mehul and Umasankari, N. and Geethanjali, D.},
	title = {Document-Based Text Summarization using T5 small and gTTS},
	year = {2024},
	journal = {2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems, ADICS 2024},
	doi = {10.1109/ADICS58448.2024.10533605},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195172009&doi=10.1109%2fADICS58448.2024.10533605&partnerID=40&md5=5db397f334ccdad37a26ac820fa0cc94},
	affiliations = {Sathyabama Institute Of Science And Technology, Department Of Computer Science And Engineering, Chennai, India},
	abstract = {The research project on 'Deep Learning-Based Text Summarization System using T5 small and gTTS' introduces a method to automatically extract and understand information from PDFs. The first step is to extract text from PDF files accurately. Following this, advanced natural language processing techniques are applied, utilizing a BERT-based model for sentiment analysis to identify emotional nuances in the text. Additionally, the integration of the T5 model streamlines the text summarization process, condensing extensive information into a clear and concise summary. The sophistication of the project is enhanced through the inclusion of Google Text-to-Speech (GTTS) capabilities, converting the written summary into an audio file. This feature accommodates various user preferences, improving overall accessibility. The research establishes a multimodal strategy for information dissemination, delivering the summary in both written and audio formats for a concise yet inclusive presentation. Beyond its technical contributions, the document summarization system has applications in education, content curation, and information retrieval. This system can assist educators in creating concise educational materials, support content curators in efficiently summarizing articles, and aid users in quickly extracting relevant information from large datasets, showcasing its versatility and potential impact across different fields. The integration of advanced natural language processing techniques underscores their adaptability and efficiency in handling textual data, ultimately enhancing the overall user experience and accommodating a broader spectrum of use.  © 2024 IEEE.},
	author_keywords = {audio summary; gTTS; Sentiment analysis; T5 small; Text-extraction; Text-summarization},
	keywords = {Data handling; Deep learning; Information dissemination; Large datasets; Search engines; Audio summary; Document-based; GTTS; Language processing techniques; Natural languages; Sentiment analysis; Summarization systems; T5 small; Text extraction; Text Summarisation; Sentiment analysis},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835036482-8},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Data Eng. Intell. Comput. Syst., ADICS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems, ADICS 2024; Conference date: 17 April 2024 through 18 April 2024; Conference code: 199752}
}

@CONFERENCE{Kashyap2018365,
	author = {Kashyap, Abhinav Ramesh and Von Der Weth, Christian and Cheng, Zhiyong and Kankanhalli, Mohan},
	title = {EPICURE - Aspect-based multimodal review summarization},
	year = {2018},
	journal = {WebSci 2018 - Proceedings of the 10th ACM Conference on Web Science},
	pages = {365 – 369},
	doi = {10.1145/3201064.3202917},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049396093&doi=10.1145%2f3201064.3202917&partnerID=40&md5=05e53c08643cb827bb734e17af330866},
	affiliations = {SeSaMe Centre, Smart Systems Insitute, National University of Singapore, Singapore},
	abstract = {Restaurant reviews are popular and a valuable source of information. Often, large number of reviews are written for restaurants which warrants the need for automated summarization systems. In this paper we present epicure, a novel text and image summarization platform. For the summarization of opinionated content like reviews, considering different aspects have largely been ignored, and we address this by creating balanced reviews for different aspects like food and service. We argue that traditional criteria for extractive review summarization such as coverage and diversity have limited applicability. We draw on the power and usefulness of submodular functions for extractive summarization and introduce novel submodular functions such as importance, freshness, purity, trustworthiness and balanced opinion. We are also one of the first to provide an image summary for different aspects of a restaurant by mapping text to images using a multimodal neural network, for which we provide initial experiments. We show the effectiveness of our platform by evaluating it against strong baselines and also use crowdsourcing experiments for a subjective comparison of our approach with existing works. © 2018 Association for Computing Machinery.},
	author_keywords = {Multimodal summarization; Online reviews; Sentence-to-image mapping; Sentiment analysis; Text classification; User study},
	keywords = {Classification (of information); Mapping; Sentiment analysis; Text processing; Image mapping; Multi-modal; Online reviews; Text classification; User study; Natural language processing systems},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145035563-6},
	language = {English},
	abbrev_source_title = {WebSci - Proc. ACM Conf. Web Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 10th ACM Conference on Web Science, WebSci 2018; Conference date: 27 May 2018 through 30 May 2018; Conference code: 136907}
}

@CONFERENCE{Wang20197216,
	author = {Wang, Yansen and Shen, Ying and Liu, Zhun and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
	title = {Words can shift: Dynamically adjusting word representations using nonverbal behaviors},
	year = {2019},
	journal = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
	pages = {7216 – 7223},
	doi = {10.1609/aaai.v33i01.33017216},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077799083&doi=10.1609%2faaai.v33i01.33017216&partnerID=40&md5=61516b7a5fad2f6b671762629829a9d7},
	affiliations = {Department of Computer Science, Tsinghua University, China; School of Computer Science, Carnegie Mellon University, United States},
	abstract = {Humans convey their intentions through the usage of both verbal and nonverbal behaviors during face-to-face communication. Speaker intentions often vary dynamically depending on different nonverbal contexts, such as vocal patterns and facial expressions. As a result, when modeling human language, it is essential to not only consider the literal meaning of the words but also the nonverbal contexts in which these words appear. To better model human language, we first model expressive nonverbal representations by analyzing the fine-grained visual and acoustic patterns that occur during word segments. In addition, we seek to capture the dynamic nature of nonverbal intents by shifting word representations based on the accompanying nonverbal behaviors. To this end, we propose the Recurrent Attended Variation Embedding Network (RAVEN) that models the fine-grained structure of nonverbal subword sequences and dynamically shifts word representations based on nonverbal cues. Our proposed model achieves competitive performance on two publicly available datasets for multimodal sentiment analysis and emotion recognition. We also visualize the shifted word representations in different nonverbal contexts and summarize common patterns regarding multimodal variations of word representations. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	keywords = {Artificial intelligence; Embeddings; Modeling languages; Visual languages; Face-to-face communications; Facial Expressions; Fine grained; Human language; Literals; Multi-modal; Non-verbal behaviours; Nonverbals; Pattern expressions; Word representations; Emotion Recognition},
	publisher = {AAAI Press},
	isbn = {978-157735809-1},
	language = {English},
	abbrev_source_title = {AAAI Conf. Artif. Intell., AAAI, Innov. Appl. Artif. .igence Conf., IAAI AAAI Symp. Educ. Adv. Artif. Intell., EAAI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 362; Conference name: 33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019; Conference date: 27 January 2019 through 1 February 2019; Conference code: 160302; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Chaudhuri201925,
	author = {Chaudhuri, Arindam},
	title = {Experimental setup: Visual and text sentiment analysis through hierarchical deep learning networks},
	year = {2019},
	journal = {SpringerBriefs in Computer Science},
	pages = {25 – 49},
	doi = {10.1007/978-981-13-7474-6_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064606419&doi=10.1007%2f978-981-13-7474-6_6&partnerID=40&md5=fc284887dbeeebd0ebfbba8129e941a0},
	affiliations = {Samsung R & D Institute Delhi, Noida, India},
	abstract = {The experimental setup consists of performing visual and text sentiment analysis through hierarchical based deep learning networks. A brief discussion on the deep learning networks is presented for the interested readers. The cross-media bag-of-words model (CBM) is used as the baseline method. The basic aspects of the gated feedforward recurrent neural networks (GFRNN) are illustrated. The mathematical abstraction of HGFRNN is vividly explained. The chapter concludes with hierarchical gated feedforward recurrent neural networks for multimodal sentiment analysis. © 2019, The Author(s), under exclusive to Springer Nature Singapore Pte Ltd.},
	keywords = {Feedforward neural networks; Information retrieval; Learning systems; Sentiment analysis; Bag-of-words models; Baseline methods; Cross-media; Feed-Forward; Learning network; Mathematical abstraction; Multi-modal; Recurrent neural networks},
	correspondence_address = {A. Chaudhuri; Samsung R & D Institute Delhi, Noida, India; email: arindamphdthesis@gmail.com},
	publisher = {Springer},
	issn = {21915768},
	language = {English},
	abbrev_source_title = {SpringerBriefs Comp. Sci.},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Shah2016102,
	author = {Shah, Rajiv Ratn and Yu, Yi and Verma, Akshay and Tang, Suhua and Shaikh, Anwar Dilawar and Zimmermann, Roger},
	title = {Leveraging multimodal information for event summarization and concept-level sentiment analysis},
	year = {2016},
	journal = {Knowledge-Based Systems},
	volume = {108},
	pages = {102 – 109},
	doi = {10.1016/j.knosys.2016.05.022},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975497501&doi=10.1016%2fj.knosys.2016.05.022&partnerID=40&md5=2fd37f8c69321443f2b3d56745c99ef6},
	affiliations = {School of Computing, National University of Singapore, Singapore; Digital Content and Media Sciences Research, National Institute of Informatics, Japan; Department of Computer Science and Engineering, MNNIT, India; Graduate School of Informatics and Engineering, UEC, Japan; Department of Computer Engineering, Delhi Technological University, India},
	abstract = {The rapid growth in the amount of user-generated content (UGCs) online necessitates for social media companies to automatically extract knowledge structures (concepts) from photos and videos to provide diverse multimedia-related services. However, real-world photos and videos are complex and noisy, and extracting semantics and sentics from the multimedia content alone is a very difficult task because suitable concepts may be exhibited in different representations. Hence, it is desirable to analyze UGCs from multiple modalities for a better understanding. To this end, we first present the EventBuilder system that deals with semantics understanding and automatically generates a multimedia summary for a given event in real-time by leveraging different social media such as Wikipedia and Flickr. Subsequently, we present the EventSensor system that aims to address sentics understanding and produces a multimedia summary for a given mood. It extracts concepts and mood tags from visual content and textual metadata of UGCs, and exploits them in supporting several significant multimedia-related services such as a musical multimedia summary. Moreover, EventSensor supports sentics-based event summarization by leveraging EventBuilder as its semantics engine component. Experimental results confirm that both EventBuilder and EventSensor outperform their baselines and efficiently summarize knowledge structures on the YFCC100M dataset. © 2016 Elsevier B.V.},
	author_keywords = {Multimedia summarization; Multimedia-related services; Multimodal analysis; Semantics analysis; Sentics analysis},
	keywords = {Image processing; Metadata; Modal analysis; Natural language processing systems; Semantics; Social networking (online); 00-01; 99-00; Multimedia summarization; Multimedia-related services; Multimodal analysis; Semantics analysis; Sentics analysis; Multimedia services},
	correspondence_address = {R.R. Shah; School of Computing, National University of Singapore, Singapore; email: rajiv@comp.nus.edu.sg},
	publisher = {Elsevier B.V.},
	issn = {09507051},
	coden = {KNSYE},
	language = {English},
	abbrev_source_title = {Knowl Based Syst},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 59}
}

@CONFERENCE{Taamneh20161483,
	author = {Taamneh, Salah and Dcosta, Malcolm and Kwon, Kyeong-An and Pavlidis, Ioannis},
	title = {Subjectbook: Hypothesis-driven ubiquitous visualization for affective studies},
	year = {2016},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	volume = {07-12-May-2016},
	pages = {1483 – 1489},
	doi = {10.1145/2851581.2892338},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014638669&doi=10.1145%2f2851581.2892338&partnerID=40&md5=4dc155b73650c88373abd1770e8d690b},
	affiliations = {Computational Physiology Lab., University of Houston, Houston, 77004, TX, United States},
	abstract = {Analyzing affective studies is challenging because they feature multimodal data, such as psychometric scores, imaging sequences, and signals from wearable sensors, with the latter streaming continuously for hours on end. Meaningful visual representations of such data can greatly facilitate insights and qualitative analysis. Various tools that were proposed to tackle this problem provide visualizations of the original data only; they do not support higher level abstractions. In this paper, we introduce SubjectBook, an interactive web-based tool for synchronizing, visualizing, exploring, and analyzing affective datasets. Uniquely, SubjectBook operates at three levels of abstraction, mirroring the stages of quantitative analysis in hypothesis-driven research. The top level uses a grid visualization to show the study's significant outcomes across subjects. The middle level summarizes, for each subject, context information along with the explanatory and response measurements in a construct reminiscent of an ID card. This enables the analyst to appreciate within subject phenomena. Finally, the bottom level brings together detailed information concerning the inner and outer state of human subjects along with their real-world interactions - a visualization fusion that supports cause and effect reasoning at the experimental session level. SubjectBook was evaluated on a case study focused on driving behaviors. © 2016 Authors.},
	author_keywords = {Affective computing; Affective datasets; Affective studies; Data visualization; Physiological signals; Qualitative analysis},
	keywords = {Abstracting; Human computer interaction; Human engineering; Visualization; Affective Computing; Affective dataset; Affective study; High-level abstraction; Higher-level abstraction; Imaging sequence; Multi-modal data; Physiological signals; Qualitative analysis; Visual representations; Data visualization},
	publisher = {Association for Computing Machinery},
	isbn = {978-145034082-3},
	language = {English},
	abbrev_source_title = {Conf Hum Fact Comput Syst Proc},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 34th Annual CHI Conference on Human Factors in Computing Systems, CHI EA 2016; Conference date: 7 May 2016 through 12 May 2016; Conference code: 125618}
}

@CONFERENCE{Wu20232231,
	author = {Wu, Shaoxiang and Dai, Damai and Qin, Ziwei and Liu, Tianyu and Lin, Binghuai and Cao, Yunbo and Sui, Zhifang},
	title = {Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion},
	year = {2023},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	volume = {1},
	pages = {2231 – 2243},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174384949&partnerID=40&md5=e5b7c2d98fc23042304fcd5a4300f661},
	affiliations = {MOE Key Lab of Computational Linguistics, Peking University, China; Tencent Cloud AI, China},
	abstract = {Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on multiple benchmarks covering multimodal sentiment analysis and multimodal summarization tasks. It proves that our model can effectively capture salient features from noisy and redundant video, audio, and text inputs. The code for this paper is publicly available at https://github.com/WSXRHFG/DBF. © 2023 Association for Computational Linguistics.},
	keywords = {Acoustic noise; Audio acoustics; Redundancy; De-noising; Denoising methods; Fine grained; Fusion model; Image texts; Multi-modal; Multi-modal fusion; Multiple modalities; Mutual information maximization; Noise filtering; Sentiment analysis},
	publisher = {Association for Computational Linguistics (ACL)},
	issn = {0736587X},
	isbn = {978-195942972-2},
	language = {English},
	abbrev_source_title = {Proc. Annu. Meet. Assoc. Comput Linguist.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023; Conference date: 9 July 2023 through 14 July 2023; Conference code: 192160}
}

@CONFERENCE{Jung2015387,
	author = {Jung, Merel M. and Cang, Xi Laura and Poel, Mannes and Maclean, Karon E.},
	title = {Touch challenge'15: Recognizing social touch gestures},
	year = {2015},
	journal = {ICMI 2015 - Proceedings of the 2015 ACM International Conference on Multimodal Interaction},
	pages = {387 – 390},
	doi = {10.1145/2818346.2829993},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959282492&doi=10.1145%2f2818346.2829993&partnerID=40&md5=70c235ea273b5414d409fab29d4fe98a},
	affiliations = {University of Twente, P.O. Box 217, AE, Enschede, 7500, Netherlands; University of British Columbia, 2366 Main Mall, Vancouver, V6T 1Z4, BC, Canada},
	abstract = {Advances in the field of touch recognition could open up applications for touch-based interaction in areas such as Human-Robot Interaction (HRI). We extended this challenge to the research community working on multimodal interaction with the goal of sparking interest in the touch modality and to promote exploration of the use of data processing techniques from other more mature modalities for touch recognition. Two data sets were made available containing labeled pressure sensor data of social touch gestures that were performed by touching a touch-sensitive surface with the hand. Each set was collected from similar sensor grids, but under conditions reflecting different application orientations: CoST: Corpus of Social Touch and HAART: The Human-Animal Affective Robot Touch gesture set. In this paper we describe the challenge protocol and summarize the results from the touch challenge hosted in conjunction with the 2015 ACM International Conference on Multimodal Interaction (ICMI). The most important outcomes of the challenges were: (1) transferring techniques from other modalities, such as image processing, speech, and human action recognition provided valuable feature sets; (2) gesture classification confusions were similar despite the various data processing methods used. © 2015 ACM.},
	author_keywords = {Social touch; Touch data set; Touch gesture recognition},
	keywords = {Data handling; Human robot interaction; Image processing; Interactive computer systems; Motion estimation; Robots; Speech recognition; Data processing techniques; Data set; Gesture classifications; Human robot Interaction (HRI); Human-action recognition; Multi-Modal Interactions; Social touch; Touch based interactions; Gesture recognition},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145033912-4},
	language = {English},
	abbrev_source_title = {ICMI - Proc. ACM Int. Conf. Multimodal Interact.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; Conference name: ACM International Conference on Multimodal Interaction, ICMI 2015; Conference date: 9 November 2015 through 13 November 2015; Conference code: 118057; All Open Access, Green Open Access}
}

@CONFERENCE{Sharma20225597,
	author = {Sharma, Shivam and Alam, Firoj and Akhtar, Md Shad and Dimitrov, Dimitar and Da San Martino, Giovanni and Firooz, Hamed and Halevy, Alon and Silvestri, Fabrizio and Nakov, Preslav and Chakraborty, Tanmoy},
	title = {Detecting and Understanding Harmful Memes: A Survey},
	year = {2022},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	pages = {5597 – 5606},
	doi = {10.24963/ijcai.2022/781},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130459583&doi=10.24963%2fijcai.2022%2f781&partnerID=40&md5=63174f5e00b9a9b358982b41669cf684},
	affiliations = {IIIT-Delhi, India; Wipro AI Labs, India; Qatar Computing Research Institute, HBKU, Qatar; Sofia University, Bulgaria; University of Padova, Italy; Facebook AI, United States; Sapienza University of Rome, Italy; Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates},
	abstract = {The automatic identification of harmful content online is of major concern for social media platforms, policymakers, and society. Researchers have studied textual, visual, and audio content, but typically in isolation. Yet, harmful content often combines multiple modalities, as in the case of memes. With this in mind, here we offer a comprehensive survey with a focus on harmful memes. Based on a systematic analysis of recent literature, we first propose a new typology of harmful memes, and then we highlight and summarize the relevant state of the art. One interesting finding is that many types of harmful memes are not really studied, e.g., such featuring self-harm and extremism, partly due to the lack of suitable datasets. We further find that existing datasets mostly capture multi-class scenarios, which are not inclusive of the affective spectrum that memes can represent. Another observation is that memes can propagate globally through repackaging in different languages and that they can also be multilingual, blending different cultures. We conclude by highlighting several challenges related to multimodal semiotics, technological constraints, and non-trivial social engagement, and we present several open-ended aspects such as delineating online harm and empirically examining related frameworks and assistive interventions, which we believe will motivate and drive future research. © 2022 International Joint Conferences on Artificial Intelligence. All rights reserved.},
	keywords = {Artificial intelligence; Automation; Blending; Audio content; Automatic identification; Multiple modalities; Policy makers; Social media platforms; Spectra's; State of the art; Systematic analysis; Textual content; Visual content; Surveys},
	editor = {De Raedt L. and De Raedt L.},
	publisher = {International Joint Conferences on Artificial Intelligence},
	issn = {10450823},
	isbn = {978-195679200-3},
	language = {English},
	abbrev_source_title = {IJCAI Int. Joint Conf. Artif. Intell.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40; Conference name: 31st International Joint Conference on Artificial Intelligence, IJCAI 2022; Conference date: 23 July 2022 through 29 July 2022; Conference code: 182301; All Open Access, Bronze Open Access, Green Open Access}
}

@BOOK{Martin2009267,
	author = {Martin, Jean-Claude and Devillers, Laurence},
	title = {A multimodal corpus approach for the study of spontaneous emotions},
	year = {2009},
	journal = {Affective Information Processing},
	pages = {267 – 291},
	doi = {10.1007/978-1-84800-306-4_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885819730&doi=10.1007%2f978-1-84800-306-4_15&partnerID=40&md5=ac6e49e34f5b7338ce31cd8f4c3b53da},
	affiliations = {LIMSI-CNRS, France},
	abstract = {The design of future interactive affective computing systems requires the representation of spontaneous emotions and their associated multimodal signs. Current prototypes are often limited to the detection and synthesis of a few primary emotions and are most of the time grounded on acted data collected in-lab. In order to model the sophisticated relations between spontaneous emotions and their expressions in different modalities, an exploratory approach was defined.We collected and annotated a TV corpus of interviews. The collected data displayed emotions that are more complex than the six basic emotions (anger, fear, joy, sadness, surprise, disgust). We observed superpositions, masking, and conflicts between positive and negative emotions. We report several experiments that enabled us to provide some answers to questions such as how to reliably annotate and represent the multimodal signs of spontaneous complex emotions and at which level of abstraction and temporality. We also defined a copy-synthesis approach in which these behaviors were annotated, represented, and replayed by an expressive agent, enabling a validation and refinement of our annotations. We also studied individual differences in the perception of these blends of emotions. These experiments enabled the identification and definition of several levels of representation of emotions and their associated expressions that are relevant for spontaneous complex emotions. © 2009 Springer London.},
	correspondence_address = {J.-C. Martin; LIMSI-CNRS, France; email: MARTIN@LIMSI.FR},
	publisher = {Springer London},
	isbn = {978-184800305-7},
	language = {English},
	abbrev_source_title = {Affective Info. Proc.},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Zadeh2015587,
	author = {Zadeh, Amir},
	title = {Micro-opinion sentiment intensity analysis and summarization in online videos},
	year = {2015},
	journal = {ICMI 2015 - Proceedings of the 2015 ACM International Conference on Multimodal Interaction},
	pages = {587 – 591},
	doi = {10.1145/2818346.2823317},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959278750&doi=10.1145%2f2818346.2823317&partnerID=40&md5=01c8f844a65f5077bc4a6b80a215952c},
	affiliations = {Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, 15213, PA, United States},
	abstract = {There has been substantial progress in the field of text based sentiment analysis but little effort has been made to incorporate other modalities. Previous work in sentiment analysis has shown that using multimodal data yields to more accurate models of sentiment. Efforts have been made towards expressing sentiment as a spectrum of intensity rather than just positive or negative. Such models are useful not only for detection of positivity or negativity, but also giving out a score of how positive or negative a statement is. Based on the state of the art studies in sentiment analysis, prediction in terms of sentiment score is still far from accurate, even in large datasets [27]. Another challenge in sentiment analysis is dealing with small segments or micro opinions as they carry less context than large segments thus making analysis of the sentiment harder. This paper presents a Ph.D. thesis shaped towards comprehensive studies in multimodal micro-opinion sentiment intensity analysis. © 2015 ACM.},
	author_keywords = {Multimodal machine learning; Sentiment analysis; Sentiment summarization},
	keywords = {Artificial intelligence; Data mining; Interactive computer systems; Learning systems; Natural language processing systems; Intensity analysis; Large datasets; Multi-modal; Multi-modal data; Sentiment analysis; Sentiment scores; Sentiment summarization; State of the art; Modal analysis},
	correspondence_address = {A. Zadeh; Carnegie Mellon University, Pittsburgh, 5000 Forbes Avenue, 15213, United States; email: abagherz@cs.cmu.edu},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145033912-4},
	language = {English},
	abbrev_source_title = {ICMI - Proc. ACM Int. Conf. Multimodal Interact.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; Conference name: ACM International Conference on Multimodal Interaction, ICMI 2015; Conference date: 9 November 2015 through 13 November 2015; Conference code: 118057}
}

@BOOK{Voros20151,
	author = {Voros, Nikolaos S. and Antonopoulos, Christos P.},
	title = {Introduction to ARMOR project},
	year = {2015},
	journal = {Cyberphysical Systems for Epilepsy and Related Brain Disorders: Multi-Parametric Monitoring and Analysis for Diagnosis and Optimal Disease Management},
	pages = {1 – 10},
	doi = {10.1007/978-3-319-20049-1_1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943602510&doi=10.1007%2f978-3-319-20049-1_1&partnerID=40&md5=7279da77e7c0c1c0bb55fa3dc7421c87},
	affiliations = {Embedded System Design and Application Laboratory, Department of Computer and Informatics Engineering, Technological Educational Institute of Western Greece, Patras, Greece; Department of Clinical Neurophysiology and Epilepsy, Guy’s and St Thomas NHS Foundation Trust, London, United Kingdom; Neurophysiology Unit, Department of Physiology, University of Patras, Patras, Greece; Laboratory for Human Brain Dynamics, AAI Scientifi c Cultural Services Ltd, Nicosia, Cyprus},
	abstract = {Epilepsies affect 1-2 % of general population, especially in childhood and adolescence. Epileptic seizures, manifest with a wide range of paroxysmally recurring motor, cognitive, affective, and autonomic symptoms and EEG changes. Their recognition and full understanding is the basis of their optimal management. The yield of epilepsy diagnosis is considered unsatisfactory, as seizures occur unpredictably and typically outside hospital, other paroxysmal disorders are often misdiagnosed as epilepsy, and hospital evaluation costs of patients with uncertain clinical features or possibly mixed disorders are quite substantial. Reliable diagnosis requires state of the art monitoring and communication technologies providing real-time, accurate and continuous brain and body multi-parametric data measurements, suited to the patient’s medical condition and normal environment and facing issues of patient and data security, integrity and privacy. In this context, a cornerstone objective of the ARMOR project was to manage and analyze a large number of already acquired and new multimodal and advanced technology data from brain and body activities of epileptic patients and controls (MEG, multichannel EEG, ECG, GSR, EMG, etc.) aiming to design a more holistic, personalized, medically effi cient and economical monitoring system. New methods and tools have been developed for multimodal data pre-processing and fusion, real- time and offl ine data mining of multi-parametric streaming and archived data to discover patterns and associations between external indicators and mental states, lag correlation detection, identifi cation of motifs or outliers (vital signs changing signifi cantly), automatic summarization of results and effi cient medical context data management. In addition to the technical advances, work within research produced signifi cant clinical results and important new insights on the nature of sleep and its putative reciprocal relationship with sleep. © Springer International Publishing Switzerland 2015.},
	keywords = {Armor; Brain; Data handling; Diagnosis; Hospitals; Medical computing; Neurology; Sleep research; Advanced technology; Automatic summarization; Communication technologies; Context data managements; Correlation detection; Economical monitoring; Medical conditions; Technical advances; Information management},
	publisher = {Springer International Publishing},
	isbn = {978-331920049-1; 978-331920048-4},
	language = {English},
	abbrev_source_title = {Cyberphysical Systems for Epilepsy and Relat. Brain Disorders: Multi-Parametric Monitoring and Analysis for Diagnosis and Optimal Disease Management},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Xie2024,
	author = {Xie, Jiehang and Chen, Xuanbai and Zhao, Sicheng and Lu, Shao-Ping},
	title = {Video summarization via knowledge-aware multimodal deep networks},
	year = {2024},
	journal = {Knowledge-Based Systems},
	volume = {293},
	doi = {10.1016/j.knosys.2024.111670},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188988439&doi=10.1016%2fj.knosys.2024.111670&partnerID=40&md5=4a0b443a464a3fe7161d021ea6e6531f},
	affiliations = {TKLNDST, Tianjin, 300350, China; College of Computer Science, Nankai University, Tianjin, 300350, China; Robotics Institute, Carnegie Mellon University, United States; BNRist, Tsinghua University, Beijing, 100084, China},
	abstract = {Video summarization has unprecedented importance in facilitating the rapid browsing, retrieval, and comprehension of large numbers of videos. Benefiting from possessing rich prior knowledge of the raw video and the capability to filter less crucial frames by employing multimodal information, humans can condense a lengthy video into a compact and reasonable video summary. However, existing automated video summarization approaches struggle to determine which shots in a video are significant concurrently and robustly, which is detrimental to the generation of high-quality summaries. To improve the quality of video summaries further, drawing inspiration from human abilities, we propose a novel video summarization approach based on a knowledge-aware multimodal network (KAMN). In particular, we present a knowledge-based encoder to obtain the corresponding representation for each frame. This representation is composed of captured descriptive content and affections, which are retrieved from large-scale external knowledge bases. Owing to these knowledge bases, rich implicit knowledge is provided to better understand the viewed video. Moreover, to integrate the visual, audio, and implicit knowledge features more effectively and to identify valuable information across different modalities further, we design a fusion module to learn these multimodal feature relationships more thoroughly. KAMN operates in both unsupervised and supervised training modes. Objective quantitative experiments and subjective user studies were conducted using four publicly available datasets. The results verified the effectiveness of the proposed modules and demonstrated the superior performance yielded by our framework. © 2024 Elsevier B.V.},
	author_keywords = {External knowledge; Multimodal information; Video summarization},
	keywords = {Video recording; Automated video; External knowledge; Filter-less; Implicit knowledge; Multi-modal; Multi-modal information; Multimodal network; Prior-knowledge; Video summaries; Video summarization; Knowledge based systems},
	correspondence_address = {S.-P. Lu; College of Computer Science, Nankai University, Tianjin, 300350, China; email: slu@nankai.edu.cn},
	publisher = {Elsevier B.V.},
	issn = {09507051},
	coden = {KNSYE},
	language = {English},
	abbrev_source_title = {Knowl Based Syst},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Sharma20239763,
	author = {Sharma, Shivam and Agarwal, Siddhant and Suresh, Tharun and Nakov, Preslav and Akhtar, Md. Shad and Chakraborty, Tanmoy},
	title = {What Do You MEME? Generating Explanations for Visual Semantic Role Labelling in Memes},
	year = {2023},
	journal = {Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023},
	volume = {37},
	pages = {9763 – 9771},
	doi = {10.1609/aaai.v37i8.26166},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168239355&doi=10.1609%2faaai.v37i8.26166&partnerID=40&md5=5cb8a91ce75215b6269705d3b4475f9c},
	affiliations = {Indraprastha Institute of Information Technology, Delhi, India; Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates; Indian Institute of Technology, Delhi, India; Wipro AI Labs (Lab45), India},
	abstract = {Memes are powerful means for effective communication on social media. Their effortless amalgamation of viral visuals and compelling messages can have far-reaching implications with proper marketing. Previous research on memes has primarily focused on characterizing their affective spectrum and detecting whether the meme's message insinuates any intended harm, such as hate, offense, racism, etc. However, memes often use abstraction, which can be elusive. Here, we introduce a novel task - EXCLAIM, generating explanations for visual semantic role labeling in memes. To this end, we curate ExHVV, a novel dataset that offers natural language explanations of connotative roles for three types of entities - heroes, villains, and victims, encompassing 4,680 entities present in 3K memes. We also benchmark ExHVV with several strong unimodal and multimodal baselines. Moreover, we posit LUMEN, a novel multimodal, multi-task learning framework that endeavors to address EXCLAIM optimally by jointly learning to predict the correct semantic roles and correspondingly to generate suitable natural language explanations. LUMEN distinctly outperforms the best baseline across 18 standard natural language generation evaluation metrics. Our systematic evaluation and analyses demonstrate that characteristic multimodal cues required for adjudicating semantic roles are also helpful for generating suitable explanations. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	keywords = {Artificial intelligence; Learning systems; Natural language processing systems; Effective communication; Multi-modal; Natural language explanations; Novel task; Semantic role labeling; Semantic roles; Social media; Spectra's; Unimodal; Visual semantics; Semantics},
	editor = {Williams B. and Chen Y. and Neville J.},
	publisher = {AAAI Press},
	isbn = {978-157735880-0},
	language = {English},
	abbrev_source_title = {Proc. AAAI Conf. Artif. Intell., AAAI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 37th AAAI Conference on Artificial Intelligence, AAAI 2023; Conference date: 7 February 2023 through 14 February 2023; Conference code: 190493; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Ullah2017,
	author = {Ullah, Mohammad Aman and Islam, Md. Monirul and Azman, Norhidayah Binti and Zaki, Zulkifly Mohd},
	title = {An overview of Multimodal Sentiment Analysis research: Opportunities and Difficulties},
	year = {2017},
	journal = {2017 IEEE International Conference on Imaging, Vision and Pattern Recognition, icIVPR 2017},
	doi = {10.1109/ICIVPR.2017.7890858},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018177784&doi=10.1109%2fICIVPR.2017.7890858&partnerID=40&md5=68fba02cac5696a1a67ea9a00e9dd40c},
	affiliations = {Dept. of Computer Science and Engineering, International Islamic University Chittagong, Chittagong, 4203, Bangladesh; Faculty of Science and Technology, University Sains Islam Malaysia (USIM), Bandar Baru Nilai, Nilai, Negeri Sembilan, 71800, Malaysia},
	abstract = {The scatter form of multimedia data such as text, image, audio, and video posted regularly in the social media may contain useful information for the organizations. But, this information should be derived with the use of some form of analysis known as Multimodal Sentiment Analysis (MSA). But, there is a lack of proper analytic tools for such analysis. This paper presents a thorough overview of more than fifty most recent MSA research articles to find the gaps in terms of tasks, approaches theories and applications used till date. There seems to be no single approach, theory, and tool which can support MSA. The study showed that each and every mode presents different difficulties which have not bee n fully solved yet, such as feature points of a face, voice clarity in audio, video summarization and so on, and are great research opportunities for the future researchers. Also, this research recommends a list of existing and upcoming difficulties and opportunities of MSA research. © 2017 IEEE.},
	author_keywords = {Difficulties; Multimodal; Opportunities; Review; Sentiments},
	keywords = {Data mining; Natural language processing systems; Pattern recognition; Reviews; Difficulties; Multi-modal; Multimedia data; Opportunities; Research opportunities; Sentiment analysis; Sentiments; Video summarization; Modal analysis},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150906003-0},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Imaging, Vis. Pattern Recognit., icIVPR},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; Conference name: 2017 IEEE International Conference on Imaging, Vision and Pattern Recognition, icIVPR 2017; Conference date: 13 February 2017; Conference code: 127153}
}

@ARTICLE{Ma2025,
	author = {Ma, Zhuang and Li, Ao and Tang, Jiehao and Zhang, Jianhua and Yin, Zhong},
	title = {Multimodal emotion recognition by fusing complementary patterns from central to peripheral neurophysiological signals across feature domains},
	year = {2025},
	journal = {Engineering Applications of Artificial Intelligence},
	volume = {143},
	doi = {10.1016/j.engappai.2025.110004},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214311388&doi=10.1016%2fj.engappai.2025.110004&partnerID=40&md5=8e570028717b4f54ef449b9025b9cc71},
	affiliations = {Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, 200093, China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, 200093, China; OsloMet Artificial Intelligence Lab, Department of Computer Science, Oslo Metropolitan University, Oslo, N-0130, Norway},
	abstract = {The implementation and application of artificial intelligence are propelling various advanced affective computing frameworks. Automatic recognition of emotions using multimodal physiological signals enhances the efficiency of systems such as health-care applications, pilot cognitive state monitoring, and passive brain-computer interfaces. However, challenges remain in capturing topological-frequency patterns in diverse electroencephalogram (EEG) electrode layouts, uncovering coupling dynamics across adjacent peripheral modalities, and integrating complementary affective patterns from the feature to modality levels. To address these issues, we propose a central-to-peripheral complementary integration network that employs hybrid encoders to extract and integrate affective patterns from EEG and peripheral signals. For EEG, the model unifies features from different channels into a single map to extract local-to-global representations, while for peripheral signals, adjacent cross-modal information is embedded into global affective patterns. These abstractions are systematically aggregated for emotion recognition by aligning affective relevance across domains and modalities within the central and peripheral nervous systems. The proposed model was evaluated on four publicly available multimodal databases using a leave-one-subject-out cross-validation approach. On the Database for Emotion Analysis using Physiological signal (DEAP), the binary recognition accuracy for valence and arousal scales was 75.00% and 77.33%, respectively. On the Human-Computer Interaction (HCI) database, the corresponding binary accuracies were 78.78% and 75.38%. For the SJTU Emotion EEG Datasets IV and V (SEED-IV and SEED-V), the four-class and five-class accuracies were 71.94% and 84.83%, respectively. These results validate the robustness and remarkable generalization capability of the proposed method. © 2025 Elsevier Ltd},
	author_keywords = {Affective human-machine interaction; Application of artificial intelligence; Biomedical signal processing; Implemented artificial intelligence; Multimodal integration},
	keywords = {Brain computer interface; Electrotherapeutics; Local binary pattern; mHealth; Physiological models; Affective Computing; Affective human-machine interaction; Application of artificial intelligence; Biomedical signals processing; Feature domain; Human machine interaction; Implemented artificial intelligence; Multimodal emotion recognition; Multimodal integration; Physiological signals; Emotion Recognition},
	correspondence_address = {Z. Yin; Shanghai, Jungong Road 516, Yangpu District, 200093, China; email: yinzhong@usst.edu.cn},
	publisher = {Elsevier Ltd},
	issn = {09521976},
	coden = {EAAIE},
	language = {English},
	abbrev_source_title = {Eng Appl Artif Intell},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Gan202232,
	author = {Gan, Wenbin and Dao, Minh Son and Zettsu, Koji and Sun, Yuan},
	title = {IoT-based Multimodal Analysis for Smart Education: Current Status, Challenges and Opportunities},
	year = {2022},
	journal = {ICDAR 2022 - Proceedings of the 3rd ACM Workshop on Intelligent Cross-Data Analysis and Retrieval},
	pages = {32 – 40},
	doi = {10.1145/3512731.3534208},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134532381&doi=10.1145%2f3512731.3534208&partnerID=40&md5=11d7498e6c666db89b2c2cae413856de},
	affiliations = {National Institute of Information and Communications Technology, Tokyo, Japan; National Institute of Informatics, Tokyo, Japan},
	abstract = {IoT-based multimodal learning analytics promises to obtain an in-depth understanding of the learning process. It provides the insights for not only the explicit learning indicators but also the implicit attributes of learners, based on which further potential learning support can be timely provided in both physical and cyber world accordingly. In this paper, we present a systematic review of the existing studies for examining the empirical evidences on the usage of IoT data in education and the capabilities of multimodal analysis to provide useful insights for smarter education. In particular, we classify the multimodal data into four categories based on the data sources (data from digital, physical, physiological and environmental spaces). Moreover, we propose a concept framework for better understanding the current state of the filed and summarize the insights into six main themes (learner behavior understanding, learner affection computing, smart learning environment, learning performance prediction, group collaboration modeling and intelligent feedback) based on the objectives for intelligent learning. The associations between different combinations of data modalities and various learning indicators are comprehensively discussed. Finally, the challenges and future directions are also presented from three aspects.  © 2022 ACM.},
	author_keywords = {internet of things; IoT in education; learning analytics; multimodal analysis; smart education},
	keywords = {Computer aided instruction; Learning systems; Modal analysis; Current status; Explicit learning; In-depth understanding; IoT in education; Learning analytic; Learning process; Learning support; Multi-modal learning; Multimodal analysis; Smart education; Internet of things},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145039241-9},
	language = {English},
	abbrev_source_title = {ICDAR - Proc. ACM Workshop Intell. Cross-Data Anal. Retr.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 3rd ACM Workshop on Intelligent Cross-Data Analysis and Retrieval, ICDAR 2022; Conference date: 27 June 2022 through 30 June 2022; Conference code: 180407}
}

@ARTICLE{Zheng20232213,
	author = {Zheng, Jiahao and Zhang, Sen and Wang, Zilu and Wang, Xiaoping and Zeng, Zhigang},
	title = {Multi-Channel Weight-Sharing Autoencoder Based on Cascade Multi-Head Attention for Multimodal Emotion Recognition},
	year = {2023},
	journal = {IEEE Transactions on Multimedia},
	volume = {25},
	pages = {2213 – 2225},
	doi = {10.1109/TMM.2022.3144885},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123755441&doi=10.1109%2fTMM.2022.3144885&partnerID=40&md5=183862e49f0ab2d1f7b2eab2d884f896},
	affiliations = {The School of Artificial Intelligence and Automation, The Key Laboratory of Image Processing, Intelligent Control of Education Ministry of China, Huazhong University of Science and Technology, Wuhan, 430074, China},
	abstract = {Multimodal Emotion Recognition is challenging because of the heterogeneity gap among different modalities. Due to the powerful ability of feature abstraction, Deep Neural Networks (DNNs) have exhibited significant success in bridging the heterogeneity gap in cross-modal retrieval and generation tasks. In this work, a DNNs-based Multi-channel Weight-sharing Autoencoder with Cascade Multi-head Attention (MCWSA-CMHA) is proposed to generically address the affective heterogeneity gap in MER. Specifically, multimodal heterogeneity features are extracted by multiple independent encoders, and then a scalable heterogeneous feature fusion module (CMHA) is realized by connecting multiple multi-head attention modules in series. The core of the proposed algorithm is to reduce the heterogeneity between the output features of different encoders through the unsupervised training of MCWSA, and then to model the affective interactions between different modal features through the supervised training of CMHA. Experimental results demonstrate that the proposed MCWSA-CMHA achieves outperformance on two publicly available datasets compared with the state-of-the-art techniques. In addition, visualization experiments and approximation experiments are used to verify the effectiveness of each module in the proposed algorithm, and the experimental results show that the proposed MCWSA-CMHA can mine more emotion-related information among multimodal features compared with other fusion methods.  © 2022 IEEE.},
	author_keywords = {autoencoder; multi-head attention mechanism; Multimodal emotion recognition (MER)},
	keywords = {Approximation algorithms; Deep neural networks; Emotion Recognition; Job analysis; Learning systems; Signal encoding; Speech recognition; Attention mechanisms; Auto encoders; Decoding; Emotion recognition; Features extraction; Multi channel; Multi-head attention mechanism; Multimodal emotion recognition; Task analysis; Data mining},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {15209210},
	coden = {ITMUF},
	language = {English},
	abbrev_source_title = {IEEE Trans Multimedia},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20}
}

@ARTICLE{Lee2024,
	author = {Lee, Yuan-Hsuan and Jhang, Jing-Ya and Hong, Huang-Yao},
	title = {Becoming epistemically active in online reading: Facilitating elementary school students’ multimodal multiple document reading via sourcing organizers},
	year = {2024},
	journal = {Computers and Education},
	volume = {216},
	doi = {10.1016/j.compedu.2024.105048},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190836070&doi=10.1016%2fj.compedu.2024.105048&partnerID=40&md5=83acfce70e0aa887c4d52079f192fb49},
	affiliations = {Department of Education and Learning Technology, National Tsing Hua University, Taiwan; Department of Education, National Chengchi University, Taiwan},
	abstract = {In the AI era, it has become crucial to evaluate information found on the Internet critically. This research aimed to investigate the impact of a sourcing organizer on sixth graders' online multimodal and multiple document reading (MMDR) abilities, focusing on aspects such as source-content link and text integration in relation to reading on the Internet. Cognitive and affective factors associated with MMDR were examined. The study involved 52 sixth-graders (55.77% males) from two typical elementary school classes in the northern region of Taiwan. Two intact classes were randomly assigned to either the experimental or control group with the quasi-experimental design. The experimental group (n = 26) received a pre-outlined sourcing organizer, guiding them to record the article title, author, publication date, website name, and major assertions from six assigned multimodal texts. In contrast, the control group (n = 26) received a regular organizer, prompting them to summarize the main ideas from the same six assigned multimodal texts. The study's findings indicated that employing sourcing organizers positively impacted students' performance in text integration. However, it was observed that both groups, regardless of whether they used regular organizers or sourcing organizers, experienced benefits in terms of source-content links. Furthermore, reading ability emerged as the sole significant predictor for source-content links, whereas both reading ability and the use of sourcing organizers predicted text integration. The implications of these findings were discussed to provide insights into instructional strategies to develop online MMDR competencies in elementary students. © 2024 Elsevier Ltd},
	author_keywords = {Online multimodal and multiple document reading; Reading ability; Reading interest; Source-content link; Text integration},
	keywords = {Students; Control groups; Elementary schools; Experimental groups; Multi-modal; Multiple documents; Online multimodal and multiple document reading; Reading abilities; Reading interest; Source-content link; Text integration; Integration},
	correspondence_address = {H.-Y. Hong; Department of Education, National Cheng Chi University, Taiwan; email: hyhong@nccu.edu.tw},
	publisher = {Elsevier Ltd},
	issn = {03601315},
	coden = {COMED},
	language = {English},
	abbrev_source_title = {Comput Educ},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{You20161445,
	author = {You, Quanzeng},
	title = {Sentiment and emotion analysis for social multimedia: Methodologies and applications},
	year = {2016},
	journal = {MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
	pages = {1445 – 1449},
	doi = {10.1145/2964284.2971475},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994651217&doi=10.1145%2f2964284.2971475&partnerID=40&md5=f2774b542d27eae0d18065987a106027},
	affiliations = {Department of Computer Science, University of Rochester, Rochester, 14623, NY, United States},
	abstract = {Online social networks have attracted the attention from both the academia and real world. In particular, the rich multimedia information accumulated in recent years provides an easy and convenient way for more active communication between people. This offers an opportunity to research people's behaviors and activities based on those multimedia content. One emerging area is driven by the fact that these massive multimedia data contain people's daily sentiments and opinions. However, existing sentiment analysis typically focuses on textual information regardless of the visual content, which may be as informative in expressing people's sentiments and opinions. In this research, we attempt to analyze the online sentiment changes of social media users using both the textual and visual content. Nowadays, social media networks such as Twitter have become major platforms of information exchange and communication between users, with tweets as the common information carrier. As an old saying has it, an image is worth a thousand words. The image tweet is a great example of multimodal sentiment. In this research, we focus on sentiment analysis based on visual and multimedia information analysis. We will review the state-of-the-art in this topic. Several of our projects related to this research area will also be discussed. Experimental results are included to demonstrate and summarize our contributions. © 2016 ACM.},
	author_keywords = {Joint sentiment analysis; Multimodal; Visual sentiment analysis},
	keywords = {Data mining; Social networking (online); Active communications; Information carriers; Information exchanges; Multi-modal; Multimedia information; On-line social networks; Sentiment analysis; Social media networks; Behavioral research},
	correspondence_address = {Q. You; Department of Computer Science, University of Rochester, Rochester, 14623, United States; email: qyou@cs.rochester.edu},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145033603-1},
	language = {English},
	abbrev_source_title = {MM - Proc. ACM Multimed. Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; Conference name: 24th ACM Multimedia Conference, MM 2016; Conference date: 15 October 2016 through 19 October 2016; Conference code: 124107}
}

@ARTICLE{Lu202415092,
	author = {Lu, Qiang and Sun, Xia and Long, Yunfei and Gao, Zhizezhang and Feng, Jun and Sun, Tao},
	title = {Sentiment Analysis: Comprehensive Reviews, Recent Advances, and Open Challenges},
	year = {2024},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	volume = {35},
	number = {11},
	pages = {15092 – 15112},
	doi = {10.1109/TNNLS.2023.3294810},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165390829&doi=10.1109%2fTNNLS.2023.3294810&partnerID=40&md5=1af4c41b27f33e49ffc585cb6f2984b8},
	affiliations = {Northwest University, School of Information Technology, Xi'an, 710127, China; University of Essex, School of Computer Science and Electrical Engineering, Colchester, CO4 3SQ, United Kingdom; Renqiu City, North China Petroleum Tiancheng Industrial Group Company Ltd., Hebei, 062552, China},
	abstract = {Sentiment analysis (SA) aims to understand the attitudes and views of opinion holders with computers. Previous studies have achieved significant breakthroughs and extensive applications in the past decade, such as public opinion analysis and intelligent voice service. With the rapid development of deep learning, SA based on various modalities has become a research hotspot. However, only individual modality has been analyzed separately, lacking a systematic carding of comprehensive SA methods. Meanwhile, few surveys covering the topic of multimodal SA (MSA) have been explored yet. In this article, we first take the modality as the thread to design a novel framework of SA tasks to provide researchers with a comprehensive understanding of relevant advances in SA. Then, we introduce the general workflows and recent advances of single-modal in detail, discuss the similarities and differences of single-modal SA in data processing and modeling to guide MSA, and summarize the commonly used datasets to provide guidance on data and methods for researchers according to different task types. Next, a new taxonomy is proposed to fill the research gaps in MSA, which is divided into multimodal representation learning and multimodal data fusion. The similarities and differences between these two methods and the latest advances are described in detail, such as dynamic interaction between multimodalities, and the multimodal fusion technologies are further expanded. Moreover, we explore the advanced studies on multimodal alignment, chatbots, and Chat Generative Pre-trained Transformer (ChatGPT) in SA. Finally, we discuss the open research challenges of MSA and provide four potential aspects to improve future works, such as cross-modal contrastive learning and multimodal pretraining models. © 2023 IEEE.},
	author_keywords = {multimodal; Multimodal data fusion; multimodal representation learning; sentiment analysis (SA); single-modal},
	keywords = {Data fusion; Data integration; Deep learning; Modal analysis; Taxonomies; Multi-modal; Multimodal data fusion; Multimodal representation learning; Representation learning; Sentiment analyse; Sentiment analysis; Single-modal; Task analysis; Sentiment analysis},
	correspondence_address = {X. Sun; Northwest University, School of Information Technology, Xi'an, 710127, China; email: raindy@nwu.edu.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {2162237X},
	pmid = {37478046},
	language = {English},
	abbrev_source_title = {IEEE Trans. Neural Networks Learn. Sys.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Green Open Access}
}

@CONFERENCE{Moraes2022133,
	author = {Moraes, Leonardo and Marcacini, Ricardo Marcondes and Goularte, Rudinei},
	title = {Video Summarization using Text Subjectivity Classification},
	year = {2022},
	journal = {ACM International Conference Proceeding Series},
	pages = {133 – 141},
	doi = {10.1145/3539637.3556998},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139788623&doi=10.1145%2f3539637.3556998&partnerID=40&md5=141296cc87ccc9282e461687d04fb921},
	affiliations = {Computer Science Department, Universidade de São Paulo (USP), São Paulo, Brazil},
	abstract = {Video summarization has attracted researchers' attention because it provides a compact and informative video version, supporting users and systems to save efforts in searching and understanding content of interest. Current techniques employ different strategies to select which video segments should be included in the final summary. The challenge is to process multimodal data present in the video looking for relevance clues (like redundant or complementary information) that help make a decision. A recent strategy is to use subjectivity detection. The presence or the absence of subjectivity can be explored as a relevance clue, helping to bring video summaries closer to the final user's expectations. However, despite this potential, there is a gap on how to capture subjectivity information from videos. This paper investigates video summarization through subjectivity classification from video transcripts. This approach requires dealing with recent challenges that are important in video summarization tasks, such as detecting subjectivity in different languages and across multiple domains. We propose a multilingual machine learning model trained to deal with subjectivity classification in multiple domains. An experimental evaluation with different benchmark datasets indicates that our multilingual and multi-domain method achieves competitive results, even compared to language-specific models. Furthermore, such a model can be used to provide subjectivity as a content selection criterion in the video summarization task, filtering out segments that are not relevant to a video domain of interest.  © 2022 ACM.},
	author_keywords = {BERT; NLP; sentiment analysis; subjectivity classification; video summarization},
	keywords = {Classification (of information); Search engines; Video recording; 'current; BERT; Multi-modal data; Multiple domains; Sentiment analysis; Subjectivity classifications; User expectations; Video segments; Video summaries; Video summarization; Sentiment analysis},
	publisher = {Association for Computing Machinery},
	isbn = {978-145039409-3},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 28th Brazilian Symposium on Multimedia and Web, WebMedia 2022; Conference date: 7 November 2022 through 11 November 2022; Conference code: 182986}
}

@CONFERENCE{Hanjalic2003II289,
	author = {Hanjalic, A.},
	title = {Multimodal approach to measuring excitement in video},
	year = {2003},
	journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
	volume = {2},
	pages = {II289 – II292},
	doi = {10.1109/ICME.2003.1221610},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-29844449937&doi=10.1109%2fICME.2003.1221610&partnerID=40&md5=1518f748997a916509664b3db0f73f08},
	affiliations = {Delft University of Technology, Delft, Netherlands},
	abstract = {We present in this paper an approach to mimic the excitement that is evoked in a user while watching a video. The proposed approach is meant to enhance user's comfort when dealing with large amount of broadcasted digital television data reaching his home. For this reason, we deliberately avoid using any sensors placed on users: the simulation of user excitement is based here solely on cues that are available in the digital video stream, and that can be extracted by using standard audio and video processing tools as well as by observing the way video is edited. Relation between the extracted features and evoked excitement is drawn partly from psycho-physiological research and partly from analyzing the video generation practice. Our methodology is generic and can be employed broadly in the process of video abstraction and for revealing the affective characteristics of video content. © 2003 IEEE.},
	keywords = {Digital television; Multimedia systems; Television broadcasting; Video streaming; Audio and video; Digital video streams; Large amounts; Multi-modal approach; Psycho-physiological; Video abstraction; Video contents; Video generation; Computer graphics},
	publisher = {IEEE Computer Society},
	issn = {19457871},
	isbn = {0780379659},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Multimedia Expo},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; Conference name: 2003 International Conference on Multimedia and Expo, ICME 2003; Conference date: 6 July 2003 through 9 July 2003; Conference code: 114795}
}

@CONFERENCE{Shen20192938,
	author = {Shen, Yiqing and Li, Yingbo and Peng, Qinke},
	title = {The User-Pleasant Video Skimming by Multi-Sources-Indices Analysis},
	year = {2019},
	journal = {Proceedings - 2019 Chinese Automation Congress, CAC 2019},
	pages = {2938 – 2943},
	doi = {10.1109/CAC48633.2019.8996846},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080060628&doi=10.1109%2fCAC48633.2019.8996846&partnerID=40&md5=1d3305758947808e520a25c3b5479e28},
	affiliations = {System Engineering Institute, Xi' An Jiaotong University, Xi'an, China; Naister, Paris, France},
	abstract = {Video skimming is the process to extract the most significant content of the video and represent it in a concise form. Previous research of video skimming generation could not make full use of the multi-modal information and thoroughly consider the user preferences. In this paper, we propose a novel approach of video skimming through exploiting the fusion of video temporal information and semantic information, supported by audio and text classification and keyframe extraction. By means of analysis on multi-modal video information including audio, text and visual indices, one video skim is generated for one original video. In addition, we introduce the brand-safe filtering and sentiment analysis in order to only reserve the user-pleasant content. Experiment conducted on Youtube-8M dataset has proved that our method highly outperforms the approaches which only consider partial information in video. Moreover, with brand-safe filtering, the unpleasant content has been successfully removed. © 2019 IEEE.},
	author_keywords = {audio and text classification; keyframe extraction; multi-modal information fusion; video skimming},
	keywords = {Extraction; Modal analysis; Semantics; Sentiment analysis; Key-frame extraction; Multi-modal information; Multimodal information fusion; Partial information; Semantic information; Temporal information; Text classification; Video skimming; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172814094-0},
	language = {English},
	abbrev_source_title = {Proc. - Chin. Autom. Congr., CAC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2019 Chinese Automation Congress, CAC 2019; Conference date: 22 November 2019 through 24 November 2019; Conference code: 157710}
}

@ARTICLE{Stratou2017190,
	author = {Stratou, Giota and Morency, Louis-Philippe},
	title = {MultiSense-Context-Aware Nonverbal Behavior Analysis Framework: A Psychological Distress Use Case},
	year = {2017},
	journal = {IEEE Transactions on Affective Computing},
	volume = {8},
	number = {2},
	pages = {190 – 203},
	doi = {10.1109/TAFFC.2016.2614300},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027457094&doi=10.1109%2fTAFFC.2016.2614300&partnerID=40&md5=6a590906e176ff9ae2161ee14a7f822a},
	affiliations = {Institute for Creative Technologies, University of Southern California, Playa Vista, 90094, CA, United States; Language Technology Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States},
	abstract = {During face-to-face interactions, people naturally integrate nonverbal behaviors such as facial expressions and body postures as part of the conversation to infer the communicative intent or emotional state of their interlocutor. The interpretation of these nonverbal behaviors will often be contextualized by interactional cues such as the previous spoken question, the general discussion topic or the physical environment. A critical step in creating computers able to understand or participate in this type of social face-to-face interactions is to develop a computational platform to synchronously recognize nonverbal behaviors as part of the interactional context. In this platform, information for the acoustic and visual modalities should be carefully synchronized and rapidly processed. At the same time, contextual and interactional cues should be remembered and integrated to better interpret nonverbal (and verbal) behaviors. In this article, we introduce a real-time computational framework, MultiSense, which offers flexible and efficient synchronization approaches for context-based nonverbal behavior analysis. MultiSense is designed to utilize interactional cues from both interlocutors (e.g., from the computer and the human participant) and integrate this contextual information when interpreting nonverbal behaviors. MultiSense can also assimilate behaviors over a full interaction and summarize the observed affective states of the user. We demonstrate the capabilities of the new framework with a concrete use case from the mental health domain where MultiSense is used as part of a decision support tool to assess indicators of psychological distress such as depression and post-traumatic stress disorder (PTSD). In this scenario, MultiSense not only infers psychological distress indicators from nonverbal behaviors but also broadcasts the user state in real-time to a virtual agent (i.e., a digital interviewer) designed to conduct semi-structured interviews with human participants. Our experiments show the added value of our multimodal synchronization approaches and also demonstrate the importance of MultiSense contextual interpretation when inferring distress indicators. © 2016 IEEE.},
	author_keywords = {automatic distress assessment; behavior quantification; framework for multimodal behavioral understanding; MultiSense; system for affective computing},
	keywords = {Decision support systems; Network function virtualization; Synchronization; Virtual reality; Affective Computing; automatic distress assessment; behavior quantification; Multi-modal; MultiSense; Human computer interaction},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {19493045},
	language = {English},
	abbrev_source_title = {IEEE Trans. Affective Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 47}
}